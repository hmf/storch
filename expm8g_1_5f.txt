nohup: ignoring input
[info] compiling 1 Scala source to /workspaces/storch/out/examples/compile.dest/classes ...
[warn] there were 7 deprecation warnings; re-run with -deprecation for details
[warn] one warning found
[info] done compiling
[W interface.cpp:47] Warning: Loading nvfuser library failed with: Error in dlopen: libtorch.so: cannot open shared object file: No such file or directory (function LoadingNvfuserLibrary)
BiGram
Using device: Device(CUDA,-1)
File /workspaces/storch/data/input.txt already exists.
chars = 
,  , !, $, &, ', ,, -, ., 3, :, ;, ?, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z
vocab_size = 65
"BiGram!" = "BiGram!"
inputs:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
targets:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
----
xb:
Let's he
.
for that
yb:
et's hea
.
or that 
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39
when input is [57] the target: 43
when input is [57, 43] the target: 60
when input is [57, 43, 60] the target: 43
when input is [57, 43, 60, 43] the target: 52
when input is [57, 43, 60, 43, 52] the target: 1
when input is [57, 43, 60, 43, 52, 1] the target: 63
when input is [57, 43, 60, 43, 52, 1, 63] the target: 43
when input is [57, 43, 60, 43, 52, 1, 63, 43] the target: 39
when input is [60] the target: 43
when input is [60, 43] the target: 42
when input is [60, 43, 42] the target: 8
when input is [60, 43, 42, 8] the target: 0
when input is [60, 43, 42, 8, 0] the target: 25
when input is [60, 43, 42, 8, 0, 25] the target: 63
when input is [60, 43, 42, 8, 0, 25, 63] the target: 1
when input is [60, 43, 42, 8, 0, 25, 63, 1] the target: 45
when input is [56] the target: 42
when input is [56, 42] the target: 5
when input is [56, 42, 5] the target: 57
when input is [56, 42, 5, 57] the target: 1
when input is [56, 42, 5, 57, 1] the target: 57
when input is [56, 42, 5, 57, 1, 57] the target: 39
when input is [56, 42, 5, 57, 1, 57, 39] the target: 49
when input is [56, 42, 5, 57, 1, 57, 39, 49] the target: 43
when input is [43] the target: 57
when input is [43, 57] the target: 58
when input is [43, 57, 58] the target: 63
when input is [43, 57, 58, 63] the target: 6
when input is [43, 57, 58, 63, 6] the target: 1
when input is [43, 57, 58, 63, 6, 1] the target: 58
when input is [43, 57, 58, 63, 6, 1, 58] the target: 46
when input is [43, 57, 58, 63, 6, 1, 58, 46] the target: 47
when input is [43] the target: 1
when input is [43, 1] the target: 51
when input is [43, 1, 51] the target: 39
when input is [43, 1, 51, 39] the target: 63
when input is [43, 1, 51, 39, 63] the target: 1
when input is [43, 1, 51, 39, 63, 1] the target: 40
when input is [43, 1, 51, 39, 63, 1, 40] the target: 43
when input is [43, 1, 51, 39, 63, 1, 40, 43] the target: 1
when input is [58] the target: 46
when input is [58, 46] the target: 43
when input is [58, 46, 43] the target: 1
when input is [58, 46, 43, 1] the target: 43
when input is [58, 46, 43, 1, 43] the target: 39
when input is [58, 46, 43, 1, 43, 39] the target: 56
when input is [58, 46, 43, 1, 43, 39, 56] the target: 57
when input is [58, 46, 43, 1, 43, 39, 56, 57] the target: 10
when input is [39] the target: 58
when input is [39, 58] the target: 47
when input is [39, 58, 47] the target: 53
when input is [39, 58, 47, 53] the target: 52
when input is [39, 58, 47, 53, 52] the target: 12
when input is [39, 58, 47, 53, 52, 12] the target: 1
when input is [39, 58, 47, 53, 52, 12, 1] the target: 37
when input is [39, 58, 47, 53, 52, 12, 1, 37] the target: 53
when input is [53] the target: 56
when input is [53, 56] the target: 43
when input is [53, 56, 43] the target: 1
when input is [53, 56, 43, 1] the target: 21
when input is [53, 56, 43, 1, 21] the target: 1
when input is [53, 56, 43, 1, 21, 1] the target: 41
when input is [53, 56, 43, 1, 21, 1, 41] the target: 39
when input is [53, 56, 43, 1, 21, 1, 41, 39] the target: 51
when input is [50] the target: 39
when input is [50, 39] the target: 52
when input is [50, 39, 52] the target: 63
when input is [50, 39, 52, 63] the target: 1
when input is [50, 39, 52, 63, 1] the target: 47
when input is [50, 39, 52, 63, 1, 47] the target: 58
when input is [50, 39, 52, 63, 1, 47, 58] the target: 57
when input is [50, 39, 52, 63, 1, 47, 58, 57] the target: 43
when input is [56] the target: 53
when input is [56, 53] the target: 63
when input is [56, 53, 63] the target: 1
when input is [56, 53, 63, 1] the target: 42
when input is [56, 53, 63, 1, 42] the target: 47
when input is [56, 53, 63, 1, 42, 47] the target: 42
when input is [56, 53, 63, 1, 42, 47, 42] the target: 1
when input is [56, 53, 63, 1, 42, 47, 42, 1] the target: 57
when input is [39] the target: 51
when input is [39, 51] the target: 1
when input is [39, 51, 1] the target: 39
when input is [39, 51, 1, 39] the target: 44
when input is [39, 51, 1, 39, 44] the target: 56
when input is [39, 51, 1, 39, 44, 56] the target: 39
when input is [39, 51, 1, 39, 44, 56, 39] the target: 47
when input is [39, 51, 1, 39, 44, 56, 39, 47] the target: 42
when input is [17] the target: 24
when input is [17, 24] the target: 21
when input is [17, 24, 21] the target: 38
when input is [17, 24, 21, 38] the target: 13
when input is [17, 24, 21, 38, 13] the target: 14
when input is [17, 24, 21, 38, 13, 14] the target: 17
when input is [17, 24, 21, 38, 13, 14, 17] the target: 32
when input is [17, 24, 21, 38, 13, 14, 17, 32] the target: 20
xb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
yb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
xb (set Device(CUDA,-1)):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
loss0 = tensor dtype=float32, shape=[], device=CPU 
2.1746
loss1 = tensor dtype=float32, shape=[], device=CPU 
1.9668
loss2 = tensor dtype=float32, shape=[], device=CPU 
1.8103
batch_size * block_size = 128
logits.shape = ArraySeq(128, 65)
loss m0 = 4.6391506
decode:'
y
lX$fDkRZ,
dco?f,Zh,OLFb,e&sK
;:iPTCmLBbzA$3:.aS&JO-3GSMwF?gLTaUhXFY'X3FhhMNuwq&J,K$.t?VrYdX3rDoa'e'
4.624553
decode 2:'
NAwMEQPgKWxvfDEZa3rxzkkNQ:
YoR&$FMtofVimE;q$!BAm$W;$dYlM!Rueg ixveesY3hcieOlxS&HFG?Zrov E;,,,BeqWk Gn&hD!.vrWjco!pkAJljndGUVQu.C-Ax;ZqPScwlDN:pSsO;?Oee&X3Uwty.vwlvBmUHI.
Bm&pjXPggvwE;qPgDGyqwJ'l
lXSkkqyoaW-;s;&FbrVCeIib3Hr'Tab-&fM$HZqETCgK
hieKqyOp-Lj3gAg-;T3H
hohkOxvFvFrkgW&A Lkk;3Hrkh!Bm:f't,Cdy$flMUE;,wYfMfMPrD?UqY'S?U.JaHK-NLbE!ar,
yb&h&:w:adspbWP$!BE;DxsYBtuicJKNtk&Jar?Any-Rr-Ibs-I&fym&EZ!NMJk'QNEZFEAk3RJ3&.JA-IXq'RO3GROePm !BCy
;emWsNBmeXnxugpVqweV-e&ArXaJR?;$HOzx;jWX$.Ct'cUlugUbxQEOT$Tqrc'
step 0: train loss 4.593183, val loss 4.556398
decode 3:'
$ Dfspy&psStz&$UD l..N
EEiasAvJ?mVp ijqsjEoYSWXpPxAbN
Ymov3tL-Z?ACa3!3LxXCPxsFHkp-vm;YHKieHP-HnmdgufWxVO?eRUC$;Lx:yhD$ZYCCN3gscUFw?c$YmSu3idhMUeUq,FXoxlgqKG!ZcS?'3aak-&OcXavzc-E&F''3:O k ! .vDCBUmlxnFm,CMqJ:N
ZlgWS?'PCkvy,wNF'vkdIiGZ-ADNpIHxdk
$HqZC&X$GiU,LxXCD?mFyvkeHRI,zHoJxMiuGoKtQDCn?DKt.e C3tm, kYpQ;tG!oJPs-b.AengdgNtyc$zkDU3EFBlTQJbkeHPYcUrAqMO
FwD;SLx.gTBwht-g&LXvY$W'ZtT
TWL:Jc;qylxkpw?GoCeMTI3tyLBv.NuwpA.NaFQiWScQOwHRnu;wg.PSLMRd&c&UD ,CL3g,X LYf;a;SDXan$:CKayNuJIs?E
g

EM:,Fme&3vvmSBLsO'
a=tensor dtype=float32, shape=[3, 3], device=CPU 
[[1.0000, 0.0000, 0.0000],
 [0.5000, 0.5000, 0.0000],
 [0.3333, 0.3333, 0.3333]]
--
b=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [6.0000, 4.0000],
 [6.0000, 5.0000]]
--
c=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [4.0000, 5.5000],
 [4.6667, 5.3333]]
ArraySeq(4, 8, 2)
wei0.shape = ArraySeq(8, 8)
true
true
Token embedding: BigramLanguageModel1
4225 parameters
BigramLanguageModel1: #3 4225 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
step 0: train loss 4.346576, val loss 4.3445053
decode 4:'
?q$;xfDkRZkNdc'wb,ZTkOLOT,eCtK
bHxPj&kMBbzA$3:.aSKgO-33SMBc?gcTa
hX;YV HtpXeNuwqcPkxv.tbar dXl!DZaLeWuwccHPmREx,fDEdnYzxzCWNuX
Yo3&$LMtofXiEIvBE!&V!$W;Kd!lHx,ae3 irweYERnIciK;lSW;HFGAZroG EsSXUB;qWklJ.gGD-.CyWjbH!pelJlinFAp;av.C-huDZqoVchvVy:pUup;Mais'X3UwtyfMJ'vBPUuI.3BmTpaY-iMvIEjqkpD:lqwJclmBtSkklmoaW-nNA&QPdVCeIib3Tw'TSEG&fM$HZLETcg$
hxJ$AsLC-LK3gAN-xTrA
XeLkXMmnvnrufWqA s
;;3;QDLWTm:fvtwgdy.vlMUE$Tw,fMfMPrD?CXYIS?B.KrHK-NLbE!rs,dyb&i&a
aadKabWPh!JEgDFHYBhuihVKN.M?DUrAAnyHRrxfbsmc&fy &Ec!NMJ'
Token + positional embedding: BigramLanguageModel2
4481 parameters
BigramLanguageModel2: #4 4481 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
decode 5:'
k'nEEPFrPkULmKYy.AYHXq'WO3;R
S?m !b&Hx;EgWsNB-r?KXm;FVqqrxmiYArSaJR?;$H-zgKjOhBGC?' EwugybxIE.T$Jmuc$ yfv:y&tsSFD&cYsgJ.m 
EEiasmGJtlMpKSjTkXxsLueIpPTAbN'kmlvMkL-Z?AC-?!3LRoCPTmFFkm-vX;YHKieO:PHuEEgusGxVO?gRz,XALI:ytb$ZGCCI!gscPkn?iKYUj,; QhRUedq,FsoxmgqjGhZcE!HbAakw!O?gwvzc-E.
'ww3C k ! .vPCBuml3NFm,CRz!:NUZlhWIvNPGiIyBOYFkvLhIisZ-A?NdI3idk
bHpZF&XnGenmLzXCD?tFymk?HLIYzqoY3MiuGdKtLoCnijTv.e A3AmN xYpDytGFoxPwMbLC?KgviPt c$zkDG3EiBlTQlbkmHl!P&sSqMO
F&X;fL,.cTjwrtc,&LiuY$WxZtTXTWO;!u;qylCkW;gGoSe'
ArraySeq(4, 8, 16)
tensor dtype=float32, shape=[8, 8], device=CPU 
[[1.0000, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.1574, 0.8426, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.2088, 0.1646, 0.6266, ..., 0.0000, 0.0000, 0.0000],
 ...,
 [0.0176, 0.2689, 0.0215, ..., 0.0019, 0.0000, 0.0000],
 [0.1691, 0.4066, 0.0438, ..., 0.2012, 0.0329, 0.0000],
 [0.0210, 0.0843, 0.0555, ..., 0.0709, 0.2423, 0.2391]]
tensor dtype=float32, shape=[], device=CPU 
1.0449
tensor dtype=float32, shape=[], device=CPU 
1.0700
tensor dtype=float32, shape=[], device=CPU 
17.4690
tensor dtype=float32, shape=[], device=CPU 
1.0918
tensor dtype=float64, shape=[5], device=CPU 
[0.1925, 0.1426, 0.2351, 0.1426, 0.2872]
tensor dtype=float64, shape=[5], device=CPU 
[0.0326, 0.0030, 0.1615, 0.0030, 0.8000]
Single head attention: BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
step 0: train loss 4.1745915, val loss 4.1752186
step 0: train loss 4.166315, val loss 4.169002
decode 6:'







?qfXxfDkRZkNwc.wj,ZTkOLFT,ebtK
b:!PjCkMBbzA$3:.aSvgO-33SM:F?gLTa
hX:YVXJthXfNuwqcPMxG.tbar dXl!DZaLeWFwccHPmRWk,fDEZaYzxzCImuX
YoR&$LMtofViEIvB!!&V!$W;KdYlNZ,ue3 ixYeYEYnkciK;lxW;HFGEZroG EsSXUB;qWk G..GD!.FyWjbm!pelJljnFFUVcu.C-huD3qcnchvVy:?Uup;Mnis'X3Uwty.OJlvBPUHI.yBfTpjY-lgvIEjqk:DGyqwJdlNBtSkklmoaW-CNA&QPdVCeIib3sI'TStG&dE$HZLETxN$Fhx&$FsgC-LKKgAe-xT3H
hexkNVmnvnrufW&A '
;;3;QDL!Tm:fEE,Cey$alPUE$tw,fMFEPRD?UqYIS?m.UrHK-NLuk!aK,iyb&i&:
aadsaUWG$!VE'DFsYBvuihVKN.k?Dar?AnyHRr-utsmI&fn VEc!NMJ'
Single head attention (b): BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
Multi-head attention BigramLanguageModel4
MultiHeadAttention_1 registering hs_0:Head1
MultiHeadAttention_1 registering hs_1:Head1
MultiHeadAttention_1 registering hs_2:Head1
MultiHeadAttention_1 registering hs_3:Head1
10625 parameters
BigramLanguageModel4: #28 10625 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Multi-head attention + FFWD BigramLanguageModel5
MultiHeadAttention_1 registering hs_0:Head1
MultiHeadAttention_1 registering hs_1:Head1
MultiHeadAttention_1 registering hs_2:Head1
MultiHeadAttention_1 registering hs_3:Head1
11681 parameters
BigramLanguageModel5: #30 11681 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  ffwd: FeedFoward(nEmbed = 32): #2 1056 (
    net: Sequential: #2 1056 (
      0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      1: ReLU: #0 <> 
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks BigramLanguageModel6
MultiHeadAttention_1 registering hs_0:Head1
MultiHeadAttention_1 registering hs_1:Head1
MultiHeadAttention_1 registering hs_2:Head1
MultiHeadAttention_1 registering hs_3:Head1
MultiHeadAttention_1 registering hs_0:Head1
MultiHeadAttention_1 registering hs_1:Head1
MultiHeadAttention_1 registering hs_2:Head1
MultiHeadAttention_1 registering hs_3:Head1
MultiHeadAttention_1 registering hs_0:Head1
MultiHeadAttention_1 registering hs_1:Head1
MultiHeadAttention_1 registering hs_2:Head1
MultiHeadAttention_1 registering hs_3:Head1
26081 parameters
BigramLanguageModel6: #82 26081 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #78 21600 (
    0: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    1: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    2: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections - BigramLanguageModel7
MultiHeadAttention_2 registering hs_0:Head1
MultiHeadAttention_2 registering hs_1:Head1
MultiHeadAttention_2 registering hs_2:Head1
MultiHeadAttention_2 registering hs_3:Head1
MultiHeadAttention_2 registering hs_0:Head1
MultiHeadAttention_2 registering hs_1:Head1
MultiHeadAttention_2 registering hs_2:Head1
MultiHeadAttention_2 registering hs_3:Head1
MultiHeadAttention_2 registering hs_0:Head1
MultiHeadAttention_2 registering hs_1:Head1
MultiHeadAttention_2 registering hs_2:Head1
MultiHeadAttention_2 registering hs_3:Head1
51137 parameters
BigramLanguageModel7: #94 51137 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #90 46656 (
    0: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    1: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    2: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Device = Device(CUDA,-1)
51137 parameters
learningRate = 6.0E-6
maxIterations = 75000
step 0: train loss 4.6134653, val loss 4.632922
step 500: train loss 4.1570916, val loss 4.186932
step 1000: train loss 3.9121318, val loss 3.9514165
step 1500: train loss 3.77717, val loss 3.8063
step 2000: train loss 3.629327, val loss 3.6691
step 2500: train loss 3.5300052, val loss 3.564721
step 3000: train loss 3.4163306, val loss 3.4587636
step 3500: train loss 3.3480797, val loss 3.3765874
step 4000: train loss 3.2947326, val loss 3.3612978
step 4500: train loss 3.242309, val loss 3.280067
step 5000: train loss 3.169024, val loss 3.2270517
step 5500: train loss 3.1374266, val loss 3.1886108
step 6000: train loss 3.0868819, val loss 3.1371422
step 6500: train loss 3.073743, val loss 3.1150806
step 7000: train loss 3.049432, val loss 3.0874276
step 7500: train loss 3.032313, val loss 3.0665405
step 8000: train loss 3.0000546, val loss 3.052383
step 8500: train loss 2.995341, val loss 3.018457
step 9000: train loss 2.978937, val loss 3.0104048
step 9500: train loss 2.9575732, val loss 2.9912167
step 10000: train loss 2.933811, val loss 2.9765239
step 10500: train loss 2.911219, val loss 2.9557617
step 11000: train loss 2.9238083, val loss 2.9275494
step 11500: train loss 2.892971, val loss 2.9127147
step 12000: train loss 2.8876903, val loss 2.8987865
step 12500: train loss 2.859414, val loss 2.861057
step 13000: train loss 2.8534722, val loss 2.88483
step 13500: train loss 2.8478677, val loss 2.8631291
step 14000: train loss 2.8434937, val loss 2.8591833
step 14500: train loss 2.8204918, val loss 2.843896
step 15000: train loss 2.8043132, val loss 2.840191
step 15500: train loss 2.7919867, val loss 2.8294582
step 16000: train loss 2.8025992, val loss 2.8060658
step 16500: train loss 2.7718287, val loss 2.7795455
step 17000: train loss 2.7444713, val loss 2.79216
step 17500: train loss 2.7454948, val loss 2.7805996
step 18000: train loss 2.7464797, val loss 2.7707672
step 18500: train loss 2.7465887, val loss 2.7728796
step 19000: train loss 2.7132747, val loss 2.7478542
step 19500: train loss 2.7266486, val loss 2.7464278
step 20000: train loss 2.7325397, val loss 2.7289557
step 20500: train loss 2.7152143, val loss 2.7231884
step 21000: train loss 2.6953287, val loss 2.7065604
step 21500: train loss 2.696127, val loss 2.7122753
step 22000: train loss 2.6760669, val loss 2.7002552
step 22500: train loss 2.676962, val loss 2.7123444
step 23000: train loss 2.6699538, val loss 2.688797
step 23500: train loss 2.6823518, val loss 2.6835206
step 24000: train loss 2.657658, val loss 2.6728554
step 24500: train loss 2.6458495, val loss 2.6525617
step 25000: train loss 2.6425915, val loss 2.645353
step 25500: train loss 2.6369708, val loss 2.6493392
step 26000: train loss 2.6394305, val loss 2.647013
step 26500: train loss 2.619687, val loss 2.6365237
step 27000: train loss 2.6127803, val loss 2.6046898
step 27500: train loss 2.6145067, val loss 2.6360674
step 28000: train loss 2.6056726, val loss 2.614536
step 28500: train loss 2.605, val loss 2.6279564
step 29000: train loss 2.6064403, val loss 2.598963
step 29500: train loss 2.5984778, val loss 2.6147227
step 30000: train loss 2.5837, val loss 2.5968554
step 30500: train loss 2.5894513, val loss 2.5890453
step 31000: train loss 2.5828302, val loss 2.600304
step 31500: train loss 2.5647783, val loss 2.607981
step 32000: train loss 2.5513675, val loss 2.5867398
step 32500: train loss 2.5583115, val loss 2.5791423
step 33000: train loss 2.5661788, val loss 2.5778356
step 33500: train loss 2.5540543, val loss 2.56352
step 34000: train loss 2.5568476, val loss 2.564494
step 34500: train loss 2.5575294, val loss 2.5425286
step 35000: train loss 2.5303547, val loss 2.5584269
step 35500: train loss 2.5320923, val loss 2.5424063
step 36000: train loss 2.5492043, val loss 2.515915
step 36500: train loss 2.53565, val loss 2.5491052
step 37000: train loss 2.5300114, val loss 2.5356388
step 37500: train loss 2.5293734, val loss 2.5388126
step 38000: train loss 2.5263436, val loss 2.5288591
step 38500: train loss 2.518509, val loss 2.5249133
step 39000: train loss 2.5140069, val loss 2.5185611
step 39500: train loss 2.5074258, val loss 2.5195055
step 40000: train loss 2.4947906, val loss 2.5295596
step 40500: train loss 2.5009575, val loss 2.5165656
step 41000: train loss 2.497663, val loss 2.498406
step 41500: train loss 2.5115862, val loss 2.5035062
step 42000: train loss 2.5030923, val loss 2.5052783
step 42500: train loss 2.4938982, val loss 2.4990435
step 43000: train loss 2.4751837, val loss 2.5058656
step 43500: train loss 2.4904985, val loss 2.5077083
step 44000: train loss 2.4931421, val loss 2.5001066
step 44500: train loss 2.4743543, val loss 2.4861486
step 45000: train loss 2.4630585, val loss 2.501151
step 45500: train loss 2.4853551, val loss 2.4931235
step 46000: train loss 2.4671342, val loss 2.462622
step 46500: train loss 2.4733927, val loss 2.4783397
step 47000: train loss 2.4681795, val loss 2.4666564
step 47500: train loss 2.4622366, val loss 2.4671276
step 48000: train loss 2.4560053, val loss 2.4755766
step 48500: train loss 2.4558396, val loss 2.4484656
step 49000: train loss 2.4543767, val loss 2.4670618
step 49500: train loss 2.4526772, val loss 2.4596086
step 50000: train loss 2.4373953, val loss 2.4727635
step 50500: train loss 2.4465368, val loss 2.4516354
step 51000: train loss 2.4435863, val loss 2.4627793
step 51500: train loss 2.4484527, val loss 2.4513562
step 52000: train loss 2.454969, val loss 2.4444525
step 52500: train loss 2.4580448, val loss 2.4400163
step 53000: train loss 2.443047, val loss 2.447718
step 53500: train loss 2.4494407, val loss 2.4438868
step 54000: train loss 2.4362557, val loss 2.446924
step 54500: train loss 2.4351428, val loss 2.4463687
step 55000: train loss 2.435822, val loss 2.4499917
step 55500: train loss 2.4217, val loss 2.440394
step 56000: train loss 2.4228399, val loss 2.441773
step 56500: train loss 2.432208, val loss 2.4370792
step 57000: train loss 2.427175, val loss 2.422596
step 57500: train loss 2.433473, val loss 2.4209237
step 58000: train loss 2.4155922, val loss 2.4252532
step 58500: train loss 2.4173949, val loss 2.4233027
step 59000: train loss 2.4074438, val loss 2.419918
step 59500: train loss 2.4075656, val loss 2.4193347
step 60000: train loss 2.43445, val loss 2.428559
step 60500: train loss 2.4076755, val loss 2.40647
step 61000: train loss 2.398917, val loss 2.4159346
step 61500: train loss 2.4188614, val loss 2.4180353
step 62000: train loss 2.403164, val loss 2.4206014
step 62500: train loss 2.392966, val loss 2.4084036
step 63000: train loss 2.3945236, val loss 2.4137802
step 63500: train loss 2.3833268, val loss 2.4187706
step 64000: train loss 2.377177, val loss 2.4055398
step 64500: train loss 2.395082, val loss 2.4007895
step 65000: train loss 2.396096, val loss 2.410178
step 65500: train loss 2.3954232, val loss 2.423708
step 66000: train loss 2.378127, val loss 2.3993497
step 66500: train loss 2.4048975, val loss 2.3802474
step 67000: train loss 2.3849454, val loss 2.377885
step 67500: train loss 2.3932414, val loss 2.393958
step 68000: train loss 2.3796847, val loss 2.403701
step 68500: train loss 2.3896403, val loss 2.392418
step 69000: train loss 2.3911893, val loss 2.4025927
step 69500: train loss 2.3782759, val loss 2.3910496
step 70000: train loss 2.366243, val loss 2.3716755
step 70500: train loss 2.3592386, val loss 2.379334
step 71000: train loss 2.3998156, val loss 2.4001372
step 71500: train loss 2.3649485, val loss 2.3825123
step 72000: train loss 2.3605323, val loss 2.3795447
step 72500: train loss 2.3637192, val loss 2.3926306
step 73000: train loss 2.3632023, val loss 2.3733888
step 73500: train loss 2.3644714, val loss 2.3880494
step 74000: train loss 2.3843045, val loss 2.3805432
step 74500: train loss 2.373176, val loss 2.373484
step 74999: train loss 2.3712134, val loss 2.3789077
step 75000: train loss 2.3715358, val loss 2.3802633
decode 11:'







And thek bridcowi, har lay ber

Hisenk Bbeou eravegrtansted?
Lhanss:
Wanthat usqorthe. bar dilas ane awice my thanstarut onou 
Yowht, tof is he me mil;
dill!,

W iree sengcin lat Horilkev the and the nor pilerans!
 lolind me litlis me derivy ptugr aiss hew yean's mal normopetelg lower:
Gllw:
Sile akllo W-Cou whd Ceiiby, wirth dourive ceeshime st so mower; the the danteruf bf son; igr tom:

Enge inle ont ffaf Prred my om.
rand Lok! stiyby ak
Sadst Wices and thiuin cour ayraney iry to chan yf voul'
Exception in thread "main" java.lang.ExceptionInInitializerError
	at gpt.BiGram.main(BiGram.scala)
Caused by: java.lang.ArithmeticException: / by zero
	at gpt.BiGram$.<clinit>(BiGram.scala:2119)
	... 1 more
1 targets failed
examples.runMain subprocess failed
