nohup: ignoring input
[info] compiling 1 Scala source to /workspaces/storch/out/examples/compile.dest/classes ...
[warn] there were 8 deprecation warnings; re-run with -deprecation for details
[warn] one warning found
[info] done compiling
BiGram
Using device: Device(CUDA,-1)
File /workspaces/storch/data/input.txt already exists.
chars = 
,  , !, $, &, ', ,, -, ., 3, :, ;, ?, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z
vocab_size = 65
"BiGram!" = "BiGram!"
inputs:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
targets:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
----
xb:
Let's he
.
for that
yb:
et's hea
.
or that 
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39
when input is [57] the target: 43
when input is [57, 43] the target: 60
when input is [57, 43, 60] the target: 43
when input is [57, 43, 60, 43] the target: 52
when input is [57, 43, 60, 43, 52] the target: 1
when input is [57, 43, 60, 43, 52, 1] the target: 63
when input is [57, 43, 60, 43, 52, 1, 63] the target: 43
when input is [57, 43, 60, 43, 52, 1, 63, 43] the target: 39
when input is [60] the target: 43
when input is [60, 43] the target: 42
when input is [60, 43, 42] the target: 8
when input is [60, 43, 42, 8] the target: 0
when input is [60, 43, 42, 8, 0] the target: 25
when input is [60, 43, 42, 8, 0, 25] the target: 63
when input is [60, 43, 42, 8, 0, 25, 63] the target: 1
when input is [60, 43, 42, 8, 0, 25, 63, 1] the target: 45
when input is [56] the target: 42
when input is [56, 42] the target: 5
when input is [56, 42, 5] the target: 57
when input is [56, 42, 5, 57] the target: 1
when input is [56, 42, 5, 57, 1] the target: 57
when input is [56, 42, 5, 57, 1, 57] the target: 39
when input is [56, 42, 5, 57, 1, 57, 39] the target: 49
when input is [56, 42, 5, 57, 1, 57, 39, 49] the target: 43
when input is [43] the target: 57
when input is [43, 57] the target: 58
when input is [43, 57, 58] the target: 63
when input is [43, 57, 58, 63] the target: 6
when input is [43, 57, 58, 63, 6] the target: 1
when input is [43, 57, 58, 63, 6, 1] the target: 58
when input is [43, 57, 58, 63, 6, 1, 58] the target: 46
when input is [43, 57, 58, 63, 6, 1, 58, 46] the target: 47
when input is [43] the target: 1
when input is [43, 1] the target: 51
when input is [43, 1, 51] the target: 39
when input is [43, 1, 51, 39] the target: 63
when input is [43, 1, 51, 39, 63] the target: 1
when input is [43, 1, 51, 39, 63, 1] the target: 40
when input is [43, 1, 51, 39, 63, 1, 40] the target: 43
when input is [43, 1, 51, 39, 63, 1, 40, 43] the target: 1
when input is [58] the target: 46
when input is [58, 46] the target: 43
when input is [58, 46, 43] the target: 1
when input is [58, 46, 43, 1] the target: 43
when input is [58, 46, 43, 1, 43] the target: 39
when input is [58, 46, 43, 1, 43, 39] the target: 56
when input is [58, 46, 43, 1, 43, 39, 56] the target: 57
when input is [58, 46, 43, 1, 43, 39, 56, 57] the target: 10
when input is [39] the target: 58
when input is [39, 58] the target: 47
when input is [39, 58, 47] the target: 53
when input is [39, 58, 47, 53] the target: 52
when input is [39, 58, 47, 53, 52] the target: 12
when input is [39, 58, 47, 53, 52, 12] the target: 1
when input is [39, 58, 47, 53, 52, 12, 1] the target: 37
when input is [39, 58, 47, 53, 52, 12, 1, 37] the target: 53
when input is [53] the target: 56
when input is [53, 56] the target: 43
when input is [53, 56, 43] the target: 1
when input is [53, 56, 43, 1] the target: 21
when input is [53, 56, 43, 1, 21] the target: 1
when input is [53, 56, 43, 1, 21, 1] the target: 41
when input is [53, 56, 43, 1, 21, 1, 41] the target: 39
when input is [53, 56, 43, 1, 21, 1, 41, 39] the target: 51
when input is [50] the target: 39
when input is [50, 39] the target: 52
when input is [50, 39, 52] the target: 63
when input is [50, 39, 52, 63] the target: 1
when input is [50, 39, 52, 63, 1] the target: 47
when input is [50, 39, 52, 63, 1, 47] the target: 58
when input is [50, 39, 52, 63, 1, 47, 58] the target: 57
when input is [50, 39, 52, 63, 1, 47, 58, 57] the target: 43
when input is [56] the target: 53
when input is [56, 53] the target: 63
when input is [56, 53, 63] the target: 1
when input is [56, 53, 63, 1] the target: 42
when input is [56, 53, 63, 1, 42] the target: 47
when input is [56, 53, 63, 1, 42, 47] the target: 42
when input is [56, 53, 63, 1, 42, 47, 42] the target: 1
when input is [56, 53, 63, 1, 42, 47, 42, 1] the target: 57
when input is [39] the target: 51
when input is [39, 51] the target: 1
when input is [39, 51, 1] the target: 39
when input is [39, 51, 1, 39] the target: 44
when input is [39, 51, 1, 39, 44] the target: 56
when input is [39, 51, 1, 39, 44, 56] the target: 39
when input is [39, 51, 1, 39, 44, 56, 39] the target: 47
when input is [39, 51, 1, 39, 44, 56, 39, 47] the target: 42
when input is [17] the target: 24
when input is [17, 24] the target: 21
when input is [17, 24, 21] the target: 38
when input is [17, 24, 21, 38] the target: 13
when input is [17, 24, 21, 38, 13] the target: 14
when input is [17, 24, 21, 38, 13, 14] the target: 17
when input is [17, 24, 21, 38, 13, 14, 17] the target: 32
when input is [17, 24, 21, 38, 13, 14, 17, 32] the target: 20
xb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
yb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
xb (set Device(CUDA,-1)):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
loss0 = tensor dtype=float32, shape=[], device=CPU 
2.1746
loss1 = tensor dtype=float32, shape=[], device=CPU 
1.9668
loss2 = tensor dtype=float32, shape=[], device=CPU 
1.8103
batch_size * block_size = 128
logits.shape = ArraySeq(128, 65)
loss m0 = 4.6391506
decode:'
y
lX$fDkRZ,
dco?f,Zh,OLFb,e&sK
;:iPTCmLBbzA$3:.aS&JO-3GSMwF?gLTaUhXFY'X3FhhMNuwq&J,K$.t?VrYdX3rDoa'e'
4.624553
decode 2:'
NAwMEQPgKWxvfDEZa3rxzkkNQ:
YoR&$FMtofVimE;q$!BAm$W;$dYlM!Rueg ixveesY3hcieOlxS&HFG?Zrov E;,,,BeqWk Gn&hD!.vrWjco!pkAJljndGUVQu.C-Ax;ZqPScwlDN:pSsO;?Oee&X3Uwty.vwlvBmUHI.
Bm&pjXPggvwE;qPgDGyqwJ'l
lXSkkqyoaW-;s;&FbrVCeIib3Hr'Tab-&fM$HZqETCgK
hieKqyOp-Lj3gAg-;T3H
hohkOxvFvFrkgW&A Lkk;3Hrkh!Bm:f't,Cdy$flMUE;,wYfMfMPrD?UqY'S?U.JaHK-NLbE!ar,
yb&h&:w:adspbWP$!BE;DxsYBtuicJKNtk&Jar?Any-Rr-Ibs-I&fym&EZ!NMJk'QNEZFEAk3RJ3&.JA-IXq'RO3GROePm !BCy
;emWsNBmeXnxugpVqweV-e&ArXaJR?;$HOzx;jWX$.Ct'cUlugUbxQEOT$Tqrc'
step 0: train loss 4.593183, val loss 4.556398
decode 3:'
$ Dfspy&psStz&$UD l..N
EEiasAvJ?mVp ijqsjEoYSWXpPxAbN
Ymov3tL-Z?ACa3!3LxXCPxsFHkp-vm;YHKieHP-HnmdgufWxVO?eRUC$;Lx:yhD$ZYCCN3gscUFw?c$YmSu3idhMUeUq,FXoxlgqKG!ZcS?'3aak-&OcXavzc-E&F''3:O k ! .vDCBUmlxnFm,CMqJ:N
ZlgWS?'PCkvy,wNF'vkdIiGZ-ADNpIHxdk
$HqZC&X$GiU,LxXCD?mFyvkeHRI,zHoJxMiuGoKtQDCn?DKt.e C3tm, kYpQ;tG!oJPs-b.AengdgNtyc$zkDU3EFBlTQJbkeHPYcUrAqMO
FwD;SLx.gTBwht-g&LXvY$W'ZtT
TWL:Jc;qylxkpw?GoCeMTI3tyLBv.NuwpA.NaFQiWScQOwHRnu;wg.PSLMRd&c&UD ,CL3g,X LYf;a;SDXan$:CKayNuJIs?E
g

EM:,Fme&3vvmSBLsO'
a=tensor dtype=float32, shape=[3, 3], device=CPU 
[[1.0000, 0.0000, 0.0000],
 [0.5000, 0.5000, 0.0000],
 [0.3333, 0.3333, 0.3333]]
--
b=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [6.0000, 4.0000],
 [6.0000, 5.0000]]
--
c=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [4.0000, 5.5000],
 [4.6667, 5.3333]]
ArraySeq(4, 8, 2)
wei0.shape = ArraySeq(8, 8)
true
true
Token embedding: BigramLanguageModel1
4225 parameters
BigramLanguageModel1: #3 4225 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
step 0: train loss 4.346576, val loss 4.3445053
decode 4:'
?q$;xfDkRZkNdc'wb,ZTkOLOT,eCtK
bHxPj&kMBbzA$3:.aSKgO-33SMBc?gcTa
hX;YV HtpXeNuwqcPkxv.tbar dXl!DZaLeWuwccHPmREx,fDEdnYzxzCWNuX
Yo3&$LMtofXiEIvBE!&V!$W;Kd!lHx,ae3 irweYERnIciK;lSW;HFGAZroG EsSXUB;qWklJ.gGD-.CyWjbH!pelJlinFAp;av.C-huDZqoVchvVy:pUup;Mais'X3UwtyfMJ'vBPUuI.3BmTpaY-iMvIEjqkpD:lqwJclmBtSkklmoaW-nNA&QPdVCeIib3Tw'TSEG&fM$HZLETcg$
hxJ$AsLC-LK3gAN-xTrA
XeLkXMmnvnrufWqA s
;;3;QDLWTm:fvtwgdy.vlMUE$Tw,fMfMPrD?CXYIS?B.KrHK-NLbE!rs,dyb&i&a
aadKabWPh!JEgDFHYBhuihVKN.M?DUrAAnyHRrxfbsmc&fy &Ec!NMJ'
Token + positional embedding: BigramLanguageModel2
4481 parameters
BigramLanguageModel2: #4 4481 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
decode 5:'
k'nEEPFrPkULmKYy.AYHXq'WO3;R
S?m !b&Hx;EgWsNB-r?KXm;FVqqrxmiYArSaJR?;$H-zgKjOhBGC?' EwugybxIE.T$Jmuc$ yfv:y&tsSFD&cYsgJ.m 
EEiasmGJtlMpKSjTkXxsLueIpPTAbN'kmlvMkL-Z?AC-?!3LRoCPTmFFkm-vX;YHKieO:PHuEEgusGxVO?gRz,XALI:ytb$ZGCCI!gscPkn?iKYUj,; QhRUedq,FsoxmgqjGhZcE!HbAakw!O?gwvzc-E.
'ww3C k ! .vPCBuml3NFm,CRz!:NUZlhWIvNPGiIyBOYFkvLhIisZ-A?NdI3idk
bHpZF&XnGenmLzXCD?tFymk?HLIYzqoY3MiuGdKtLoCnijTv.e A3AmN xYpDytGFoxPwMbLC?KgviPt c$zkDG3EiBlTQlbkmHl!P&sSqMO
F&X;fL,.cTjwrtc,&LiuY$WxZtTXTWO;!u;qylCkW;gGoSe'
ArraySeq(4, 8, 16)
tensor dtype=float32, shape=[8, 8], device=CPU 
[[1.0000, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.1574, 0.8426, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.2088, 0.1646, 0.6266, ..., 0.0000, 0.0000, 0.0000],
 ...,
 [0.0176, 0.2689, 0.0215, ..., 0.0019, 0.0000, 0.0000],
 [0.1691, 0.4066, 0.0438, ..., 0.2012, 0.0329, 0.0000],
 [0.0210, 0.0843, 0.0555, ..., 0.0709, 0.2423, 0.2391]]
tensor dtype=float32, shape=[], device=CPU 
1.0449
tensor dtype=float32, shape=[], device=CPU 
1.0700
tensor dtype=float32, shape=[], device=CPU 
17.4690
tensor dtype=float32, shape=[], device=CPU 
1.0918
tensor dtype=float64, shape=[5], device=CPU 
[0.1925, 0.1426, 0.2351, 0.1426, 0.2872]
tensor dtype=float64, shape=[5], device=CPU 
[0.0326, 0.0030, 0.1615, 0.0030, 0.8000]
Single head attention: BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head_1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
step 0: train loss 4.1745915, val loss 4.1752186
step 0: train loss 4.166315, val loss 4.169002
decode 6:'







?qfXxfDkRZkNwc.wj,ZTkOLFT,ebtK
b:!PjCkMBbzA$3:.aSvgO-33SM:F?gLTa
hX:YVXJthXfNuwqcPMxG.tbar dXl!DZaLeWFwccHPmRWk,fDEZaYzxzCImuX
YoR&$LMtofViEIvB!!&V!$W;KdYlNZ,ue3 ixYeYEYnkciK;lxW;HFGEZroG EsSXUB;qWk G..GD!.FyWjbm!pelJljnFFUVcu.C-huD3qcnchvVy:?Uup;Mnis'X3Uwty.OJlvBPUHI.yBfTpjY-lgvIEjqk:DGyqwJdlNBtSkklmoaW-CNA&QPdVCeIib3sI'TStG&dE$HZLETxN$Fhx&$FsgC-LKKgAe-xT3H
hexkNVmnvnrufW&A '
;;3;QDL!Tm:fEE,Cey$alPUE$tw,fMFEPRD?UqYIS?m.UrHK-NLuk!aK,iyb&i&:
aadsaUWG$!VE'DFsYBvuihVKN.k?Dar?AnyHRr-utsmI&fn VEc!NMJ'
Single head attention (b): BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head_1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
Multi-head attention BigramLanguageModel4
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
10625 parameters
BigramLanguageModel4: #28 10625 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Multi-head attention + FFWD BigramLanguageModel5
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
11681 parameters
BigramLanguageModel5: #30 11681 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  ffwd: FeedFoward(nEmbed = 32): #2 1056 (
    net: Sequential: #2 1056 (
      0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      1: ReLU: #0 <> 
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks BigramLanguageModel6
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
26081 parameters
BigramLanguageModel6: #82 26081 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #78 21600 (
    0: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    1: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    2: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections - BigramLanguageModel7
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
51137 parameters
BigramLanguageModel7: #94 51137 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #90 46656 (
    0: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    1: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    2: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections + layer norm - BigramLanguageModel8
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
51585 parameters
BigramLanguageModel8: #108 51585 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #104 47104 (
    0: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    1: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    2: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    3: LayerNorm: #2 <32,32> 
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections + layer norm + Dropout - BigramLanguageModel9
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
51585 parameters
BigramLanguageModel9: #108 51585 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #102 47040 (
    0: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    1: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    2: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
  )
  ln_f: LayerNorm: #2 <32,32> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Device = Device(CUDA,-1)
51585 parameters
learningRate = 2.0E-5
maxIterations = 75000
dropout = 0.2
step 0: train loss 4.3045254, val loss 4.314669, mem 1.1 GiB @ 00 00:00:00.000, mean 00 00:00:00.000
step 500: train loss 3.7620537, val loss 3.7862778, mem 1.3 GiB @ 00 00:00:09.306, mean 00 00:00:00.018
step 1000: train loss 3.5285873, val loss 3.5655096, mem 1.5 GiB @ 00 00:00:18.614, mean 00 00:00:00.018
step 1500: train loss 3.412396, val loss 3.4441087, mem 1.7 GiB @ 00 00:00:27.772, mean 00 00:00:00.018
step 2000: train loss 3.3351123, val loss 3.3722796, mem 1.7 GiB @ 00 00:00:36.894, mean 00 00:00:00.018
step 2500: train loss 3.280093, val loss 3.318217, mem 1.7 GiB @ 00 00:00:45.961, mean 00 00:00:00.018
step 3000: train loss 3.2325158, val loss 3.2738843, mem 1.7 GiB @ 00 00:00:55.453, mean 00 00:00:00.018
step 3500: train loss 3.200335, val loss 3.2175434, mem 1.7 GiB @ 00 00:01:04.848, mean 00 00:00:00.018
step 4000: train loss 3.1714685, val loss 3.2343576, mem 1.8 GiB @ 00 00:01:14.278, mean 00 00:00:00.018
step 4500: train loss 3.1440694, val loss 3.1723747, mem 1.8 GiB @ 00 00:01:24.081, mean 00 00:00:00.019
step 5000: train loss 3.2993145, val loss 3.1469655, mem 1.8 GiB @ 00 00:01:33.435, mean 00 00:00:00.018
step 5500: train loss 3.104011, val loss 3.121018, mem 1.8 GiB @ 00 00:01:42.855, mean 00 00:00:00.018
step 6000: train loss 3.5072143, val loss 3.1792107, mem 1.8 GiB @ 00 00:01:52.424, mean 00 00:00:00.019
step 6500: train loss 3.575367, val loss 3.2120016, mem 1.8 GiB @ 00 00:02:01.721, mean 00 00:00:00.018
step 7000: train loss 3.550752, val loss 3.5670102, mem 1.8 GiB @ 00 00:02:11.135, mean 00 00:00:00.018
step 7500: train loss 3.498151, val loss 3.5303032, mem 1.8 GiB @ 00 00:02:20.568, mean 00 00:00:00.018
step 8000: train loss 4.060225, val loss 3.531538, mem 1.8 GiB @ 00 00:02:30.024, mean 00 00:00:00.018
step 8500: train loss 3.4637582, val loss 3.4743652, mem 1.8 GiB @ 00 00:02:39.403, mean 00 00:00:00.018
step 9000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:02:48.731, mean 00 00:00:00.018
step 9500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:02:58.222, mean 00 00:00:00.018
step 10000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:03:07.693, mean 00 00:00:00.018
step 10500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:03:17.163, mean 00 00:00:00.018
step 11000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:03:26.762, mean 00 00:00:00.019
step 11500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:03:36.265, mean 00 00:00:00.019
step 12000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:03:45.779, mean 00 00:00:00.019
step 12500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:03:55.154, mean 00 00:00:00.018
step 13000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:04:04.629, mean 00 00:00:00.018
step 13500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:04:14.031, mean 00 00:00:00.018
step 14000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:04:23.463, mean 00 00:00:00.018
step 14500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:04:32.911, mean 00 00:00:00.018
step 15000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:04:42.250, mean 00 00:00:00.018
step 15500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:04:51.787, mean 00 00:00:00.019
step 16000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:05:01.296, mean 00 00:00:00.019
step 16500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:05:10.838, mean 00 00:00:00.019
step 17000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:05:20.105, mean 00 00:00:00.018
step 17500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:05:29.268, mean 00 00:00:00.018
step 18000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:05:38.941, mean 00 00:00:00.019
step 18500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:05:48.437, mean 00 00:00:00.018
step 19000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:05:57.879, mean 00 00:00:00.018
step 19500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:06:07.380, mean 00 00:00:00.019
step 20000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:06:16.804, mean 00 00:00:00.018
step 20500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:06:26.329, mean 00 00:00:00.019
step 21000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:06:35.775, mean 00 00:00:00.018
step 21500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:06:45.253, mean 00 00:00:00.018
step 22000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:06:54.642, mean 00 00:00:00.018
step 22500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:07:04.245, mean 00 00:00:00.019
step 23000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:07:13.713, mean 00 00:00:00.018
step 23500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:07:23.239, mean 00 00:00:00.019
step 24000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:07:32.671, mean 00 00:00:00.018
step 24500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:07:42.081, mean 00 00:00:00.018
step 25000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:07:51.439, mean 00 00:00:00.018
step 25500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:08:00.869, mean 00 00:00:00.018
step 26000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:08:10.189, mean 00 00:00:00.018
step 26500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:08:19.507, mean 00 00:00:00.018
step 27000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:08:28.727, mean 00 00:00:00.018
step 27500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:08:38.262, mean 00 00:00:00.019
step 28000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:08:47.655, mean 00 00:00:00.018
step 28500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:08:57.087, mean 00 00:00:00.018
step 29000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:09:06.409, mean 00 00:00:00.018
step 29500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:09:15.913, mean 00 00:00:00.019
step 30000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:09:25.567, mean 00 00:00:00.019
step 30500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:09:34.917, mean 00 00:00:00.018
step 31000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:09:44.471, mean 00 00:00:00.019
step 31500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:09:53.875, mean 00 00:00:00.018
step 32000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:10:03.567, mean 00 00:00:00.019
step 32500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:10:12.920, mean 00 00:00:00.018
step 33000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:10:22.424, mean 00 00:00:00.019
step 33500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:10:32.166, mean 00 00:00:00.019
step 34000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:10:41.686, mean 00 00:00:00.019
step 34500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:10:51.263, mean 00 00:00:00.019
step 35000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:11:00.814, mean 00 00:00:00.019
step 35500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:11:10.252, mean 00 00:00:00.018
step 36000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:11:19.537, mean 00 00:00:00.018
step 36500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:11:29.023, mean 00 00:00:00.018
step 37000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:11:38.622, mean 00 00:00:00.019
step 37500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:11:48.187, mean 00 00:00:00.019
step 38000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:11:57.638, mean 00 00:00:00.018
step 38500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:12:07.064, mean 00 00:00:00.018
step 39000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:12:16.482, mean 00 00:00:00.018
step 39500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:12:25.964, mean 00 00:00:00.018
step 40000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:12:35.539, mean 00 00:00:00.019
step 40500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:12:45.024, mean 00 00:00:00.018
step 41000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:12:54.634, mean 00 00:00:00.019
step 41500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:13:04.214, mean 00 00:00:00.019
step 42000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:13:13.771, mean 00 00:00:00.019
step 42500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:13:22.937, mean 00 00:00:00.018
step 43000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:13:32.377, mean 00 00:00:00.018
step 43500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:13:41.972, mean 00 00:00:00.019
step 44000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:13:51.461, mean 00 00:00:00.018
step 44500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:14:01.005, mean 00 00:00:00.019
step 45000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:14:10.439, mean 00 00:00:00.018
step 45500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:14:20.016, mean 00 00:00:00.019
step 46000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:14:29.698, mean 00 00:00:00.019
step 46500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:14:39.335, mean 00 00:00:00.019
step 47000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:14:48.760, mean 00 00:00:00.018
step 47500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:14:58.319, mean 00 00:00:00.019
step 48000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:15:07.702, mean 00 00:00:00.018
step 48500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:15:17.335, mean 00 00:00:00.019
step 49000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:15:26.799, mean 00 00:00:00.018
step 49500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:15:36.392, mean 00 00:00:00.019
step 50000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:15:45.785, mean 00 00:00:00.018
step 50500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:15:55.313, mean 00 00:00:00.019
step 51000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:16:04.809, mean 00 00:00:00.018
step 51500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:16:14.386, mean 00 00:00:00.019
step 52000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:16:23.873, mean 00 00:00:00.018
step 52500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:16:33.397, mean 00 00:00:00.019
step 53000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:16:42.922, mean 00 00:00:00.019
step 53500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:16:52.288, mean 00 00:00:00.018
step 54000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:17:01.807, mean 00 00:00:00.019
step 54500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:17:11.508, mean 00 00:00:00.019
step 55000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:17:21.320, mean 00 00:00:00.019
step 55500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:17:31.182, mean 00 00:00:00.019
step 56000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:17:40.956, mean 00 00:00:00.019
step 56500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:17:50.777, mean 00 00:00:00.019
step 57000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:18:00.589, mean 00 00:00:00.019
step 57500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:18:10.351, mean 00 00:00:00.019
step 58000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:18:20.106, mean 00 00:00:00.019
step 58500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:18:29.865, mean 00 00:00:00.019
step 59000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:18:39.726, mean 00 00:00:00.019
step 59500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:18:49.545, mean 00 00:00:00.019
step 60000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:18:59.500, mean 00 00:00:00.019
step 60500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:19:09.213, mean 00 00:00:00.019
step 61000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:19:19.024, mean 00 00:00:00.019
step 61500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:19:28.914, mean 00 00:00:00.019
step 62000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:19:38.684, mean 00 00:00:00.019
step 62500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:19:48.585, mean 00 00:00:00.019
step 63000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:19:58.373, mean 00 00:00:00.019
step 63500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:20:08.115, mean 00 00:00:00.019
step 64000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:20:17.954, mean 00 00:00:00.019
step 64500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:20:27.813, mean 00 00:00:00.019
step 65000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:20:37.581, mean 00 00:00:00.019
step 65500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:20:47.351, mean 00 00:00:00.019
step 66000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:20:57.127, mean 00 00:00:00.019
step 66500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:21:06.865, mean 00 00:00:00.019
step 67000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:21:16.603, mean 00 00:00:00.019
step 67500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:21:26.460, mean 00 00:00:00.019
step 68000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:21:36.296, mean 00 00:00:00.019
step 68500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:21:46.172, mean 00 00:00:00.019
step 69000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:21:55.985, mean 00 00:00:00.019
step 69500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:22:05.879, mean 00 00:00:00.019
step 70000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:22:15.684, mean 00 00:00:00.019
step 70500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:22:25.529, mean 00 00:00:00.019
step 71000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:22:35.479, mean 00 00:00:00.019
step 71500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:22:45.383, mean 00 00:00:00.019
step 72000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:22:55.279, mean 00 00:00:00.019
step 72500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:23:05.231, mean 00 00:00:00.019
step 73000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:23:15.309, mean 00 00:00:00.020
step 73500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:23:24.902, mean 00 00:00:00.019
step 74000: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:23:34.488, mean 00 00:00:00.019
step 74500: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:23:43.961, mean 00 00:00:00.018
step 74999: train loss NaN, val loss NaN, mem 1.8 GiB @ 00 00:23:53.460, mean 00 00:00:00.018
step 75000: train loss NaN, val loss NaN, @ 00 00:23:53.474, mean 00 00:00:00.019
Device = Device(CUDA,-1)
51585 parameters
learningRate = 3.0E-5
maxIterations = 75000
dropout = 0.2
step 0: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:00:00.000, mean 00 00:00:00.000
step 500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:00:09.241, mean 00 00:00:00.018
step 1000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:00:19.012, mean 00 00:00:00.019
step 1500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:00:28.175, mean 00 00:00:00.018
step 2000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:00:37.289, mean 00 00:00:00.018
step 2500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:00:46.475, mean 00 00:00:00.018
step 3000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:00:55.478, mean 00 00:00:00.018
step 3500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:01:04.561, mean 00 00:00:00.018
step 4000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:01:13.773, mean 00 00:00:00.018
step 4500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:01:22.942, mean 00 00:00:00.018
step 5000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:01:31.964, mean 00 00:00:00.018
step 5500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:01:41.126, mean 00 00:00:00.018
step 6000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:01:50.351, mean 00 00:00:00.018
step 6500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:01:59.527, mean 00 00:00:00.018
step 7000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:02:08.363, mean 00 00:00:00.017
step 7500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:02:17.324, mean 00 00:00:00.017
step 8000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:02:26.624, mean 00 00:00:00.018
step 8500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:02:35.480, mean 00 00:00:00.017
step 9000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:02:44.640, mean 00 00:00:00.018
step 9500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:02:53.859, mean 00 00:00:00.018
step 10000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:03:03.160, mean 00 00:00:00.018
step 10500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:03:12.378, mean 00 00:00:00.018
step 11000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:03:21.491, mean 00 00:00:00.018
step 11500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:03:30.857, mean 00 00:00:00.018
step 12000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:03:40.323, mean 00 00:00:00.018
step 12500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:03:49.805, mean 00 00:00:00.018
step 13000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:03:58.737, mean 00 00:00:00.017
step 13500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:04:07.950, mean 00 00:00:00.018
step 14000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:04:17.122, mean 00 00:00:00.018
step 14500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:04:26.466, mean 00 00:00:00.018
step 15000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:04:35.955, mean 00 00:00:00.018
step 15500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:04:45.278, mean 00 00:00:00.018
step 16000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:04:54.561, mean 00 00:00:00.018
step 16500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:05:03.955, mean 00 00:00:00.018
step 17000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:05:13.275, mean 00 00:00:00.018
step 17500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:05:22.504, mean 00 00:00:00.018
step 18000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:05:31.679, mean 00 00:00:00.018
step 18500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:05:41.235, mean 00 00:00:00.019
step 19000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:05:50.314, mean 00 00:00:00.018
step 19500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:05:59.746, mean 00 00:00:00.018
step 20000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:06:09.101, mean 00 00:00:00.018
step 20500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:06:18.622, mean 00 00:00:00.019
step 21000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:06:27.782, mean 00 00:00:00.018
step 21500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:06:36.929, mean 00 00:00:00.018
step 22000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:06:46.080, mean 00 00:00:00.018
step 22500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:06:55.419, mean 00 00:00:00.018
step 23000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:07:04.428, mean 00 00:00:00.018
step 23500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:07:13.586, mean 00 00:00:00.018
step 24000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:07:22.813, mean 00 00:00:00.018
step 24500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:07:32.061, mean 00 00:00:00.018
step 25000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:07:41.460, mean 00 00:00:00.018
step 25500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:07:50.855, mean 00 00:00:00.018
step 26000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:08:00.355, mean 00 00:00:00.018
step 26500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:08:09.657, mean 00 00:00:00.018
step 27000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:08:18.816, mean 00 00:00:00.018
step 27500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:08:28.294, mean 00 00:00:00.018
step 28000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:08:37.559, mean 00 00:00:00.018
step 28500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:08:46.822, mean 00 00:00:00.018
step 29000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:08:56.084, mean 00 00:00:00.018
step 29500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:09:05.344, mean 00 00:00:00.018
step 30000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:09:14.619, mean 00 00:00:00.018
step 30500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:09:23.873, mean 00 00:00:00.018
step 31000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:09:33.155, mean 00 00:00:00.018
step 31500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:09:42.346, mean 00 00:00:00.018
step 32000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:09:51.665, mean 00 00:00:00.018
step 32500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:10:00.959, mean 00 00:00:00.018
step 33000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:10:10.213, mean 00 00:00:00.018
step 33500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:10:19.507, mean 00 00:00:00.018
step 34000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:10:28.593, mean 00 00:00:00.018
step 34500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:10:37.863, mean 00 00:00:00.018
step 35000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:10:47.258, mean 00 00:00:00.018
step 35500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:10:56.503, mean 00 00:00:00.018
step 36000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:11:05.864, mean 00 00:00:00.018
step 36500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:11:15.118, mean 00 00:00:00.018
step 37000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:11:24.486, mean 00 00:00:00.018
step 37500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:11:33.865, mean 00 00:00:00.018
step 38000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:11:43.207, mean 00 00:00:00.018
step 38500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:11:52.704, mean 00 00:00:00.018
step 39000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:12:02.670, mean 00 00:00:00.019
step 39500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:12:12.272, mean 00 00:00:00.019
step 40000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:12:21.506, mean 00 00:00:00.018
step 40500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:12:30.996, mean 00 00:00:00.018
step 41000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:12:40.431, mean 00 00:00:00.018
step 41500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:12:49.775, mean 00 00:00:00.018
step 42000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:12:58.944, mean 00 00:00:00.018
step 42500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:13:08.523, mean 00 00:00:00.019
step 43000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:13:17.914, mean 00 00:00:00.018
step 43500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:13:26.633, mean 00 00:00:00.017
step 44000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:13:36.013, mean 00 00:00:00.018
step 44500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:13:45.143, mean 00 00:00:00.018
step 45000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:13:54.510, mean 00 00:00:00.018
step 45500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:14:03.784, mean 00 00:00:00.018
step 46000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:14:13.138, mean 00 00:00:00.018
step 46500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:14:21.990, mean 00 00:00:00.017
step 47000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:14:30.806, mean 00 00:00:00.017
step 47500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:14:39.801, mean 00 00:00:00.017
step 48000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:14:48.961, mean 00 00:00:00.018
step 48500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:14:58.189, mean 00 00:00:00.018
step 49000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:15:07.021, mean 00 00:00:00.017
step 49500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:15:15.834, mean 00 00:00:00.017
step 50000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:15:24.745, mean 00 00:00:00.017
step 50500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:15:33.590, mean 00 00:00:00.017
step 51000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:15:42.431, mean 00 00:00:00.017
step 51500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:15:51.473, mean 00 00:00:00.018
step 52000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:16:00.513, mean 00 00:00:00.018
step 52500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:16:09.597, mean 00 00:00:00.018
step 53000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:16:18.367, mean 00 00:00:00.017
step 53500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:16:27.361, mean 00 00:00:00.017
step 54000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:16:36.201, mean 00 00:00:00.017
step 54500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:16:44.882, mean 00 00:00:00.017
step 55000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:16:53.565, mean 00 00:00:00.017
step 55500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:17:02.142, mean 00 00:00:00.017
step 56000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:17:10.897, mean 00 00:00:00.017
step 56500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:17:19.579, mean 00 00:00:00.017
step 57000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:17:28.265, mean 00 00:00:00.017
step 57500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:17:36.960, mean 00 00:00:00.017
step 58000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:17:45.659, mean 00 00:00:00.017
step 58500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:17:54.417, mean 00 00:00:00.017
step 59000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:18:03.579, mean 00 00:00:00.018
step 59500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:18:12.768, mean 00 00:00:00.018
step 60000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:18:21.954, mean 00 00:00:00.018
step 60500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:18:31.108, mean 00 00:00:00.018
step 61000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:18:40.289, mean 00 00:00:00.018
step 61500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:18:49.497, mean 00 00:00:00.018
step 62000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:18:58.677, mean 00 00:00:00.018
step 62500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:19:07.823, mean 00 00:00:00.018
step 63000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:19:17.025, mean 00 00:00:00.018
step 63500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:19:26.217, mean 00 00:00:00.018
step 64000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:19:35.416, mean 00 00:00:00.018
step 64500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:19:44.643, mean 00 00:00:00.018
step 65000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:19:53.802, mean 00 00:00:00.018
step 65500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:20:02.976, mean 00 00:00:00.018
step 66000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:20:12.191, mean 00 00:00:00.018
step 66500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:20:21.380, mean 00 00:00:00.018
step 67000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:20:30.594, mean 00 00:00:00.018
step 67500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:20:39.827, mean 00 00:00:00.018
step 68000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:20:49.006, mean 00 00:00:00.018
step 68500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:20:57.962, mean 00 00:00:00.017
step 69000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:21:07.175, mean 00 00:00:00.018
step 69500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:21:16.365, mean 00 00:00:00.018
step 70000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:21:25.414, mean 00 00:00:00.018
step 70500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:21:34.167, mean 00 00:00:00.017
step 71000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:21:42.911, mean 00 00:00:00.017
step 71500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:21:51.633, mean 00 00:00:00.017
step 72000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:22:00.386, mean 00 00:00:00.017
step 72500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:22:09.128, mean 00 00:00:00.017
step 73000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:22:18.192, mean 00 00:00:00.018
step 73500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:22:27.385, mean 00 00:00:00.018
step 74000: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:22:36.578, mean 00 00:00:00.018
step 74500: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:22:45.768, mean 00 00:00:00.018
step 74999: train loss NaN, val loss NaN, mem 2.0 GiB @ 00 00:22:54.938, mean 00 00:00:00.018
step 75000: train loss NaN, val loss NaN, @ 00 00:22:54.953, mean 00 00:00:00.018
Exception in thread "main" java.lang.ExceptionInInitializerError
	at gpt.BiGram.main(BiGram.scala)
Caused by: java.lang.RuntimeException: probability tensor contains either `inf`, `nan` or element < 0
Exception raised from multinomial_out at /home/runner/work/javacpp-presets/javacpp-presets/pytorch/cppbuild/linux-x86_64-gpu/pytorch/aten/src/ATen/native/Distributions.cpp:617 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6c (0x7f07177b7d8c in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x84 (0x7f071777e07e in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libc10.so)
frame #2: at::native::multinomial_out(at::Tensor const&, long, bool, c10::optional<at::Generator>, at::Tensor&) + 0xe9a (0x7f06489e9a1a in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libtorch_cpu.so)
frame #3: at::native::multinomial(at::Tensor const&, long, bool, c10::optional<at::Generator>) + 0xb7 (0x7f06489e9cd7 in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libtorch_cpu.so)
frame #4: <unknown function> + 0x2cd6a84 (0x7f065538da84 in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libtorch_cuda.so)
frame #5: <unknown function> + 0x2cd6bbd (0x7f065538dbbd in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libtorch_cuda.so)
frame #6: at::_ops::multinomial::redispatch(c10::DispatchKeySet, at::Tensor const&, long, bool, c10::optional<at::Generator>) + 0x133 (0x7f064921d333 in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libtorch_cpu.so)
frame #7: <unknown function> + 0x3c566d7 (0x7f064af9c6d7 in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libtorch_cpu.so)
frame #8: at::_ops::multinomial::call(at::Tensor const&, long, bool, c10::optional<at::Generator>) + 0x207 (0x7f06492c0017 in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libtorch_cpu.so)
frame #9: Java_org_bytedeco_pytorch_global_torch_multinomial__Lorg_bytedeco_pytorch_Tensor_2JZLorg_bytedeco_pytorch_GeneratorOptional_2 + 0x10b (0x7f06e1e077ab in /home/vscode/.javacpp/cache/pytorch-2.0.1-1.5.10-20230923.234018-54-linux-x86_64-gpu.jar/org/bytedeco/pytorch/linux-x86_64-gpu/libjnitorch.so)
frame #10: [0x7f0919084db6]

	at org.bytedeco.pytorch.global.torch.multinomial(Native Method)
	at torch.ops.RandomSamplingOps.multinomial(RandomSamplingOps.scala:48)
	at torch.ops.RandomSamplingOps.multinomial$(RandomSamplingOps.scala:32)
	at torch.package$.multinomial(package.scala:30)
	at gpt.BiGram$BigramLanguageModel9.generate$$anonfun$10(BiGram.scala:2619)
	at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.scala:18)
	at scala.collection.immutable.Range.foreach(Range.scala:190)
	at gpt.BiGram$BigramLanguageModel9.generate(BiGram.scala:2621)
	at gpt.BiGram$.<clinit>(BiGram.scala:2653)
	... 1 more
1 targets failed
examples.runMain subprocess failed
