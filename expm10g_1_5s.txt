nohup: ignoring input
[info] compiling 2 Scala sources to /workspaces/storch/out/examples/compile.dest/classes ...
[warn] there were 8 deprecation warnings; re-run with -deprecation for details
[warn] one warning found
[info] done compiling
BiGram
Using device: Device(CUDA,-1)
File /workspaces/storch/data/input.txt already exists.
chars = 
,  , !, $, &, ', ,, -, ., 3, :, ;, ?, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z
vocab_size = 65
"BiGram!" = "BiGram!"
inputs:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
targets:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
----
xb:
Let's he
.
for that
yb:
et's hea
.
or that 
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39
when input is [57] the target: 43
when input is [57, 43] the target: 60
when input is [57, 43, 60] the target: 43
when input is [57, 43, 60, 43] the target: 52
when input is [57, 43, 60, 43, 52] the target: 1
when input is [57, 43, 60, 43, 52, 1] the target: 63
when input is [57, 43, 60, 43, 52, 1, 63] the target: 43
when input is [57, 43, 60, 43, 52, 1, 63, 43] the target: 39
when input is [60] the target: 43
when input is [60, 43] the target: 42
when input is [60, 43, 42] the target: 8
when input is [60, 43, 42, 8] the target: 0
when input is [60, 43, 42, 8, 0] the target: 25
when input is [60, 43, 42, 8, 0, 25] the target: 63
when input is [60, 43, 42, 8, 0, 25, 63] the target: 1
when input is [60, 43, 42, 8, 0, 25, 63, 1] the target: 45
when input is [56] the target: 42
when input is [56, 42] the target: 5
when input is [56, 42, 5] the target: 57
when input is [56, 42, 5, 57] the target: 1
when input is [56, 42, 5, 57, 1] the target: 57
when input is [56, 42, 5, 57, 1, 57] the target: 39
when input is [56, 42, 5, 57, 1, 57, 39] the target: 49
when input is [56, 42, 5, 57, 1, 57, 39, 49] the target: 43
when input is [43] the target: 57
when input is [43, 57] the target: 58
when input is [43, 57, 58] the target: 63
when input is [43, 57, 58, 63] the target: 6
when input is [43, 57, 58, 63, 6] the target: 1
when input is [43, 57, 58, 63, 6, 1] the target: 58
when input is [43, 57, 58, 63, 6, 1, 58] the target: 46
when input is [43, 57, 58, 63, 6, 1, 58, 46] the target: 47
when input is [43] the target: 1
when input is [43, 1] the target: 51
when input is [43, 1, 51] the target: 39
when input is [43, 1, 51, 39] the target: 63
when input is [43, 1, 51, 39, 63] the target: 1
when input is [43, 1, 51, 39, 63, 1] the target: 40
when input is [43, 1, 51, 39, 63, 1, 40] the target: 43
when input is [43, 1, 51, 39, 63, 1, 40, 43] the target: 1
when input is [58] the target: 46
when input is [58, 46] the target: 43
when input is [58, 46, 43] the target: 1
when input is [58, 46, 43, 1] the target: 43
when input is [58, 46, 43, 1, 43] the target: 39
when input is [58, 46, 43, 1, 43, 39] the target: 56
when input is [58, 46, 43, 1, 43, 39, 56] the target: 57
when input is [58, 46, 43, 1, 43, 39, 56, 57] the target: 10
when input is [39] the target: 58
when input is [39, 58] the target: 47
when input is [39, 58, 47] the target: 53
when input is [39, 58, 47, 53] the target: 52
when input is [39, 58, 47, 53, 52] the target: 12
when input is [39, 58, 47, 53, 52, 12] the target: 1
when input is [39, 58, 47, 53, 52, 12, 1] the target: 37
when input is [39, 58, 47, 53, 52, 12, 1, 37] the target: 53
when input is [53] the target: 56
when input is [53, 56] the target: 43
when input is [53, 56, 43] the target: 1
when input is [53, 56, 43, 1] the target: 21
when input is [53, 56, 43, 1, 21] the target: 1
when input is [53, 56, 43, 1, 21, 1] the target: 41
when input is [53, 56, 43, 1, 21, 1, 41] the target: 39
when input is [53, 56, 43, 1, 21, 1, 41, 39] the target: 51
when input is [50] the target: 39
when input is [50, 39] the target: 52
when input is [50, 39, 52] the target: 63
when input is [50, 39, 52, 63] the target: 1
when input is [50, 39, 52, 63, 1] the target: 47
when input is [50, 39, 52, 63, 1, 47] the target: 58
when input is [50, 39, 52, 63, 1, 47, 58] the target: 57
when input is [50, 39, 52, 63, 1, 47, 58, 57] the target: 43
when input is [56] the target: 53
when input is [56, 53] the target: 63
when input is [56, 53, 63] the target: 1
when input is [56, 53, 63, 1] the target: 42
when input is [56, 53, 63, 1, 42] the target: 47
when input is [56, 53, 63, 1, 42, 47] the target: 42
when input is [56, 53, 63, 1, 42, 47, 42] the target: 1
when input is [56, 53, 63, 1, 42, 47, 42, 1] the target: 57
when input is [39] the target: 51
when input is [39, 51] the target: 1
when input is [39, 51, 1] the target: 39
when input is [39, 51, 1, 39] the target: 44
when input is [39, 51, 1, 39, 44] the target: 56
when input is [39, 51, 1, 39, 44, 56] the target: 39
when input is [39, 51, 1, 39, 44, 56, 39] the target: 47
when input is [39, 51, 1, 39, 44, 56, 39, 47] the target: 42
when input is [17] the target: 24
when input is [17, 24] the target: 21
when input is [17, 24, 21] the target: 38
when input is [17, 24, 21, 38] the target: 13
when input is [17, 24, 21, 38, 13] the target: 14
when input is [17, 24, 21, 38, 13, 14] the target: 17
when input is [17, 24, 21, 38, 13, 14, 17] the target: 32
when input is [17, 24, 21, 38, 13, 14, 17, 32] the target: 20
xb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
yb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
xb (set Device(CUDA,-1)):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
loss0 = tensor dtype=float32, shape=[], device=CPU 
2.1746
loss1 = tensor dtype=float32, shape=[], device=CPU 
1.9668
loss2 = tensor dtype=float32, shape=[], device=CPU 
1.8103
batch_size * block_size = 128
logits.shape = ArraySeq(128, 65)
loss m0 = 4.6391506
decode:'
y
lX$fDkRZ,
dco?f,Zh,OLFb,e&sK
;:iPTCmLBbzA$3:.aS&JO-3GSMwF?gLTaUhXFY'X3FhhMNuwq&J,K$.t?VrYdX3rDoa'e'
4.624553
decode 2:'
NAwMEQPgKWxvfDEZa3rxzkkNQ:
YoR&$FMtofVimE;q$!BAm$W;$dYlM!Rueg ixveesY3hcieOlxS&HFG?Zrov E;,,,BeqWk Gn&hD!.vrWjco!pkAJljndGUVQu.C-Ax;ZqPScwlDN:pSsO;?Oee&X3Uwty.vwlvBmUHI.
Bm&pjXPggvwE;qPgDGyqwJ'l
lXSkkqyoaW-;s;&FbrVCeIib3Hr'Tab-&fM$HZqETCgK
hieKqyOp-Lj3gAg-;T3H
hohkOxvFvFrkgW&A Lkk;3Hrkh!Bm:f't,Cdy$flMUE;,wYfMfMPrD?UqY'S?U.JaHK-NLbE!ar,
yb&h&:w:adspbWP$!BE;DxsYBtuicJKNtk&Jar?Any-Rr-Ibs-I&fym&EZ!NMJk'QNEZFEAk3RJ3&.JA-IXq'RO3GROePm !BCy
;emWsNBmeXnxugpVqweV-e&ArXaJR?;$HOzx;jWX$.Ct'cUlugUbxQEOT$Tqrc'
step 0: train loss 4.593183, val loss 4.556398
decode 3:'
$ Dfspy&psStz&$UD l..N
EEiasAvJ?mVp ijqsjEoYSWXpPxAbN
Ymov3tL-Z?ACa3!3LxXCPxsFHkp-vm;YHKieHP-HnmdgufWxVO?eRUC$;Lx:yhD$ZYCCN3gscUFw?c$YmSu3idhMUeUq,FXoxlgqKG!ZcS?'3aak-&OcXavzc-E&F''3:O k ! .vDCBUmlxnFm,CMqJ:N
ZlgWS?'PCkvy,wNF'vkdIiGZ-ADNpIHxdk
$HqZC&X$GiU,LxXCD?mFyvkeHRI,zHoJxMiuGoKtQDCn?DKt.e C3tm, kYpQ;tG!oJPs-b.AengdgNtyc$zkDU3EFBlTQJbkeHPYcUrAqMO
FwD;SLx.gTBwht-g&LXvY$W'ZtT
TWL:Jc;qylxkpw?GoCeMTI3tyLBv.NuwpA.NaFQiWScQOwHRnu;wg.PSLMRd&c&UD ,CL3g,X LYf;a;SDXan$:CKayNuJIs?E
g

EM:,Fme&3vvmSBLsO'
a=tensor dtype=float32, shape=[3, 3], device=CPU 
[[1.0000, 0.0000, 0.0000],
 [0.5000, 0.5000, 0.0000],
 [0.3333, 0.3333, 0.3333]]
--
b=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [6.0000, 4.0000],
 [6.0000, 5.0000]]
--
c=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [4.0000, 5.5000],
 [4.6667, 5.3333]]
ArraySeq(4, 8, 2)
wei0.shape = ArraySeq(8, 8)
true
true
Token embedding: BigramLanguageModel1
4225 parameters
BigramLanguageModel1: #3 4225 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
step 0: train loss 4.346576, val loss 4.3445053
decode 4:'
?q$;xfDkRZkNdc'wb,ZTkOLOT,eCtK
bHxPj&kMBbzA$3:.aSKgO-33SMBc?gcTa
hX;YV HtpXeNuwqcPkxv.tbar dXl!DZaLeWuwccHPmREx,fDEdnYzxzCWNuX
Yo3&$LMtofXiEIvBE!&V!$W;Kd!lHx,ae3 irweYERnIciK;lSW;HFGAZroG EsSXUB;qWklJ.gGD-.CyWjbH!pelJlinFAp;av.C-huDZqoVchvVy:pUup;Mais'X3UwtyfMJ'vBPUuI.3BmTpaY-iMvIEjqkpD:lqwJclmBtSkklmoaW-nNA&QPdVCeIib3Tw'TSEG&fM$HZLETcg$
hxJ$AsLC-LK3gAN-xTrA
XeLkXMmnvnrufWqA s
;;3;QDLWTm:fvtwgdy.vlMUE$Tw,fMfMPrD?CXYIS?B.KrHK-NLbE!rs,dyb&i&a
aadKabWPh!JEgDFHYBhuihVKN.M?DUrAAnyHRrxfbsmc&fy &Ec!NMJ'
Token + positional embedding: BigramLanguageModel2
4481 parameters
BigramLanguageModel2: #4 4481 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
decode 5:'
k'nEEPFrPkULmKYy.AYHXq'WO3;R
S?m !b&Hx;EgWsNB-r?KXm;FVqqrxmiYArSaJR?;$H-zgKjOhBGC?' EwugybxIE.T$Jmuc$ yfv:y&tsSFD&cYsgJ.m 
EEiasmGJtlMpKSjTkXxsLueIpPTAbN'kmlvMkL-Z?AC-?!3LRoCPTmFFkm-vX;YHKieO:PHuEEgusGxVO?gRz,XALI:ytb$ZGCCI!gscPkn?iKYUj,; QhRUedq,FsoxmgqjGhZcE!HbAakw!O?gwvzc-E.
'ww3C k ! .vPCBuml3NFm,CRz!:NUZlhWIvNPGiIyBOYFkvLhIisZ-A?NdI3idk
bHpZF&XnGenmLzXCD?tFymk?HLIYzqoY3MiuGdKtLoCnijTv.e A3AmN xYpDytGFoxPwMbLC?KgviPt c$zkDG3EiBlTQlbkmHl!P&sSqMO
F&X;fL,.cTjwrtc,&LiuY$WxZtTXTWO;!u;qylCkW;gGoSe'
ArraySeq(4, 8, 16)
tensor dtype=float32, shape=[8, 8], device=CPU 
[[1.0000, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.1574, 0.8426, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.2088, 0.1646, 0.6266, ..., 0.0000, 0.0000, 0.0000],
 ...,
 [0.0176, 0.2689, 0.0215, ..., 0.0019, 0.0000, 0.0000],
 [0.1691, 0.4066, 0.0438, ..., 0.2012, 0.0329, 0.0000],
 [0.0210, 0.0843, 0.0555, ..., 0.0709, 0.2423, 0.2391]]
tensor dtype=float32, shape=[], device=CPU 
1.0449
tensor dtype=float32, shape=[], device=CPU 
1.0700
tensor dtype=float32, shape=[], device=CPU 
17.4690
tensor dtype=float32, shape=[], device=CPU 
1.0918
tensor dtype=float64, shape=[5], device=CPU 
[0.1925, 0.1426, 0.2351, 0.1426, 0.2872]
tensor dtype=float64, shape=[5], device=CPU 
[0.0326, 0.0030, 0.1615, 0.0030, 0.8000]
Single head attention: BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head_1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
step 0: train loss 4.1745915, val loss 4.1752186
step 0: train loss 4.166315, val loss 4.169002
decode 6:'







?qfXxfDkRZkNwc.wj,ZTkOLFT,ebtK
b:!PjCkMBbzA$3:.aSvgO-33SM:F?gLTa
hX:YVXJthXfNuwqcPMxG.tbar dXl!DZaLeWFwccHPmRWk,fDEZaYzxzCImuX
YoR&$LMtofViEIvB!!&V!$W;KdYlNZ,ue3 ixYeYEYnkciK;lxW;HFGEZroG EsSXUB;qWk G..GD!.FyWjbm!pelJljnFFUVcu.C-huD3qcnchvVy:?Uup;Mnis'X3Uwty.OJlvBPUHI.yBfTpjY-lgvIEjqk:DGyqwJdlNBtSkklmoaW-CNA&QPdVCeIib3sI'TStG&dE$HZLETxN$Fhx&$FsgC-LKKgAe-xT3H
hexkNVmnvnrufW&A '
;;3;QDL!Tm:fEE,Cey$alPUE$tw,fMFEPRD?UqYIS?m.UrHK-NLuk!aK,iyb&i&:
aadsaUWG$!VE'DFsYBvuihVKN.k?Dar?AnyHRr-utsmI&fn VEc!NMJ'
Single head attention (b): BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head_1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
Multi-head attention BigramLanguageModel4
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
10625 parameters
BigramLanguageModel4: #28 10625 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Multi-head attention + FFWD BigramLanguageModel5
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
11681 parameters
BigramLanguageModel5: #30 11681 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  ffwd: FeedFoward(nEmbed = 32): #2 1056 (
    net: Sequential: #2 1056 (
      0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      1: ReLU: #0 <> 
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks BigramLanguageModel6
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
26081 parameters
BigramLanguageModel6: #82 26081 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #78 21600 (
    0: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    1: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    2: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections - BigramLanguageModel7
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
51137 parameters
BigramLanguageModel7: #94 51137 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #90 46656 (
    0: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    1: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    2: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections + layer norm - BigramLanguageModel8
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
51585 parameters
BigramLanguageModel8: #108 51585 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #104 47104 (
    0: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    1: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    2: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    3: LayerNorm: #2 <32,32> 
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections + layer norm + Dropout - BigramLanguageModel9
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
51585 parameters
BigramLanguageModel9: #108 51585 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #102 47040 (
    0: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    1: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    2: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
  )
  ln_f: LayerNorm: #2 <32,32> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Device = Device(CUDA,-1)
51585 parameters
learningRate = 1.1E-5
maxIterations = 1000000
dropout = 0.2
step 0: train loss 4.3073688, val loss 4.316937, mem 1.2 GiB @ 00 00:00:00.000, mean 00 00:00:00.000
step 500: train loss 3.9592516, val loss 3.977592, mem 1.2 GiB @ 00 00:00:09.140, mean 00 00:00:00.018
step 1000: train loss 3.715615, val loss 3.7437103, mem 1.2 GiB @ 00 00:00:18.267, mean 00 00:00:00.018
step 1500: train loss 3.596614, val loss 3.6149218, mem 1.3 GiB @ 00 00:00:27.161, mean 00 00:00:00.017
step 2000: train loss 3.4998028, val loss 3.5305223, mem 1.6 GiB @ 00 00:00:36.135, mean 00 00:00:00.017
step 2500: train loss 3.4294884, val loss 3.458459, mem 1.8 GiB @ 00 00:00:45.380, mean 00 00:00:00.018
step 3000: train loss 3.370824, val loss 3.4028509, mem 1.8 GiB @ 00 00:00:54.542, mean 00 00:00:00.018
step 3500: train loss 3.3235397, val loss 3.3447766, mem 1.8 GiB @ 00 00:01:03.495, mean 00 00:00:00.017
step 4000: train loss 3.2978084, val loss 3.3568785, mem 1.8 GiB @ 00 00:01:12.292, mean 00 00:00:00.017
step 4500: train loss 3.2660065, val loss 3.3018878, mem 1.8 GiB @ 00 00:01:21.066, mean 00 00:00:00.017
step 5000: train loss 3.2109761, val loss 3.2667475, mem 1.8 GiB @ 00 00:01:29.725, mean 00 00:00:00.017
step 5500: train loss 3.1943944, val loss 3.2382536, mem 1.8 GiB @ 00 00:01:38.551, mean 00 00:00:00.017
step 6000: train loss 3.1540446, val loss 3.1984346, mem 1.8 GiB @ 00 00:01:47.327, mean 00 00:00:00.017
step 6500: train loss 3.1436038, val loss 3.1818268, mem 1.8 GiB @ 00 00:01:56.133, mean 00 00:00:00.017
step 7000: train loss 3.1223052, val loss 3.162098, mem 1.8 GiB @ 00 00:02:05.170, mean 00 00:00:00.018
step 7500: train loss 3.115147, val loss 3.155515, mem 1.8 GiB @ 00 00:02:14.086, mean 00 00:00:00.017
step 8000: train loss 3.0966313, val loss 3.152597, mem 1.8 GiB @ 00 00:02:23.103, mean 00 00:00:00.018
step 8500: train loss 3.0898612, val loss 3.126668, mem 1.8 GiB @ 00 00:02:32.256, mean 00 00:00:00.018
step 9000: train loss 3.0684192, val loss 3.1087983, mem 1.8 GiB @ 00 00:02:41.362, mean 00 00:00:00.018
step 9500: train loss 3.0516574, val loss 3.092061, mem 1.8 GiB @ 00 00:02:50.507, mean 00 00:00:00.018
step 10000: train loss 3.021613, val loss 3.075371, mem 1.8 GiB @ 00 00:02:59.555, mean 00 00:00:00.018
step 10500: train loss 3.0006518, val loss 3.0479271, mem 1.8 GiB @ 00 00:03:08.682, mean 00 00:00:00.018
step 11000: train loss 3.0383403, val loss 3.0510216, mem 1.8 GiB @ 00 00:03:17.819, mean 00 00:00:00.018
step 11500: train loss 3.0418565, val loss 3.0783265, mem 1.8 GiB @ 00 00:03:26.959, mean 00 00:00:00.018
step 12000: train loss 3.0426192, val loss 3.0730977, mem 1.8 GiB @ 00 00:03:36.010, mean 00 00:00:00.018
step 12500: train loss 3.0170755, val loss 3.0353246, mem 1.8 GiB @ 00 00:03:45.169, mean 00 00:00:00.018
step 13000: train loss 3.0042543, val loss 3.0601287, mem 1.8 GiB @ 00 00:03:54.319, mean 00 00:00:00.018
step 13500: train loss 2.993733, val loss 3.0222845, mem 1.8 GiB @ 00 00:04:03.451, mean 00 00:00:00.018
step 14000: train loss 2.9869788, val loss 3.008951, mem 1.8 GiB @ 00 00:04:12.522, mean 00 00:00:00.018
step 14500: train loss 2.9511907, val loss 2.9963062, mem 1.8 GiB @ 00 00:04:21.668, mean 00 00:00:00.018
step 15000: train loss 2.9429016, val loss 2.9817522, mem 1.8 GiB @ 00 00:04:30.806, mean 00 00:00:00.018
step 15500: train loss 2.9223013, val loss 2.9756987, mem 1.8 GiB @ 00 00:04:40.043, mean 00 00:00:00.018
step 16000: train loss 2.9328365, val loss 2.9463506, mem 1.8 GiB @ 00 00:04:49.215, mean 00 00:00:00.018
step 16500: train loss 2.9015386, val loss 2.9193578, mem 1.8 GiB @ 00 00:04:58.022, mean 00 00:00:00.017
step 17000: train loss 2.873179, val loss 2.9265108, mem 1.8 GiB @ 00 00:05:06.797, mean 00 00:00:00.017
step 17500: train loss 2.8658159, val loss 2.9062383, mem 1.8 GiB @ 00 00:05:15.637, mean 00 00:00:00.017
step 18000: train loss 2.865797, val loss 2.9027765, mem 1.8 GiB @ 00 00:05:24.463, mean 00 00:00:00.017
step 18500: train loss 2.86601, val loss 2.9082897, mem 1.8 GiB @ 00 00:05:33.238, mean 00 00:00:00.017
step 19000: train loss 2.829333, val loss 2.8761284, mem 1.8 GiB @ 00 00:05:42.012, mean 00 00:00:00.017
step 19500: train loss 2.8465242, val loss 2.868509, mem 1.8 GiB @ 00 00:05:50.838, mean 00 00:00:00.017
step 20000: train loss 2.8443859, val loss 2.8489935, mem 1.8 GiB @ 00 00:05:59.654, mean 00 00:00:00.017
step 20500: train loss 2.8270953, val loss 2.8378568, mem 1.8 GiB @ 00 00:06:08.505, mean 00 00:00:00.017
step 21000: train loss 2.7999272, val loss 2.8219116, mem 1.8 GiB @ 00 00:06:17.523, mean 00 00:00:00.018
step 21500: train loss 2.8045897, val loss 2.8286037, mem 1.8 GiB @ 00 00:06:26.696, mean 00 00:00:00.018
step 22000: train loss 2.773002, val loss 2.811199, mem 1.8 GiB @ 00 00:06:35.858, mean 00 00:00:00.018
step 22500: train loss 2.7767754, val loss 2.8159413, mem 1.8 GiB @ 00 00:06:45.006, mean 00 00:00:00.018
step 23000: train loss 2.762655, val loss 2.7874453, mem 1.8 GiB @ 00 00:06:54.143, mean 00 00:00:00.018
step 23500: train loss 2.7637787, val loss 2.7817037, mem 1.8 GiB @ 00 00:07:03.299, mean 00 00:00:00.018
step 24000: train loss 2.7372284, val loss 2.7716873, mem 1.8 GiB @ 00 00:07:12.449, mean 00 00:00:00.018
step 24500: train loss 2.7274992, val loss 2.7483497, mem 1.8 GiB @ 00 00:07:21.478, mean 00 00:00:00.018
step 25000: train loss 2.7187293, val loss 2.7450135, mem 1.8 GiB @ 00 00:07:30.602, mean 00 00:00:00.018
step 25500: train loss 2.7084513, val loss 2.7314165, mem 1.8 GiB @ 00 00:07:39.752, mean 00 00:00:00.018
step 26000: train loss 2.7068517, val loss 2.731223, mem 1.8 GiB @ 00 00:07:48.890, mean 00 00:00:00.018
step 26500: train loss 2.690652, val loss 2.7197495, mem 1.8 GiB @ 00 00:07:58.019, mean 00 00:00:00.018
step 27000: train loss 2.6831949, val loss 2.686125, mem 1.8 GiB @ 00 00:08:07.136, mean 00 00:00:00.018
step 27500: train loss 2.6752384, val loss 2.7125823, mem 1.8 GiB @ 00 00:08:16.281, mean 00 00:00:00.018
step 28000: train loss 2.6659114, val loss 2.6888177, mem 1.8 GiB @ 00 00:08:25.459, mean 00 00:00:00.018
step 28500: train loss 2.6615272, val loss 2.7007318, mem 1.8 GiB @ 00 00:08:34.508, mean 00 00:00:00.018
step 29000: train loss 2.6656833, val loss 2.6711822, mem 1.8 GiB @ 00 00:08:43.389, mean 00 00:00:00.017
step 29500: train loss 2.6509368, val loss 2.6808245, mem 1.8 GiB @ 00 00:08:52.273, mean 00 00:00:00.017
step 30000: train loss 2.6369956, val loss 2.6631114, mem 1.8 GiB @ 00 00:09:01.420, mean 00 00:00:00.018
step 30500: train loss 2.641143, val loss 2.65175, mem 1.8 GiB @ 00 00:09:10.212, mean 00 00:00:00.017
step 31000: train loss 2.6302168, val loss 2.6616206, mem 1.8 GiB @ 00 00:09:18.923, mean 00 00:00:00.017
step 31500: train loss 2.6138964, val loss 2.6667273, mem 1.8 GiB @ 00 00:09:27.752, mean 00 00:00:00.017
step 32000: train loss 2.5994961, val loss 2.641593, mem 1.8 GiB @ 00 00:09:36.565, mean 00 00:00:00.017
step 32500: train loss 2.6433585, val loss 2.6718073, mem 1.8 GiB @ 00 00:09:45.319, mean 00 00:00:00.017
step 33000: train loss 2.7551706, val loss 2.7609503, mem 1.8 GiB @ 00 00:09:54.092, mean 00 00:00:00.017
step 33500: train loss 2.7306383, val loss 2.7558124, mem 1.8 GiB @ 00 00:10:02.793, mean 00 00:00:00.017
step 34000: train loss 2.698046, val loss 2.7147593, mem 1.8 GiB @ 00 00:10:11.632, mean 00 00:00:00.017
step 34500: train loss 2.6830144, val loss 2.6744595, mem 1.8 GiB @ 00 00:10:20.449, mean 00 00:00:00.017
step 35000: train loss 2.6519732, val loss 2.6827936, mem 1.8 GiB @ 00 00:10:29.239, mean 00 00:00:00.017
step 35500: train loss 2.6398041, val loss 2.6583354, mem 1.8 GiB @ 00 00:10:38.024, mean 00 00:00:00.017
step 36000: train loss 2.6431286, val loss 2.6221435, mem 1.8 GiB @ 00 00:10:46.886, mean 00 00:00:00.017
step 36500: train loss 2.6269207, val loss 2.642417, mem 1.8 GiB @ 00 00:10:55.608, mean 00 00:00:00.017
step 37000: train loss 2.6165063, val loss 2.6312854, mem 1.8 GiB @ 00 00:11:04.429, mean 00 00:00:00.017
step 37500: train loss 2.6062055, val loss 2.6246195, mem 1.8 GiB @ 00 00:11:13.478, mean 00 00:00:00.018
step 38000: train loss 2.6054747, val loss 2.616462, mem 1.8 GiB @ 00 00:11:22.623, mean 00 00:00:00.018
step 38500: train loss 2.5955696, val loss 2.6119874, mem 1.8 GiB @ 00 00:11:31.681, mean 00 00:00:00.018
step 39000: train loss 2.5806978, val loss 2.5928185, mem 1.8 GiB @ 00 00:11:40.549, mean 00 00:00:00.017
step 39500: train loss 2.569532, val loss 2.5891259, mem 1.8 GiB @ 00 00:11:49.339, mean 00 00:00:00.017
step 40000: train loss 2.5526125, val loss 2.5957406, mem 1.8 GiB @ 00 00:11:58.188, mean 00 00:00:00.017
step 40500: train loss 2.552939, val loss 2.5824096, mem 1.8 GiB @ 00 00:12:06.945, mean 00 00:00:00.017
step 41000: train loss 2.5520825, val loss 2.5569441, mem 1.8 GiB @ 00 00:12:15.791, mean 00 00:00:00.017
step 41500: train loss 2.562965, val loss 2.5623662, mem 1.8 GiB @ 00 00:12:24.550, mean 00 00:00:00.017
step 42000: train loss 2.551798, val loss 2.5599592, mem 1.8 GiB @ 00 00:12:33.341, mean 00 00:00:00.017
step 42500: train loss 2.541356, val loss 2.55509, mem 1.8 GiB @ 00 00:12:42.199, mean 00 00:00:00.017
step 43000: train loss 2.525011, val loss 2.5562851, mem 1.8 GiB @ 00 00:12:51.260, mean 00 00:00:00.018
step 43500: train loss 2.5328538, val loss 2.5567296, mem 1.8 GiB @ 00 00:13:00.402, mean 00 00:00:00.018
step 44000: train loss 2.538048, val loss 2.5479329, mem 1.8 GiB @ 00 00:13:09.532, mean 00 00:00:00.018
step 44500: train loss 2.5158126, val loss 2.5333498, mem 1.8 GiB @ 00 00:13:18.565, mean 00 00:00:00.018
step 45000: train loss 2.5041509, val loss 2.5455177, mem 1.8 GiB @ 00 00:13:27.711, mean 00 00:00:00.018
step 45500: train loss 2.521844, val loss 2.5334342, mem 1.8 GiB @ 00 00:13:36.843, mean 00 00:00:00.018
step 46000: train loss 2.5047042, val loss 2.51085, mem 1.8 GiB @ 00 00:13:45.973, mean 00 00:00:00.018
step 46500: train loss 2.5154817, val loss 2.5255446, mem 1.8 GiB @ 00 00:13:55.104, mean 00 00:00:00.018
step 47000: train loss 2.5132585, val loss 2.5144482, mem 1.8 GiB @ 00 00:14:04.236, mean 00 00:00:00.018
step 47500: train loss 2.4986033, val loss 2.513545, mem 1.8 GiB @ 00 00:14:13.382, mean 00 00:00:00.018
step 48000: train loss 2.4974039, val loss 2.5194957, mem 1.8 GiB @ 00 00:14:22.534, mean 00 00:00:00.018
step 48500: train loss 2.499077, val loss 2.4979317, mem 1.8 GiB @ 00 00:14:31.680, mean 00 00:00:00.018
step 49000: train loss 2.494089, val loss 2.5135453, mem 1.8 GiB @ 00 00:14:40.853, mean 00 00:00:00.018
step 49500: train loss 2.4918337, val loss 2.5041833, mem 1.8 GiB @ 00 00:14:49.984, mean 00 00:00:00.018
step 50000: train loss 2.4759288, val loss 2.510464, mem 1.8 GiB @ 00 00:14:59.147, mean 00 00:00:00.018
step 50500: train loss 2.6310718, val loss 2.6383982, mem 1.8 GiB @ 00 00:15:08.213, mean 00 00:00:00.018
step 51000: train loss 2.6470892, val loss 2.6748931, mem 1.8 GiB @ 00 00:15:17.363, mean 00 00:00:00.018
step 51500: train loss 2.6358736, val loss 2.6528823, mem 1.8 GiB @ 00 00:15:26.498, mean 00 00:00:00.018
step 52000: train loss 2.6157653, val loss 2.6131263, mem 1.8 GiB @ 00 00:15:35.638, mean 00 00:00:00.018
step 52500: train loss 2.607855, val loss 2.5906339, mem 1.8 GiB @ 00 00:15:44.693, mean 00 00:00:00.018
step 53000: train loss 2.5855749, val loss 2.5907097, mem 1.8 GiB @ 00 00:15:53.826, mean 00 00:00:00.018
step 53500: train loss 2.5880632, val loss 2.5796597, mem 1.8 GiB @ 00 00:16:03.017, mean 00 00:00:00.018
step 54000: train loss 2.5629332, val loss 2.5718193, mem 1.8 GiB @ 00 00:16:12.086, mean 00 00:00:00.018
step 54500: train loss 2.5452766, val loss 2.5525646, mem 1.8 GiB @ 00 00:16:21.244, mean 00 00:00:00.018
step 55000: train loss 3.197227, val loss 3.2244627, mem 1.8 GiB @ 00 00:16:30.408, mean 00 00:00:00.018
step 55500: train loss 3.3303974, val loss 3.3461277, mem 1.8 GiB @ 00 00:16:39.704, mean 00 00:00:00.018
step 56000: train loss 3.353302, val loss 3.3867157, mem 1.8 GiB @ 00 00:16:49.035, mean 00 00:00:00.018
step 56500: train loss 3.3490622, val loss 3.3741145, mem 1.8 GiB @ 00 00:16:58.292, mean 00 00:00:00.018
step 57000: train loss 3.3295305, val loss 3.355035, mem 1.8 GiB @ 00 00:17:07.616, mean 00 00:00:00.018
step 57500: train loss 3.3329644, val loss 3.3216076, mem 1.8 GiB @ 00 00:17:17.003, mean 00 00:00:00.018
step 58000: train loss 3.2971609, val loss 3.348957, mem 1.8 GiB @ 00 00:17:26.275, mean 00 00:00:00.018
step 58500: train loss 3.3010714, val loss 3.3314412, mem 1.8 GiB @ 00 00:17:35.390, mean 00 00:00:00.018
step 59000: train loss 3.2932336, val loss 3.3057666, mem 1.8 GiB @ 00 00:17:44.626, mean 00 00:00:00.018
step 59500: train loss 3.2595775, val loss 3.3144655, mem 1.8 GiB @ 00 00:17:53.409, mean 00 00:00:00.017
step 60000: train loss 3.2889044, val loss 3.3138356, mem 1.8 GiB @ 00 00:18:02.522, mean 00 00:00:00.018
step 60500: train loss 3.2576146, val loss 3.311734, mem 1.8 GiB @ 00 00:18:11.499, mean 00 00:00:00.017
step 61000: train loss 3.2537472, val loss 3.298773, mem 1.8 GiB @ 00 00:18:20.674, mean 00 00:00:00.018
step 61500: train loss 3.2550368, val loss 3.2964923, mem 1.8 GiB @ 00 00:18:29.602, mean 00 00:00:00.017
step 62000: train loss 3.2465417, val loss 3.2837818, mem 1.8 GiB @ 00 00:18:38.810, mean 00 00:00:00.018
step 62500: train loss 3.2310824, val loss 3.2696264, mem 1.8 GiB @ 00 00:18:48.038, mean 00 00:00:00.018
step 63000: train loss 3.2145374, val loss 3.254098, mem 1.8 GiB @ 00 00:18:57.069, mean 00 00:00:00.018
step 63500: train loss 3.2275226, val loss 3.259315, mem 1.8 GiB @ 00 00:19:06.091, mean 00 00:00:00.018
step 64000: train loss 3.2203043, val loss 3.2300975, mem 1.8 GiB @ 00 00:19:14.849, mean 00 00:00:00.017
step 64500: train loss 3.219921, val loss 3.2258348, mem 1.8 GiB @ 00 00:19:24.042, mean 00 00:00:00.018
step 65000: train loss 3.2368999, val loss 3.2454708, mem 1.8 GiB @ 00 00:19:33.083, mean 00 00:00:00.018
step 65500: train loss 3.2492743, val loss 3.2489429, mem 1.8 GiB @ 00 00:19:42.030, mean 00 00:00:00.017
step 66000: train loss 3.170556, val loss 3.2157943, mem 1.8 GiB @ 00 00:19:50.972, mean 00 00:00:00.017
step 66500: train loss 3.1974034, val loss 3.2048967, mem 1.8 GiB @ 00 00:20:00.285, mean 00 00:00:00.018
step 67000: train loss 3.1939056, val loss 3.1933115, mem 1.8 GiB @ 00 00:20:09.640, mean 00 00:00:00.018
step 67500: train loss 3.1850555, val loss 3.2009404, mem 1.8 GiB @ 00 00:20:18.932, mean 00 00:00:00.018
step 68000: train loss 3.201627, val loss 3.2040443, mem 1.8 GiB @ 00 00:20:28.276, mean 00 00:00:00.018
step 68500: train loss 3.1707454, val loss 3.1812968, mem 1.8 GiB @ 00 00:20:37.790, mean 00 00:00:00.019
step 69000: train loss 3.159153, val loss 3.1796515, mem 1.8 GiB @ 00 00:20:46.973, mean 00 00:00:00.018
step 69500: train loss 3.147832, val loss 3.1806521, mem 1.8 GiB @ 00 00:20:56.116, mean 00 00:00:00.018
step 70000: train loss 3.1764967, val loss 3.1623132, mem 1.8 GiB @ 00 00:21:05.453, mean 00 00:00:00.018
step 70500: train loss 3.1450958, val loss 3.1487372, mem 1.8 GiB @ 00 00:21:14.800, mean 00 00:00:00.018
step 71000: train loss 3.1394732, val loss 3.1614947, mem 1.8 GiB @ 00 00:21:23.559, mean 00 00:00:00.017
step 71500: train loss 3.1103215, val loss 3.1541479, mem 1.8 GiB @ 00 00:21:32.346, mean 00 00:00:00.017
step 72000: train loss 3.1044252, val loss 3.146626, mem 1.8 GiB @ 00 00:21:40.998, mean 00 00:00:00.017
step 72500: train loss 3.0982955, val loss 3.1416392, mem 1.8 GiB @ 00 00:21:49.728, mean 00 00:00:00.017
step 73000: train loss 3.1109335, val loss 3.0992322, mem 1.8 GiB @ 00 00:21:58.522, mean 00 00:00:00.017
step 73500: train loss 3.0978112, val loss 3.1393933, mem 1.8 GiB @ 00 00:22:07.312, mean 00 00:00:00.017
step 74000: train loss 3.118739, val loss 3.1292796, mem 1.8 GiB @ 00 00:22:16.073, mean 00 00:00:00.017
step 74500: train loss 3.098095, val loss 3.1075485, mem 1.8 GiB @ 00 00:22:24.811, mean 00 00:00:00.017
step 75000: train loss 3.0903409, val loss 3.1078382, mem 1.8 GiB @ 00 00:22:33.563, mean 00 00:00:00.017
step 75500: train loss 3.0711548, val loss 3.08694, mem 1.8 GiB @ 00 00:22:42.174, mean 00 00:00:00.017
step 76000: train loss 3.0921483, val loss 3.0783706, mem 1.8 GiB @ 00 00:22:50.883, mean 00 00:00:00.017
step 76500: train loss 3.064862, val loss 3.0862775, mem 1.8 GiB @ 00 00:22:59.581, mean 00 00:00:00.017
step 77000: train loss 3.0591455, val loss 3.0698123, mem 1.8 GiB @ 00 00:23:08.297, mean 00 00:00:00.017
step 77500: train loss 3.0384552, val loss 3.0596204, mem 1.8 GiB @ 00 00:23:17.061, mean 00 00:00:00.017
step 78000: train loss 3.028604, val loss 3.068567, mem 1.8 GiB @ 00 00:23:25.822, mean 00 00:00:00.017
step 78500: train loss 3.0443594, val loss 3.0423698, mem 1.8 GiB @ 00 00:23:34.548, mean 00 00:00:00.017
step 79000: train loss 3.0815897, val loss 3.040118, mem 1.8 GiB @ 00 00:23:43.287, mean 00 00:00:00.017
step 79500: train loss 3.021563, val loss 3.0343835, mem 1.8 GiB @ 00 00:23:52.077, mean 00 00:00:00.017
step 80000: train loss 3.0129397, val loss 3.0080264, mem 1.8 GiB @ 00 00:24:00.727, mean 00 00:00:00.017
step 80500: train loss 2.996956, val loss 3.0067034, mem 1.8 GiB @ 00 00:24:09.483, mean 00 00:00:00.017
step 81000: train loss 2.9955175, val loss 2.9818687, mem 1.8 GiB @ 00 00:24:18.220, mean 00 00:00:00.017
step 81500: train loss 2.986988, val loss 2.9879792, mem 1.8 GiB @ 00 00:24:27.238, mean 00 00:00:00.018
step 82000: train loss 2.9778666, val loss 2.9750686, mem 1.8 GiB @ 00 00:24:36.350, mean 00 00:00:00.018
step 82500: train loss 2.9654553, val loss 2.9855912, mem 1.8 GiB @ 00 00:24:45.138, mean 00 00:00:00.017
step 83000: train loss 2.9540637, val loss 2.9629607, mem 1.8 GiB @ 00 00:24:53.889, mean 00 00:00:00.017
step 83500: train loss 2.9475336, val loss 2.9545007, mem 1.8 GiB @ 00 00:25:02.534, mean 00 00:00:00.017
step 84000: train loss 2.9419594, val loss 2.9637988, mem 1.8 GiB @ 00 00:25:11.313, mean 00 00:00:00.017
step 84500: train loss 2.934215, val loss 2.9419675, mem 1.8 GiB @ 00 00:25:20.074, mean 00 00:00:00.017
step 85000: train loss 2.9262726, val loss 2.9354088, mem 1.8 GiB @ 00 00:25:28.960, mean 00 00:00:00.017
step 85500: train loss 2.9108417, val loss 2.937922, mem 1.8 GiB @ 00 00:25:38.041, mean 00 00:00:00.018
step 86000: train loss 2.920247, val loss 2.923422, mem 1.8 GiB @ 00 00:25:47.197, mean 00 00:00:00.018
step 86500: train loss 2.9191928, val loss 2.9207947, mem 1.8 GiB @ 00 00:25:56.372, mean 00 00:00:00.018
step 87000: train loss 2.896528, val loss 2.9041326, mem 1.8 GiB @ 00 00:26:05.560, mean 00 00:00:00.018
step 87500: train loss 2.894516, val loss 2.914386, mem 1.8 GiB @ 00 00:26:14.728, mean 00 00:00:00.018
step 88000: train loss 2.9018626, val loss 2.9127057, mem 1.8 GiB @ 00 00:26:23.925, mean 00 00:00:00.018
step 88500: train loss 2.8924115, val loss 2.89776, mem 1.8 GiB @ 00 00:26:33.114, mean 00 00:00:00.018
step 89000: train loss 2.8885083, val loss 2.8989053, mem 1.8 GiB @ 00 00:26:42.321, mean 00 00:00:00.018
step 89500: train loss 2.8878338, val loss 2.8918962, mem 1.8 GiB @ 00 00:26:51.505, mean 00 00:00:00.018
step 90000: train loss 2.8727672, val loss 2.8881645, mem 1.8 GiB @ 00 00:27:00.680, mean 00 00:00:00.018
step 90500: train loss 2.8695486, val loss 2.8823712, mem 1.8 GiB @ 00 00:27:09.737, mean 00 00:00:00.018
step 91000: train loss 2.8722076, val loss 2.8811336, mem 1.8 GiB @ 00 00:27:18.889, mean 00 00:00:00.018
step 91500: train loss 2.8518605, val loss 2.8708966, mem 1.8 GiB @ 00 00:27:28.063, mean 00 00:00:00.018
step 92000: train loss 2.8481207, val loss 2.863853, mem 1.8 GiB @ 00 00:27:37.226, mean 00 00:00:00.018
step 92500: train loss 2.8494465, val loss 2.8410676, mem 1.8 GiB @ 00 00:27:46.410, mean 00 00:00:00.018
step 93000: train loss 2.8404894, val loss 2.8720582, mem 1.8 GiB @ 00 00:27:55.165, mean 00 00:00:00.017
step 93500: train loss 2.8300219, val loss 2.8543532, mem 1.8 GiB @ 00 00:28:03.913, mean 00 00:00:00.017
step 94000: train loss 2.8470733, val loss 2.8322735, mem 1.8 GiB @ 00 00:28:12.691, mean 00 00:00:00.017
step 94500: train loss 2.8314776, val loss 2.830786, mem 1.8 GiB @ 00 00:28:21.478, mean 00 00:00:00.017
step 95000: train loss 2.8254597, val loss 2.828233, mem 1.8 GiB @ 00 00:28:30.225, mean 00 00:00:00.017
step 95500: train loss 2.8124263, val loss 2.8328753, mem 1.8 GiB @ 00 00:28:38.982, mean 00 00:00:00.017
step 96000: train loss 2.8072019, val loss 2.840381, mem 1.8 GiB @ 00 00:28:47.761, mean 00 00:00:00.017
step 96500: train loss 2.7974772, val loss 2.8279347, mem 1.8 GiB @ 00 00:28:56.561, mean 00 00:00:00.017
step 97000: train loss 2.8009362, val loss 2.7884665, mem 1.8 GiB @ 00 00:29:05.289, mean 00 00:00:00.017
step 97500: train loss 2.8017845, val loss 2.8089318, mem 1.8 GiB @ 00 00:29:14.060, mean 00 00:00:00.017
step 98000: train loss 2.7868304, val loss 2.810138, mem 1.8 GiB @ 00 00:29:22.821, mean 00 00:00:00.017
step 98500: train loss 2.7835379, val loss 2.7994335, mem 1.8 GiB @ 00 00:29:31.496, mean 00 00:00:00.017
step 99000: train loss 2.78406, val loss 2.7957091, mem 1.8 GiB @ 00 00:29:40.276, mean 00 00:00:00.017
step 99500: train loss 2.8020122, val loss 2.7949417, mem 1.8 GiB @ 00 00:29:49.016, mean 00 00:00:00.017
step 100000: train loss 2.7799675, val loss 2.7856262, mem 1.8 GiB @ 00 00:29:57.785, mean 00 00:00:00.017
step 100500: train loss 2.7662616, val loss 2.7756882, mem 1.8 GiB @ 00 00:30:06.565, mean 00 00:00:00.017
step 101000: train loss 2.7687566, val loss 2.7597785, mem 1.8 GiB @ 00 00:30:15.241, mean 00 00:00:00.017
step 101500: train loss 2.7700038, val loss 2.7598734, mem 1.8 GiB @ 00 00:30:24.052, mean 00 00:00:00.017
step 102000: train loss 2.7589343, val loss 2.756806, mem 1.8 GiB @ 00 00:30:33.224, mean 00 00:00:00.018
step 102500: train loss 2.7529461, val loss 2.7709734, mem 1.8 GiB @ 00 00:30:42.327, mean 00 00:00:00.018
step 103000: train loss 2.7329144, val loss 2.7561607, mem 1.8 GiB @ 00 00:30:51.495, mean 00 00:00:00.018
step 103500: train loss 2.7445312, val loss 2.760907, mem 1.8 GiB @ 00 00:31:00.637, mean 00 00:00:00.018
step 104000: train loss 2.7445226, val loss 2.7512395, mem 1.8 GiB @ 00 00:31:09.706, mean 00 00:00:00.018
step 104500: train loss 2.7324665, val loss 2.7486663, mem 1.8 GiB @ 00 00:31:18.844, mean 00 00:00:00.018
step 105000: train loss 2.742105, val loss 2.737151, mem 1.8 GiB @ 00 00:31:27.739, mean 00 00:00:00.017
step 105500: train loss 2.7379863, val loss 2.743405, mem 1.8 GiB @ 00 00:31:36.509, mean 00 00:00:00.017
step 106000: train loss 2.7416973, val loss 2.7395911, mem 1.8 GiB @ 00 00:31:45.636, mean 00 00:00:00.018
step 106500: train loss 2.7266572, val loss 2.7246938, mem 1.8 GiB @ 00 00:31:54.722, mean 00 00:00:00.018
step 107000: train loss 2.7212937, val loss 2.727163, mem 1.8 GiB @ 00 00:32:03.906, mean 00 00:00:00.018
step 107500: train loss 2.7251775, val loss 2.7276816, mem 1.8 GiB @ 00 00:32:13.066, mean 00 00:00:00.018
step 108000: train loss 2.7336512, val loss 2.725909, mem 1.8 GiB @ 00 00:32:22.234, mean 00 00:00:00.018
step 108500: train loss 2.7226007, val loss 2.720756, mem 1.8 GiB @ 00 00:32:31.389, mean 00 00:00:00.018
step 109000: train loss 2.7352026, val loss 2.7113037, mem 1.8 GiB @ 00 00:32:40.545, mean 00 00:00:00.018
step 109500: train loss 2.7170486, val loss 2.7304528, mem 1.8 GiB @ 00 00:32:49.730, mean 00 00:00:00.018
step 110000: train loss 2.703827, val loss 2.6971893, mem 1.8 GiB @ 00 00:32:58.909, mean 00 00:00:00.018
step 110500: train loss 2.6998398, val loss 2.7095728, mem 1.8 GiB @ 00 00:33:08.072, mean 00 00:00:00.018
step 111000: train loss 2.704338, val loss 2.7185974, mem 1.8 GiB @ 00 00:33:17.154, mean 00 00:00:00.018
step 111500: train loss 2.6989648, val loss 2.697435, mem 1.8 GiB @ 00 00:33:26.317, mean 00 00:00:00.018
step 112000: train loss 2.7212393, val loss 2.6892204, mem 1.8 GiB @ 00 00:33:35.489, mean 00 00:00:00.018
step 112500: train loss 2.691427, val loss 2.6963959, mem 1.8 GiB @ 00 00:33:44.659, mean 00 00:00:00.018
step 113000: train loss 2.690155, val loss 2.701145, mem 1.8 GiB @ 00 00:33:53.853, mean 00 00:00:00.018
step 113500: train loss 2.6925054, val loss 2.6719885, mem 1.8 GiB @ 00 00:34:03.024, mean 00 00:00:00.018
step 114000: train loss 2.685029, val loss 2.6903565, mem 1.8 GiB @ 00 00:34:12.101, mean 00 00:00:00.018
step 114500: train loss 2.6855054, val loss 2.6902013, mem 1.8 GiB @ 00 00:34:20.827, mean 00 00:00:00.017
step 115000: train loss 2.687055, val loss 2.692568, mem 1.8 GiB @ 00 00:34:29.631, mean 00 00:00:00.017
step 115500: train loss 2.6740947, val loss 2.68078, mem 1.8 GiB @ 00 00:34:38.385, mean 00 00:00:00.017
step 116000: train loss 2.670398, val loss 2.6673086, mem 1.8 GiB @ 00 00:34:47.080, mean 00 00:00:00.017
step 116500: train loss 2.6906726, val loss 2.6748016, mem 1.8 GiB @ 00 00:34:55.858, mean 00 00:00:00.017
step 117000: train loss 2.66376, val loss 2.665943, mem 1.8 GiB @ 00 00:35:04.646, mean 00 00:00:00.017
step 117500: train loss 2.667401, val loss 2.6714613, mem 1.8 GiB @ 00 00:35:13.424, mean 00 00:00:00.017
step 118000: train loss 2.6574073, val loss 2.6692786, mem 1.8 GiB @ 00 00:35:22.128, mean 00 00:00:00.017
step 118500: train loss 2.6561391, val loss 2.6662934, mem 1.8 GiB @ 00 00:35:30.926, mean 00 00:00:00.017
step 119000: train loss 2.6627417, val loss 2.662669, mem 1.8 GiB @ 00 00:35:39.707, mean 00 00:00:00.017
step 119500: train loss 2.6686978, val loss 2.6553879, mem 1.8 GiB @ 00 00:35:48.479, mean 00 00:00:00.017
step 120000: train loss 2.6554027, val loss 2.6615672, mem 1.8 GiB @ 00 00:35:57.244, mean 00 00:00:00.017
step 120500: train loss 2.6669497, val loss 2.6447644, mem 1.8 GiB @ 00 00:36:06.029, mean 00 00:00:00.017
step 121000: train loss 2.6409009, val loss 2.6528614, mem 1.8 GiB @ 00 00:36:15.061, mean 00 00:00:00.018
step 121500: train loss 2.655092, val loss 2.659728, mem 1.8 GiB @ 00 00:36:24.198, mean 00 00:00:00.018
step 122000: train loss 2.6470232, val loss 2.6408494, mem 1.8 GiB @ 00 00:36:33.369, mean 00 00:00:00.018
step 122500: train loss 2.6467505, val loss 2.6435692, mem 1.8 GiB @ 00 00:36:42.523, mean 00 00:00:00.018
step 123000: train loss 2.6969445, val loss 2.6382928, mem 1.8 GiB @ 00 00:36:51.580, mean 00 00:00:00.018
step 123500: train loss 2.6392453, val loss 2.6458783, mem 1.8 GiB @ 00 00:37:00.685, mean 00 00:00:00.018
step 124000: train loss 2.6396778, val loss 2.6427372, mem 1.8 GiB @ 00 00:37:09.839, mean 00 00:00:00.018
step 124500: train loss 2.6494963, val loss 2.64337, mem 1.8 GiB @ 00 00:37:18.969, mean 00 00:00:00.018
step 125000: train loss 2.6549847, val loss 2.6395855, mem 1.8 GiB @ 00 00:37:27.995, mean 00 00:00:00.018
step 125500: train loss 2.6417181, val loss 2.6459734, mem 1.8 GiB @ 00 00:37:37.161, mean 00 00:00:00.018
step 126000: train loss 2.6424465, val loss 2.6258795, mem 1.8 GiB @ 00 00:37:46.337, mean 00 00:00:00.018
step 126500: train loss 2.6383464, val loss 2.6106415, mem 1.8 GiB @ 00 00:37:55.507, mean 00 00:00:00.018
step 127000: train loss 2.647812, val loss 2.64037, mem 1.8 GiB @ 00 00:38:04.574, mean 00 00:00:00.018
step 127500: train loss 2.6320179, val loss 2.6224773, mem 1.8 GiB @ 00 00:38:13.715, mean 00 00:00:00.018
step 128000: train loss 2.6224353, val loss 2.6174212, mem 1.8 GiB @ 00 00:38:22.885, mean 00 00:00:00.018
step 128500: train loss 2.635921, val loss 2.6375527, mem 1.8 GiB @ 00 00:38:32.048, mean 00 00:00:00.018
step 129000: train loss 2.628443, val loss 2.6293542, mem 1.8 GiB @ 00 00:38:41.219, mean 00 00:00:00.018
step 129500: train loss 2.6117558, val loss 2.6332061, mem 1.8 GiB @ 00 00:38:50.372, mean 00 00:00:00.018
step 130000: train loss 2.6458576, val loss 2.6209345, mem 1.8 GiB @ 00 00:38:59.524, mean 00 00:00:00.018
step 130500: train loss 2.6283147, val loss 2.601507, mem 1.8 GiB @ 00 00:39:08.676, mean 00 00:00:00.018
step 131000: train loss 2.6283495, val loss 2.622672, mem 1.8 GiB @ 00 00:39:17.804, mean 00 00:00:00.018
step 131500: train loss 2.6195898, val loss 2.606389, mem 1.8 GiB @ 00 00:39:26.882, mean 00 00:00:00.018
step 132000: train loss 2.608028, val loss 2.610834, mem 1.8 GiB @ 00 00:39:36.032, mean 00 00:00:00.018
step 132500: train loss 2.6182234, val loss 2.6141667, mem 1.8 GiB @ 00 00:39:45.192, mean 00 00:00:00.018
step 133000: train loss 2.6430285, val loss 2.6055565, mem 1.8 GiB @ 00 00:39:54.359, mean 00 00:00:00.018
step 133500: train loss 2.6241195, val loss 2.5952888, mem 1.8 GiB @ 00 00:40:03.516, mean 00 00:00:00.018
step 134000: train loss 2.6134267, val loss 2.6113317, mem 1.8 GiB @ 00 00:40:12.686, mean 00 00:00:00.018
step 134500: train loss 2.6023073, val loss 2.604655, mem 1.8 GiB @ 00 00:40:21.848, mean 00 00:00:00.018
step 135000: train loss 2.6072943, val loss 2.5992455, mem 1.8 GiB @ 00 00:40:30.900, mean 00 00:00:00.018
step 135500: train loss 2.6104877, val loss 2.600059, mem 1.8 GiB @ 00 00:40:40.046, mean 00 00:00:00.018
step 136000: train loss 2.61171, val loss 2.591893, mem 1.8 GiB @ 00 00:40:49.211, mean 00 00:00:00.018
step 136500: train loss 2.6170137, val loss 2.601796, mem 1.8 GiB @ 00 00:40:58.357, mean 00 00:00:00.018
step 137000: train loss 2.6042721, val loss 2.6012225, mem 1.8 GiB @ 00 00:41:07.418, mean 00 00:00:00.018
step 137500: train loss 2.6132684, val loss 2.5947213, mem 1.8 GiB @ 00 00:41:16.562, mean 00 00:00:00.018
step 138000: train loss 2.6081252, val loss 2.595448, mem 1.8 GiB @ 00 00:41:25.715, mean 00 00:00:00.018
step 138500: train loss 2.593008, val loss 2.5980675, mem 1.8 GiB @ 00 00:41:34.869, mean 00 00:00:00.018
step 139000: train loss 2.6057806, val loss 2.59741, mem 1.8 GiB @ 00 00:41:43.999, mean 00 00:00:00.018
step 139500: train loss 2.590303, val loss 2.5966754, mem 1.8 GiB @ 00 00:41:53.055, mean 00 00:00:00.018
step 140000: train loss 2.5940082, val loss 2.6105554, mem 1.8 GiB @ 00 00:42:02.191, mean 00 00:00:00.018
step 140500: train loss 2.5841503, val loss 2.5857613, mem 1.8 GiB @ 00 00:42:11.354, mean 00 00:00:00.018
step 141000: train loss 2.5978224, val loss 2.581309, mem 1.8 GiB @ 00 00:42:20.507, mean 00 00:00:00.018
step 141500: train loss 2.5954795, val loss 2.5794225, mem 1.8 GiB @ 00 00:42:29.639, mean 00 00:00:00.018
step 142000: train loss 2.5803726, val loss 2.576414, mem 1.8 GiB @ 00 00:42:38.708, mean 00 00:00:00.018
step 142500: train loss 2.5952208, val loss 2.5876477, mem 1.8 GiB @ 00 00:42:47.815, mean 00 00:00:00.018
step 143000: train loss 2.5769129, val loss 2.581501, mem 1.8 GiB @ 00 00:42:56.959, mean 00 00:00:00.018
step 143500: train loss 2.5981457, val loss 2.560415, mem 1.8 GiB @ 00 00:43:06.107, mean 00 00:00:00.018
step 144000: train loss 2.5821059, val loss 2.5765252, mem 1.8 GiB @ 00 00:43:15.194, mean 00 00:00:00.018
step 144500: train loss 2.5853877, val loss 2.5866516, mem 1.8 GiB @ 00 00:43:24.342, mean 00 00:00:00.018
step 145000: train loss 2.5822363, val loss 2.5734568, mem 1.8 GiB @ 00 00:43:33.511, mean 00 00:00:00.018
step 145500: train loss 2.5786865, val loss 2.568757, mem 1.8 GiB @ 00 00:43:42.654, mean 00 00:00:00.018
step 146000: train loss 2.5775313, val loss 2.59477, mem 1.8 GiB @ 00 00:43:51.699, mean 00 00:00:00.018
step 146500: train loss 2.573758, val loss 2.570093, mem 1.8 GiB @ 00 00:44:00.833, mean 00 00:00:00.018
step 147000: train loss 2.580183, val loss 2.5605047, mem 1.8 GiB @ 00 00:44:09.969, mean 00 00:00:00.018
step 147500: train loss 2.5683665, val loss 2.5671697, mem 1.8 GiB @ 00 00:44:19.091, mean 00 00:00:00.018
step 148000: train loss 2.573531, val loss 2.5674462, mem 1.8 GiB @ 00 00:44:28.228, mean 00 00:00:00.018
step 148500: train loss 2.5596342, val loss 2.5740325, mem 1.8 GiB @ 00 00:44:37.263, mean 00 00:00:00.018
step 149000: train loss 2.5924914, val loss 2.5674274, mem 1.8 GiB @ 00 00:44:46.391, mean 00 00:00:00.018
step 149500: train loss 2.5921347, val loss 2.5564914, mem 1.8 GiB @ 00 00:44:55.523, mean 00 00:00:00.018
step 150000: train loss 2.5900805, val loss 2.5568197, mem 1.8 GiB @ 00 00:45:04.679, mean 00 00:00:00.018
step 150500: train loss 2.580689, val loss 2.5748851, mem 1.8 GiB @ 00 00:45:13.847, mean 00 00:00:00.018
step 151000: train loss 2.557488, val loss 2.5621867, mem 1.8 GiB @ 00 00:45:22.996, mean 00 00:00:00.018
step 151500: train loss 2.557669, val loss 2.5579085, mem 1.8 GiB @ 00 00:45:32.061, mean 00 00:00:00.018
step 152000: train loss 2.5579257, val loss 2.5772893, mem 1.8 GiB @ 00 00:45:41.237, mean 00 00:00:00.018
step 152500: train loss 2.574652, val loss 2.5495148, mem 1.8 GiB @ 00 00:45:50.405, mean 00 00:00:00.018
step 153000: train loss 2.5530126, val loss 2.5730114, mem 1.8 GiB @ 00 00:45:59.541, mean 00 00:00:00.018
step 153500: train loss 2.5552585, val loss 2.5499167, mem 1.8 GiB @ 00 00:46:08.719, mean 00 00:00:00.018
step 154000: train loss 2.557756, val loss 2.560433, mem 1.8 GiB @ 00 00:46:17.788, mean 00 00:00:00.018
step 154500: train loss 2.5555925, val loss 2.5524707, mem 1.8 GiB @ 00 00:46:26.945, mean 00 00:00:00.018
step 155000: train loss 2.548258, val loss 2.547407, mem 1.8 GiB @ 00 00:46:36.081, mean 00 00:00:00.018
step 155500: train loss 2.546521, val loss 2.5444102, mem 1.8 GiB @ 00 00:46:45.203, mean 00 00:00:00.018
step 156000: train loss 2.5437052, val loss 2.5510385, mem 1.8 GiB @ 00 00:46:54.338, mean 00 00:00:00.018
step 156500: train loss 2.5493915, val loss 2.5528522, mem 1.8 GiB @ 00 00:47:03.468, mean 00 00:00:00.018
step 157000: train loss 2.5381615, val loss 2.541591, mem 1.8 GiB @ 00 00:47:12.598, mean 00 00:00:00.018
step 157500: train loss 2.6442618, val loss 2.5435646, mem 1.8 GiB @ 00 00:47:21.757, mean 00 00:00:00.018
step 158000: train loss 2.5618637, val loss 2.533423, mem 1.8 GiB @ 00 00:47:30.900, mean 00 00:00:00.018
step 158500: train loss 2.5516768, val loss 2.5312085, mem 1.8 GiB @ 00 00:47:40.048, mean 00 00:00:00.018
step 159000: train loss 2.6240737, val loss 2.5502584, mem 1.8 GiB @ 00 00:47:49.172, mean 00 00:00:00.018
step 159500: train loss 2.5466914, val loss 2.5447245, mem 1.8 GiB @ 00 00:47:58.308, mean 00 00:00:00.018
step 160000: train loss 2.54093, val loss 2.5552385, mem 1.8 GiB @ 00 00:48:07.339, mean 00 00:00:00.018
step 160500: train loss 2.5627043, val loss 2.527057, mem 1.8 GiB @ 00 00:48:16.457, mean 00 00:00:00.018
step 161000: train loss 2.539045, val loss 2.5430613, mem 1.8 GiB @ 00 00:48:25.573, mean 00 00:00:00.018
step 161500: train loss 2.5590665, val loss 2.5375698, mem 1.8 GiB @ 00 00:48:34.700, mean 00 00:00:00.018
step 162000: train loss 2.527223, val loss 2.5527897, mem 1.8 GiB @ 00 00:48:43.839, mean 00 00:00:00.018
step 162500: train loss 2.5441654, val loss 2.5447793, mem 1.8 GiB @ 00 00:48:52.868, mean 00 00:00:00.018
step 163000: train loss 2.5348036, val loss 2.5425344, mem 1.8 GiB @ 00 00:49:01.977, mean 00 00:00:00.018
step 163500: train loss 2.535797, val loss 2.5472846, mem 1.8 GiB @ 00 00:49:11.117, mean 00 00:00:00.018
step 164000: train loss 2.5525765, val loss 2.5548623, mem 1.8 GiB @ 00 00:49:20.270, mean 00 00:00:00.018
step 164500: train loss 2.543605, val loss 2.5387998, mem 1.8 GiB @ 00 00:49:29.297, mean 00 00:00:00.018
step 165000: train loss 2.5368884, val loss 2.5341053, mem 1.8 GiB @ 00 00:49:38.418, mean 00 00:00:00.018
step 165500: train loss 2.5271544, val loss 2.5279486, mem 1.8 GiB @ 00 00:49:47.560, mean 00 00:00:00.018
step 166000: train loss 2.5519428, val loss 2.5401773, mem 1.8 GiB @ 00 00:49:56.704, mean 00 00:00:00.018
step 166500: train loss 2.5404048, val loss 2.54687, mem 1.8 GiB @ 00 00:50:05.835, mean 00 00:00:00.018
step 167000: train loss 2.5358007, val loss 2.5329196, mem 1.8 GiB @ 00 00:50:14.960, mean 00 00:00:00.018
step 167500: train loss 2.5352387, val loss 2.5358474, mem 1.8 GiB @ 00 00:50:23.993, mean 00 00:00:00.018
step 168000: train loss 2.5617735, val loss 2.5437577, mem 1.8 GiB @ 00 00:50:33.135, mean 00 00:00:00.018
step 168500: train loss 2.5336206, val loss 2.5206435, mem 1.8 GiB @ 00 00:50:42.259, mean 00 00:00:00.018
step 169000: train loss 2.514014, val loss 2.5402973, mem 1.8 GiB @ 00 00:50:51.376, mean 00 00:00:00.018
step 169500: train loss 2.5285003, val loss 2.527598, mem 1.8 GiB @ 00 00:51:00.502, mean 00 00:00:00.018
step 170000: train loss 2.526706, val loss 2.5235028, mem 1.8 GiB @ 00 00:51:09.531, mean 00 00:00:00.018
step 170500: train loss 2.557615, val loss 2.5180004, mem 1.8 GiB @ 00 00:51:18.731, mean 00 00:00:00.018
step 171000: train loss 2.5209045, val loss 2.5137818, mem 1.8 GiB @ 00 00:51:27.696, mean 00 00:00:00.017
step 171500: train loss 2.526214, val loss 2.516703, mem 1.8 GiB @ 00 00:51:36.491, mean 00 00:00:00.017
step 172000: train loss 2.5283027, val loss 2.5299675, mem 1.8 GiB @ 00 00:51:45.301, mean 00 00:00:00.017
step 172500: train loss 2.5154245, val loss 2.517723, mem 1.8 GiB @ 00 00:51:53.987, mean 00 00:00:00.017
step 173000: train loss 2.5166047, val loss 2.5148559, mem 1.8 GiB @ 00 00:52:02.804, mean 00 00:00:00.017
step 173500: train loss 2.526689, val loss 2.502134, mem 1.8 GiB @ 00 00:52:11.562, mean 00 00:00:00.017
step 174000: train loss 2.5273664, val loss 2.5055194, mem 1.8 GiB @ 00 00:52:20.297, mean 00 00:00:00.017
step 174500: train loss 2.5251281, val loss 2.5148578, mem 1.8 GiB @ 00 00:52:29.024, mean 00 00:00:00.017
step 175000: train loss 2.5138183, val loss 2.5150354, mem 1.8 GiB @ 00 00:52:37.731, mean 00 00:00:00.017
step 175500: train loss 2.5220757, val loss 2.498464, mem 1.8 GiB @ 00 00:52:46.513, mean 00 00:00:00.017
step 176000: train loss 2.5313504, val loss 2.512738, mem 1.8 GiB @ 00 00:52:55.246, mean 00 00:00:00.017
step 176500: train loss 2.5240066, val loss 2.5046458, mem 1.8 GiB @ 00 00:53:04.003, mean 00 00:00:00.017
step 177000: train loss 2.5020943, val loss 2.5271697, mem 1.8 GiB @ 00 00:53:12.796, mean 00 00:00:00.017
step 177500: train loss 2.5142105, val loss 2.507526, mem 1.8 GiB @ 00 00:53:21.531, mean 00 00:00:00.017
step 178000: train loss 2.5214717, val loss 2.5008574, mem 1.8 GiB @ 00 00:53:30.158, mean 00 00:00:00.017
step 178500: train loss 2.5199673, val loss 2.5107973, mem 1.8 GiB @ 00 00:53:38.876, mean 00 00:00:00.017
step 179000: train loss 2.5000296, val loss 2.5194066, mem 1.8 GiB @ 00 00:53:47.651, mean 00 00:00:00.017
step 179500: train loss 2.5067203, val loss 2.4974456, mem 1.8 GiB @ 00 00:53:56.418, mean 00 00:00:00.017
step 180000: train loss 2.5056288, val loss 2.497915, mem 1.8 GiB @ 00 00:54:05.157, mean 00 00:00:00.017
step 180500: train loss 2.594002, val loss 2.4996915, mem 1.8 GiB @ 00 00:54:13.929, mean 00 00:00:00.017
step 181000: train loss 2.5193682, val loss 2.510895, mem 1.8 GiB @ 00 00:54:22.602, mean 00 00:00:00.017
step 181500: train loss 2.5053945, val loss 2.501033, mem 1.8 GiB @ 00 00:54:31.382, mean 00 00:00:00.017
step 182000: train loss 2.5003684, val loss 2.509502, mem 1.8 GiB @ 00 00:54:40.166, mean 00 00:00:00.017
step 182500: train loss 2.5168636, val loss 2.5045457, mem 1.8 GiB @ 00 00:54:48.932, mean 00 00:00:00.017
step 183000: train loss 2.5087895, val loss 2.5074468, mem 1.8 GiB @ 00 00:54:57.557, mean 00 00:00:00.017
step 183500: train loss 2.4820862, val loss 2.494611, mem 1.8 GiB @ 00 00:55:06.326, mean 00 00:00:00.017
step 184000: train loss 2.502267, val loss 2.5066352, mem 1.8 GiB @ 00 00:55:15.121, mean 00 00:00:00.017
step 184500: train loss 2.5026727, val loss 2.4899154, mem 1.8 GiB @ 00 00:55:23.873, mean 00 00:00:00.017
step 185000: train loss 2.5578895, val loss 2.4956203, mem 1.8 GiB @ 00 00:55:32.552, mean 00 00:00:00.017
step 185500: train loss 2.503206, val loss 2.4801965, mem 1.8 GiB @ 00 00:55:41.302, mean 00 00:00:00.017
step 186000: train loss 2.6391988, val loss 2.6496222, mem 1.8 GiB @ 00 00:55:50.420, mean 00 00:00:00.018
step 186500: train loss 2.7605357, val loss 2.8101664, mem 1.8 GiB @ 00 00:55:59.566, mean 00 00:00:00.018
step 187000: train loss 2.747298, val loss 2.7703528, mem 1.8 GiB @ 00 00:56:08.711, mean 00 00:00:00.018
step 187500: train loss 2.7035546, val loss 2.7561557, mem 1.8 GiB @ 00 00:56:17.885, mean 00 00:00:00.018
step 188000: train loss 2.694725, val loss 2.7074792, mem 1.8 GiB @ 00 00:56:27.032, mean 00 00:00:00.018
step 188500: train loss 2.660415, val loss 2.6848052, mem 1.8 GiB @ 00 00:56:36.059, mean 00 00:00:00.018
step 189000: train loss 2.6699243, val loss 2.686576, mem 1.8 GiB @ 00 00:56:44.900, mean 00 00:00:00.017
step 189500: train loss 2.6505117, val loss 2.6860864, mem 1.8 GiB @ 00 00:56:53.713, mean 00 00:00:00.017
step 190000: train loss 2.6276734, val loss 2.6521409, mem 1.8 GiB @ 00 00:57:02.523, mean 00 00:00:00.017
step 190500: train loss 2.614098, val loss 2.6136324, mem 1.8 GiB @ 00 00:57:11.221, mean 00 00:00:00.017
step 191000: train loss 2.6185927, val loss 2.6000452, mem 1.8 GiB @ 00 00:57:20.030, mean 00 00:00:00.017
step 191500: train loss 2.61393, val loss 2.6150014, mem 1.8 GiB @ 00 00:57:29.065, mean 00 00:00:00.018
step 192000: train loss 2.5688295, val loss 2.5952506, mem 1.8 GiB @ 00 00:57:38.346, mean 00 00:00:00.018
step 192500: train loss 2.6315835, val loss 2.5830283, mem 1.8 GiB @ 00 00:57:47.435, mean 00 00:00:00.018
step 193000: train loss 2.5459197, val loss 2.5620086, mem 1.8 GiB @ 00 00:57:56.598, mean 00 00:00:00.018
step 193500: train loss 2.5539908, val loss 2.53785, mem 1.8 GiB @ 00 00:58:05.775, mean 00 00:00:00.018
step 194000: train loss 2.5357075, val loss 2.5282996, mem 1.8 GiB @ 00 00:58:14.710, mean 00 00:00:00.017
step 194500: train loss 2.5252588, val loss 2.5360034, mem 1.8 GiB @ 00 00:58:23.703, mean 00 00:00:00.017
step 195000: train loss 2.9602966, val loss 3.0015907, mem 1.8 GiB @ 00 00:58:32.508, mean 00 00:00:00.017
step 195500: train loss 3.226125, val loss 3.32104, mem 1.8 GiB @ 00 00:58:41.269, mean 00 00:00:00.017
step 196000: train loss 3.207851, val loss 3.2976449, mem 1.8 GiB @ 00 00:58:49.979, mean 00 00:00:00.017
step 196500: train loss 3.147855, val loss 3.2276843, mem 1.8 GiB @ 00 00:58:58.960, mean 00 00:00:00.017
step 197000: train loss 3.1404169, val loss 3.181102, mem 1.8 GiB @ 00 00:59:07.764, mean 00 00:00:00.017
step 197500: train loss 3.0707848, val loss 3.1309063, mem 1.8 GiB @ 00 00:59:16.701, mean 00 00:00:00.017
step 198000: train loss 3.0371509, val loss 3.1090806, mem 1.8 GiB @ 00 00:59:25.461, mean 00 00:00:00.017
step 198500: train loss 3.0718253, val loss 3.0589926, mem 1.8 GiB @ 00 00:59:34.131, mean 00 00:00:00.017
step 199000: train loss 3.013269, val loss 3.0497398, mem 1.8 GiB @ 00 00:59:42.928, mean 00 00:00:00.017
step 199500: train loss 2.9890387, val loss 3.053209, mem 1.8 GiB @ 00 00:59:51.699, mean 00 00:00:00.017
step 200000: train loss 2.978158, val loss 3.0194566, mem 1.8 GiB @ 00 01:00:00.745, mean 00 00:00:00.018
step 200500: train loss 2.971471, val loss 3.0188708, mem 1.8 GiB @ 00 01:00:09.660, mean 00 00:00:00.017
step 201000: train loss 2.9284942, val loss 2.9707778, mem 1.8 GiB @ 00 01:00:18.452, mean 00 00:00:00.017
step 201500: train loss 2.9202826, val loss 2.9918785, mem 1.8 GiB @ 00 01:00:27.462, mean 00 00:00:00.018
step 202000: train loss 2.9046416, val loss 2.9635227, mem 1.8 GiB @ 00 01:00:36.506, mean 00 00:00:00.018
step 202500: train loss 2.9099188, val loss 2.9574237, mem 1.8 GiB @ 00 01:00:45.711, mean 00 00:00:00.018
step 203000: train loss 2.8745813, val loss 2.919703, mem 1.8 GiB @ 00 01:00:54.645, mean 00 00:00:00.017
step 203500: train loss 2.8767095, val loss 2.9057343, mem 1.8 GiB @ 00 01:01:03.677, mean 00 00:00:00.018
step 204000: train loss 2.863193, val loss 2.9001672, mem 1.8 GiB @ 00 01:01:12.444, mean 00 00:00:00.017
step 204500: train loss 2.8453667, val loss 2.886128, mem 1.8 GiB @ 00 01:01:21.354, mean 00 00:00:00.017
step 205000: train loss 2.8517902, val loss 2.8715935, mem 1.8 GiB @ 00 01:01:30.578, mean 00 00:00:00.018
step 205500: train loss 2.8430014, val loss 2.8557227, mem 1.8 GiB @ 00 01:01:39.714, mean 00 00:00:00.018
step 206000: train loss 2.8376007, val loss 2.8653963, mem 1.8 GiB @ 00 01:01:48.885, mean 00 00:00:00.018
step 206500: train loss 2.8420525, val loss 2.8656938, mem 1.8 GiB @ 00 01:01:58.051, mean 00 00:00:00.018
step 207000: train loss 2.7989218, val loss 2.8503268, mem 1.8 GiB @ 00 01:02:07.211, mean 00 00:00:00.018
step 207500: train loss 2.788247, val loss 2.8346767, mem 1.8 GiB @ 00 01:02:16.273, mean 00 00:00:00.018
step 208000: train loss 2.7960534, val loss 2.8220644, mem 1.8 GiB @ 00 01:02:25.440, mean 00 00:00:00.018
step 208500: train loss 2.7643561, val loss 2.8100643, mem 1.8 GiB @ 00 01:02:34.640, mean 00 00:00:00.018
step 209000: train loss 2.7690794, val loss 2.8051195, mem 1.8 GiB @ 00 01:02:43.791, mean 00 00:00:00.018
step 209500: train loss 2.823078, val loss 2.7799718, mem 1.8 GiB @ 00 01:02:52.681, mean 00 00:00:00.017
step 210000: train loss 2.7465246, val loss 2.7820885, mem 1.8 GiB @ 00 01:03:01.523, mean 00 00:00:00.017
step 210500: train loss 2.7599597, val loss 2.7759554, mem 1.8 GiB @ 00 01:03:10.525, mean 00 00:00:00.018
step 211000: train loss 2.7265472, val loss 2.7441819, mem 1.8 GiB @ 00 01:03:19.691, mean 00 00:00:00.018
step 211500: train loss 2.7245493, val loss 2.744273, mem 1.8 GiB @ 00 01:03:28.935, mean 00 00:00:00.018
step 212000: train loss 2.7160375, val loss 2.743356, mem 1.8 GiB @ 00 01:03:38.138, mean 00 00:00:00.018
step 212500: train loss 2.7301092, val loss 2.741784, mem 1.8 GiB @ 00 01:03:47.200, mean 00 00:00:00.018
step 213000: train loss 2.724351, val loss 2.7546093, mem 1.8 GiB @ 00 01:03:56.356, mean 00 00:00:00.018
step 213500: train loss 2.7380147, val loss 2.7332876, mem 1.8 GiB @ 00 01:04:05.232, mean 00 00:00:00.017
step 214000: train loss 2.722359, val loss 2.7512848, mem 1.8 GiB @ 00 01:04:14.043, mean 00 00:00:00.017
step 214500: train loss 2.7270098, val loss 2.729644, mem 1.8 GiB @ 00 01:04:22.846, mean 00 00:00:00.017
step 215000: train loss 2.7138848, val loss 2.7413082, mem 1.8 GiB @ 00 01:04:31.699, mean 00 00:00:00.017
step 215500: train loss 2.7238328, val loss 2.7209575, mem 1.8 GiB @ 00 01:04:40.528, mean 00 00:00:00.017
step 216000: train loss 2.7460608, val loss 2.7202165, mem 1.8 GiB @ 00 01:04:49.341, mean 00 00:00:00.017
step 216500: train loss 2.6782904, val loss 2.7043123, mem 1.8 GiB @ 00 01:04:58.115, mean 00 00:00:00.017
step 217000: train loss 2.6803453, val loss 2.700168, mem 1.8 GiB @ 00 01:05:06.778, mean 00 00:00:00.017
step 217500: train loss 2.6650314, val loss 2.6773944, mem 1.8 GiB @ 00 01:05:15.586, mean 00 00:00:00.017
step 218000: train loss 2.6522179, val loss 2.6590064, mem 1.8 GiB @ 00 01:05:24.385, mean 00 00:00:00.017
step 218500: train loss 2.7171233, val loss 2.6695492, mem 1.8 GiB @ 00 01:05:33.084, mean 00 00:00:00.017
step 219000: train loss 2.6555552, val loss 2.6603386, mem 1.8 GiB @ 00 01:05:41.885, mean 00 00:00:00.017
step 219500: train loss 2.646396, val loss 2.6657827, mem 1.8 GiB @ 00 01:05:50.663, mean 00 00:00:00.017
step 220000: train loss 2.6443288, val loss 2.665458, mem 1.8 GiB @ 00 01:05:59.514, mean 00 00:00:00.017
step 220500: train loss 2.6325238, val loss 2.659098, mem 1.8 GiB @ 00 01:06:08.243, mean 00 00:00:00.017
step 221000: train loss 2.631106, val loss 2.6308534, mem 1.8 GiB @ 00 01:06:17.426, mean 00 00:00:00.018
step 221500: train loss 2.6211464, val loss 2.6254497, mem 1.8 GiB @ 00 01:06:26.582, mean 00 00:00:00.018
step 222000: train loss 2.618364, val loss 2.6232421, mem 1.8 GiB @ 00 01:06:35.505, mean 00 00:00:00.017
step 222500: train loss 2.64242, val loss 2.626619, mem 1.8 GiB @ 00 01:06:44.286, mean 00 00:00:00.017
step 223000: train loss 2.622974, val loss 2.643144, mem 1.8 GiB @ 00 01:06:53.085, mean 00 00:00:00.017
step 223500: train loss 2.619074, val loss 2.5995605, mem 1.8 GiB @ 00 01:07:02.297, mean 00 00:00:00.018
step 224000: train loss 2.6008372, val loss 2.6041062, mem 1.8 GiB @ 00 01:07:11.368, mean 00 00:00:00.018
step 224500: train loss 2.5877063, val loss 2.596571, mem 1.8 GiB @ 00 01:07:20.539, mean 00 00:00:00.018
step 225000: train loss 2.5759888, val loss 2.615086, mem 1.8 GiB @ 00 01:07:29.513, mean 00 00:00:00.017
step 225500: train loss 2.5810564, val loss 2.588485, mem 1.8 GiB @ 00 01:07:38.579, mean 00 00:00:00.018
step 226000: train loss 2.5848305, val loss 2.5928824, mem 1.8 GiB @ 00 01:07:47.619, mean 00 00:00:00.018
step 226500: train loss 2.5751898, val loss 2.5849037, mem 1.8 GiB @ 00 01:07:56.589, mean 00 00:00:00.017
step 227000: train loss 2.591604, val loss 2.5864542, mem 1.8 GiB @ 00 01:08:05.752, mean 00 00:00:00.018
step 227500: train loss 2.6020916, val loss 2.5821967, mem 1.8 GiB @ 00 01:08:15.008, mean 00 00:00:00.018
step 228000: train loss 2.581245, val loss 2.5862474, mem 1.8 GiB @ 00 01:08:23.768, mean 00 00:00:00.017
step 228500: train loss 2.5789819, val loss 2.5769308, mem 1.8 GiB @ 00 01:08:32.871, mean 00 00:00:00.018
step 229000: train loss 2.5734887, val loss 2.579993, mem 1.8 GiB @ 00 01:08:42.098, mean 00 00:00:00.018
step 229500: train loss 2.5725763, val loss 2.5729287, mem 1.8 GiB @ 00 01:08:51.213, mean 00 00:00:00.018
step 230000: train loss 2.5711606, val loss 2.5626154, mem 1.8 GiB @ 00 01:09:00.287, mean 00 00:00:00.018
step 230500: train loss 2.578496, val loss 2.5641565, mem 1.8 GiB @ 00 01:09:09.448, mean 00 00:00:00.018
step 231000: train loss 2.5806177, val loss 2.554535, mem 1.8 GiB @ 00 01:09:18.592, mean 00 00:00:00.018
step 231500: train loss 2.5879025, val loss 2.578955, mem 1.8 GiB @ 00 01:09:27.381, mean 00 00:00:00.017
step 232000: train loss 2.5690203, val loss 2.5557141, mem 1.8 GiB @ 00 01:09:36.195, mean 00 00:00:00.017
step 232500: train loss 2.5799181, val loss 2.567418, mem 1.8 GiB @ 00 01:09:45.013, mean 00 00:00:00.017
step 233000: train loss 2.5601456, val loss 2.5528748, mem 1.8 GiB @ 00 01:09:53.820, mean 00 00:00:00.017
step 233500: train loss 2.5713599, val loss 2.5511606, mem 1.8 GiB @ 00 01:10:02.602, mean 00 00:00:00.017
step 234000: train loss 2.5697093, val loss 2.5643713, mem 1.8 GiB @ 00 01:10:11.423, mean 00 00:00:00.017
step 234500: train loss 2.5583985, val loss 2.5598552, mem 1.8 GiB @ 00 01:10:20.194, mean 00 00:00:00.017
step 235000: train loss 2.5685735, val loss 2.563855, mem 1.8 GiB @ 00 01:10:28.990, mean 00 00:00:00.017
step 235500: train loss 2.5593753, val loss 2.5496728, mem 1.8 GiB @ 00 01:10:37.838, mean 00 00:00:00.017
step 236000: train loss 2.6019444, val loss 2.5773997, mem 1.8 GiB @ 00 01:10:46.642, mean 00 00:00:00.017
step 236500: train loss 2.5580127, val loss 2.5664618, mem 1.8 GiB @ 00 01:10:55.684, mean 00 00:00:00.018
step 237000: train loss 2.539511, val loss 2.5564046, mem 1.8 GiB @ 00 01:11:04.728, mean 00 00:00:00.018
step 237500: train loss 2.5433125, val loss 2.5383692, mem 1.8 GiB @ 00 01:11:13.616, mean 00 00:00:00.017
step 238000: train loss 2.558301, val loss 2.5474987, mem 1.8 GiB @ 00 01:11:22.751, mean 00 00:00:00.018
step 238500: train loss 2.5359893, val loss 2.5403142, mem 1.8 GiB @ 00 01:11:31.938, mean 00 00:00:00.018
step 239000: train loss 2.5567253, val loss 2.5473704, mem 1.8 GiB @ 00 01:11:41.084, mean 00 00:00:00.018
step 239500: train loss 2.5437763, val loss 2.5465395, mem 1.8 GiB @ 00 01:11:50.271, mean 00 00:00:00.018
step 240000: train loss 2.5371137, val loss 2.5481472, mem 1.8 GiB @ 00 01:11:59.436, mean 00 00:00:00.018
step 240500: train loss 2.5547137, val loss 2.5480523, mem 1.8 GiB @ 00 01:12:08.581, mean 00 00:00:00.018
step 241000: train loss 2.5429409, val loss 2.5392156, mem 1.8 GiB @ 00 01:12:17.716, mean 00 00:00:00.018
step 241500: train loss 2.5523057, val loss 2.525458, mem 1.8 GiB @ 00 01:12:26.886, mean 00 00:00:00.018
step 242000: train loss 2.5401464, val loss 2.5298932, mem 1.8 GiB @ 00 01:12:36.021, mean 00 00:00:00.018
step 242500: train loss 2.5281923, val loss 2.5299575, mem 1.8 GiB @ 00 01:12:44.848, mean 00 00:00:00.017
step 243000: train loss 2.5336003, val loss 2.5257037, mem 1.8 GiB @ 00 01:12:53.617, mean 00 00:00:00.017
step 243500: train loss 2.5390625, val loss 2.5334558, mem 1.8 GiB @ 00 01:13:02.412, mean 00 00:00:00.017
step 244000: train loss 2.522012, val loss 2.5230331, mem 1.8 GiB @ 00 01:13:11.376, mean 00 00:00:00.017
step 244500: train loss 2.5535512, val loss 2.5162625, mem 1.8 GiB @ 00 01:13:20.366, mean 00 00:00:00.017
step 245000: train loss 2.5219536, val loss 2.5350945, mem 1.8 GiB @ 00 01:13:29.126, mean 00 00:00:00.017
step 245500: train loss 2.5400603, val loss 2.5226333, mem 1.8 GiB @ 00 01:13:37.928, mean 00 00:00:00.017
step 246000: train loss 2.5211527, val loss 2.5209093, mem 1.8 GiB @ 00 01:13:46.745, mean 00 00:00:00.017
step 246500: train loss 2.5326288, val loss 2.517763, mem 1.8 GiB @ 00 01:13:55.551, mean 00 00:00:00.017
step 247000: train loss 2.5239825, val loss 2.5237296, mem 1.8 GiB @ 00 01:14:04.317, mean 00 00:00:00.017
step 247500: train loss 2.5342746, val loss 2.5316978, mem 1.8 GiB @ 00 01:14:13.276, mean 00 00:00:00.017
step 248000: train loss 2.5418677, val loss 2.5229464, mem 1.8 GiB @ 00 01:14:22.443, mean 00 00:00:00.018
step 248500: train loss 2.5208218, val loss 2.5226378, mem 1.8 GiB @ 00 01:14:31.613, mean 00 00:00:00.018
step 249000: train loss 2.5205505, val loss 2.5156775, mem 1.8 GiB @ 00 01:14:40.754, mean 00 00:00:00.018
step 249500: train loss 2.5135543, val loss 2.5137305, mem 1.8 GiB @ 00 01:14:49.978, mean 00 00:00:00.018
step 250000: train loss 2.5198588, val loss 2.5200772, mem 1.8 GiB @ 00 01:14:59.165, mean 00 00:00:00.018
step 250500: train loss 2.5099273, val loss 2.5215304, mem 1.8 GiB @ 00 01:15:08.308, mean 00 00:00:00.018
step 251000: train loss 2.528972, val loss 2.5394728, mem 1.8 GiB @ 00 01:15:17.483, mean 00 00:00:00.018
step 251500: train loss 2.5411782, val loss 2.50811, mem 1.8 GiB @ 00 01:15:26.639, mean 00 00:00:00.018
step 252000: train loss 2.522227, val loss 2.5041661, mem 1.8 GiB @ 00 01:15:35.703, mean 00 00:00:00.018
step 252500: train loss 2.5129066, val loss 2.5064766, mem 1.8 GiB @ 00 01:15:44.972, mean 00 00:00:00.018
step 253000: train loss 2.5152698, val loss 2.5204523, mem 1.8 GiB @ 00 01:15:53.865, mean 00 00:00:00.017
step 253500: train loss 2.5003397, val loss 2.4902773, mem 1.8 GiB @ 00 01:16:02.559, mean 00 00:00:00.017
step 254000: train loss 2.4988005, val loss 2.5167823, mem 1.8 GiB @ 00 01:16:11.340, mean 00 00:00:00.017
step 254500: train loss 2.5055013, val loss 2.4984376, mem 1.8 GiB @ 00 01:16:20.146, mean 00 00:00:00.017
step 255000: train loss 2.505079, val loss 2.5132554, mem 1.8 GiB @ 00 01:16:29.295, mean 00 00:00:00.018
step 255500: train loss 2.5095673, val loss 2.5038614, mem 1.8 GiB @ 00 01:16:38.588, mean 00 00:00:00.018
step 256000: train loss 2.5126374, val loss 2.5136561, mem 1.8 GiB @ 00 01:16:47.740, mean 00 00:00:00.018
step 256500: train loss 2.4952636, val loss 2.5063396, mem 1.8 GiB @ 00 01:16:56.899, mean 00 00:00:00.018
step 257000: train loss 2.5047023, val loss 2.5217605, mem 1.8 GiB @ 00 01:17:06.165, mean 00 00:00:00.018
step 257500: train loss 2.4988713, val loss 2.5067458, mem 1.8 GiB @ 00 01:17:15.314, mean 00 00:00:00.018
step 258000: train loss 2.5093448, val loss 2.5069857, mem 1.8 GiB @ 00 01:17:24.457, mean 00 00:00:00.018
step 258500: train loss 2.505372, val loss 2.4963632, mem 1.8 GiB @ 00 01:17:33.835, mean 00 00:00:00.018
step 259000: train loss 2.499518, val loss 2.5130494, mem 1.8 GiB @ 00 01:17:42.990, mean 00 00:00:00.018
step 259500: train loss 2.515798, val loss 2.5166612, mem 1.8 GiB @ 00 01:17:52.163, mean 00 00:00:00.018
step 260000: train loss 2.4852397, val loss 2.4907644, mem 1.8 GiB @ 00 01:18:01.659, mean 00 00:00:00.018
step 260500: train loss 2.5120816, val loss 2.5052261, mem 1.8 GiB @ 00 01:18:10.493, mean 00 00:00:00.017
step 261000: train loss 2.5111852, val loss 2.4863603, mem 1.8 GiB @ 00 01:18:19.234, mean 00 00:00:00.017
step 261500: train loss 2.5562885, val loss 2.4973514, mem 1.8 GiB @ 00 01:18:28.045, mean 00 00:00:00.017
step 262000: train loss 2.5115092, val loss 2.4853299, mem 1.8 GiB @ 00 01:18:36.923, mean 00 00:00:00.017
step 262500: train loss 2.4925854, val loss 2.5083647, mem 1.8 GiB @ 00 01:18:45.677, mean 00 00:00:00.017
step 263000: train loss 2.4957507, val loss 2.495994, mem 1.8 GiB @ 00 01:18:54.533, mean 00 00:00:00.017
step 263500: train loss 2.4846494, val loss 2.499709, mem 1.8 GiB @ 00 01:19:03.562, mean 00 00:00:00.018
step 264000: train loss 2.5441103, val loss 2.4874752, mem 1.8 GiB @ 00 01:19:12.503, mean 00 00:00:00.017
step 264500: train loss 2.4835744, val loss 2.4986813, mem 1.8 GiB @ 00 01:19:21.315, mean 00 00:00:00.017
step 265000: train loss 2.5109034, val loss 2.496733, mem 1.8 GiB @ 00 01:19:30.167, mean 00 00:00:00.017
step 265500: train loss 2.4943607, val loss 2.496507, mem 1.8 GiB @ 00 01:19:39.309, mean 00 00:00:00.018
step 266000: train loss 2.5062056, val loss 2.4891384, mem 1.8 GiB @ 00 01:19:48.197, mean 00 00:00:00.017
step 266500: train loss 2.498169, val loss 2.4821136, mem 1.8 GiB @ 00 01:19:57.348, mean 00 00:00:00.018
step 267000: train loss 2.5506854, val loss 2.4896333, mem 1.8 GiB @ 00 01:20:06.386, mean 00 00:00:00.018
step 267500: train loss 2.4845703, val loss 2.492978, mem 1.8 GiB @ 00 01:20:15.552, mean 00 00:00:00.018
step 268000: train loss 2.49685, val loss 2.4753346, mem 1.8 GiB @ 00 01:20:24.711, mean 00 00:00:00.018
step 268500: train loss 2.4785745, val loss 2.492923, mem 1.8 GiB @ 00 01:20:33.867, mean 00 00:00:00.018
step 269000: train loss 2.4962626, val loss 2.4833417, mem 1.8 GiB @ 00 01:20:43.019, mean 00 00:00:00.018
step 269500: train loss 2.4844522, val loss 2.480994, mem 1.8 GiB @ 00 01:20:52.178, mean 00 00:00:00.018
step 270000: train loss 2.5374827, val loss 2.4813194, mem 1.8 GiB @ 00 01:21:01.321, mean 00 00:00:00.018
step 270500: train loss 2.4997509, val loss 2.487495, mem 1.8 GiB @ 00 01:21:10.169, mean 00 00:00:00.017
step 271000: train loss 2.4972954, val loss 2.4815006, mem 1.8 GiB @ 00 01:21:18.986, mean 00 00:00:00.017
step 271500: train loss 2.4859962, val loss 2.4815052, mem 1.8 GiB @ 00 01:21:27.933, mean 00 00:00:00.017
step 272000: train loss 2.4977422, val loss 2.4779897, mem 1.8 GiB @ 00 01:21:37.094, mean 00 00:00:00.018
step 272500: train loss 2.4801755, val loss 2.4828565, mem 1.8 GiB @ 00 01:21:46.256, mean 00 00:00:00.018
step 273000: train loss 2.4902484, val loss 2.4964962, mem 1.8 GiB @ 00 01:21:55.308, mean 00 00:00:00.018
step 273500: train loss 2.4856274, val loss 2.4726913, mem 1.8 GiB @ 00 01:22:04.466, mean 00 00:00:00.018
step 274000: train loss 2.4846637, val loss 2.473841, mem 1.8 GiB @ 00 01:22:13.627, mean 00 00:00:00.018
step 274500: train loss 2.4785647, val loss 2.470437, mem 1.8 GiB @ 00 01:22:22.788, mean 00 00:00:00.018
step 275000: train loss 2.4843326, val loss 2.4909556, mem 1.8 GiB @ 00 01:22:31.609, mean 00 00:00:00.017
step 275500: train loss 2.4734778, val loss 2.4840093, mem 1.8 GiB @ 00 01:22:40.394, mean 00 00:00:00.017
step 276000: train loss 2.4824631, val loss 2.4823062, mem 1.8 GiB @ 00 01:22:49.180, mean 00 00:00:00.017
step 276500: train loss 2.4876065, val loss 2.485425, mem 1.8 GiB @ 00 01:22:57.970, mean 00 00:00:00.017
step 277000: train loss 2.5129378, val loss 2.504, mem 1.8 GiB @ 00 01:23:07.094, mean 00 00:00:00.018
step 277500: train loss 2.4960916, val loss 2.4952676, mem 1.8 GiB @ 00 01:23:16.245, mean 00 00:00:00.018
step 278000: train loss 2.4949403, val loss 2.4982271, mem 1.8 GiB @ 00 01:23:25.088, mean 00 00:00:00.017
step 278500: train loss 2.500837, val loss 2.4798872, mem 1.8 GiB @ 00 01:23:33.891, mean 00 00:00:00.017
step 279000: train loss 2.4955537, val loss 2.496896, mem 1.8 GiB @ 00 01:23:42.679, mean 00 00:00:00.017
step 279500: train loss 2.4915924, val loss 2.4753497, mem 1.8 GiB @ 00 01:23:51.493, mean 00 00:00:00.017
step 280000: train loss 2.4959292, val loss 2.4794152, mem 1.8 GiB @ 00 01:24:00.174, mean 00 00:00:00.017
step 280500: train loss 2.4968822, val loss 2.4823785, mem 1.8 GiB @ 00 01:24:08.967, mean 00 00:00:00.017
step 281000: train loss 2.4881046, val loss 2.4879634, mem 1.8 GiB @ 00 01:24:17.783, mean 00 00:00:00.017
step 281500: train loss 2.4887762, val loss 2.492389, mem 1.8 GiB @ 00 01:24:26.488, mean 00 00:00:00.017
step 282000: train loss 2.5069928, val loss 2.4664295, mem 1.8 GiB @ 00 01:24:35.286, mean 00 00:00:00.017
step 282500: train loss 2.4633038, val loss 2.4828694, mem 1.8 GiB @ 00 01:24:44.146, mean 00 00:00:00.017
step 283000: train loss 2.4844198, val loss 2.493164, mem 1.8 GiB @ 00 01:24:53.283, mean 00 00:00:00.018
step 283500: train loss 2.478184, val loss 2.472743, mem 1.8 GiB @ 00 01:25:02.563, mean 00 00:00:00.018
step 284000: train loss 2.4853072, val loss 2.4659486, mem 1.8 GiB @ 00 01:25:11.778, mean 00 00:00:00.018
step 284500: train loss 2.4611216, val loss 2.4805343, mem 1.8 GiB @ 00 01:25:21.014, mean 00 00:00:00.018
step 285000: train loss 2.466699, val loss 2.4537911, mem 1.8 GiB @ 00 01:25:30.321, mean 00 00:00:00.018
step 285500: train loss 2.4739532, val loss 2.4848657, mem 1.8 GiB @ 00 01:25:39.448, mean 00 00:00:00.018
step 286000: train loss 2.4675345, val loss 2.467344, mem 1.8 GiB @ 00 01:25:48.343, mean 00 00:00:00.017
step 286500: train loss 2.4669843, val loss 2.4749553, mem 1.8 GiB @ 00 01:25:57.257, mean 00 00:00:00.017
step 287000: train loss 2.4674346, val loss 2.4856534, mem 1.8 GiB @ 00 01:26:06.465, mean 00 00:00:00.018
step 287500: train loss 2.460893, val loss 2.4816477, mem 1.8 GiB @ 00 01:26:15.445, mean 00 00:00:00.017
step 288000: train loss 2.4576397, val loss 2.4672923, mem 1.8 GiB @ 00 01:26:24.429, mean 00 00:00:00.017
step 288500: train loss 2.4659314, val loss 2.4575984, mem 1.8 GiB @ 00 01:26:33.498, mean 00 00:00:00.018
step 289000: train loss 2.4737456, val loss 2.469994, mem 1.8 GiB @ 00 01:26:42.325, mean 00 00:00:00.017
step 289500: train loss 2.4928613, val loss 2.4678295, mem 1.8 GiB @ 00 01:26:51.205, mean 00 00:00:00.017
step 290000: train loss 2.468714, val loss 2.4604895, mem 1.8 GiB @ 00 01:27:00.606, mean 00 00:00:00.018
step 290500: train loss 2.451737, val loss 2.4716587, mem 1.8 GiB @ 00 01:27:09.907, mean 00 00:00:00.018
step 291000: train loss 2.4641347, val loss 2.453641, mem 1.8 GiB @ 00 01:27:19.220, mean 00 00:00:00.018
step 291500: train loss 2.4581294, val loss 2.4505703, mem 1.8 GiB @ 00 01:27:28.380, mean 00 00:00:00.018
step 292000: train loss 2.484058, val loss 2.462889, mem 1.8 GiB @ 00 01:27:37.526, mean 00 00:00:00.018
step 292500: train loss 2.4672694, val loss 2.449896, mem 1.8 GiB @ 00 01:27:46.659, mean 00 00:00:00.018
step 293000: train loss 2.47823, val loss 2.472078, mem 1.8 GiB @ 00 01:27:55.731, mean 00 00:00:00.018
step 293500: train loss 2.4560466, val loss 2.4728088, mem 1.8 GiB @ 00 01:28:05.049, mean 00 00:00:00.018
step 294000: train loss 2.4653146, val loss 2.4496236, mem 1.8 GiB @ 00 01:28:14.159, mean 00 00:00:00.018
step 294500: train loss 2.4654021, val loss 2.4478583, mem 1.8 GiB @ 00 01:28:23.155, mean 00 00:00:00.017
step 295000: train loss 2.4723837, val loss 2.4642143, mem 1.8 GiB @ 00 01:28:32.361, mean 00 00:00:00.018
step 295500: train loss 2.462186, val loss 2.459149, mem 1.8 GiB @ 00 01:28:41.421, mean 00 00:00:00.018
step 296000: train loss 2.454473, val loss 2.4626558, mem 1.8 GiB @ 00 01:28:50.422, mean 00 00:00:00.018
step 296500: train loss 2.461655, val loss 2.4511588, mem 1.8 GiB @ 00 01:28:59.204, mean 00 00:00:00.017
step 297000: train loss 2.4678662, val loss 2.46075, mem 1.8 GiB @ 00 01:29:08.305, mean 00 00:00:00.018
step 297500: train loss 2.4409657, val loss 2.4617302, mem 1.8 GiB @ 00 01:29:17.391, mean 00 00:00:00.018
step 298000: train loss 2.450445, val loss 2.4582796, mem 1.8 GiB @ 00 01:29:26.205, mean 00 00:00:00.017
step 298500: train loss 2.4674633, val loss 2.453724, mem 1.8 GiB @ 00 01:29:35.032, mean 00 00:00:00.017
step 299000: train loss 2.4713864, val loss 2.4546542, mem 1.8 GiB @ 00 01:29:44.109, mean 00 00:00:00.018
step 299500: train loss 2.4535866, val loss 2.4553936, mem 1.8 GiB @ 00 01:29:53.251, mean 00 00:00:00.018
step 300000: train loss 2.4541569, val loss 2.4606965, mem 1.8 GiB @ 00 01:30:02.224, mean 00 00:00:00.017
step 300500: train loss 2.457836, val loss 2.4612365, mem 1.8 GiB @ 00 01:30:11.375, mean 00 00:00:00.018
step 301000: train loss 2.4611228, val loss 2.4508858, mem 1.8 GiB @ 00 01:30:20.555, mean 00 00:00:00.018
step 301500: train loss 2.4387112, val loss 2.4723322, mem 1.8 GiB @ 00 01:30:29.650, mean 00 00:00:00.018
step 302000: train loss 2.4578214, val loss 2.4586601, mem 1.8 GiB @ 00 01:30:38.710, mean 00 00:00:00.018
step 302500: train loss 2.4447916, val loss 2.4624999, mem 1.8 GiB @ 00 01:30:47.823, mean 00 00:00:00.018
step 303000: train loss 2.4315236, val loss 2.4321659, mem 1.8 GiB @ 00 01:30:57.000, mean 00 00:00:00.018
step 303500: train loss 2.4527411, val loss 2.4450252, mem 1.8 GiB @ 00 01:31:06.150, mean 00 00:00:00.018
step 304000: train loss 2.4637864, val loss 2.4465292, mem 1.8 GiB @ 00 01:31:15.089, mean 00 00:00:00.017
step 304500: train loss 2.4525595, val loss 2.4473884, mem 1.8 GiB @ 00 01:31:24.098, mean 00 00:00:00.018
step 305000: train loss 2.4405458, val loss 2.4439974, mem 1.8 GiB @ 00 01:31:33.185, mean 00 00:00:00.018
step 305500: train loss 2.450217, val loss 2.4459894, mem 1.8 GiB @ 00 01:31:41.937, mean 00 00:00:00.017
step 306000: train loss 2.4608033, val loss 2.4375963, mem 1.8 GiB @ 00 01:31:50.986, mean 00 00:00:00.018
step 306500: train loss 2.4434175, val loss 2.4332256, mem 1.8 GiB @ 00 01:32:00.121, mean 00 00:00:00.018
step 307000: train loss 2.4494884, val loss 2.444502, mem 1.8 GiB @ 00 01:32:09.275, mean 00 00:00:00.018
step 307500: train loss 2.4316363, val loss 2.4478652, mem 1.8 GiB @ 00 01:32:18.769, mean 00 00:00:00.018
step 308000: train loss 2.4512746, val loss 2.445525, mem 1.8 GiB @ 00 01:32:27.530, mean 00 00:00:00.017
step 308500: train loss 2.4581046, val loss 2.441375, mem 1.8 GiB @ 00 01:32:36.605, mean 00 00:00:00.018
step 309000: train loss 2.447893, val loss 2.4397483, mem 1.8 GiB @ 00 01:32:45.749, mean 00 00:00:00.018
step 309500: train loss 2.4437149, val loss 2.4435763, mem 1.8 GiB @ 00 01:32:54.910, mean 00 00:00:00.018
step 310000: train loss 2.460689, val loss 2.442825, mem 1.8 GiB @ 00 01:33:03.998, mean 00 00:00:00.018
step 310500: train loss 2.4500148, val loss 2.4396646, mem 1.8 GiB @ 00 01:33:12.973, mean 00 00:00:00.017
step 311000: train loss 2.4652932, val loss 2.450757, mem 1.8 GiB @ 00 01:33:22.255, mean 00 00:00:00.018
step 311500: train loss 2.4471123, val loss 2.441764, mem 1.8 GiB @ 00 01:33:31.564, mean 00 00:00:00.018
step 312000: train loss 2.4393947, val loss 2.4596093, mem 1.8 GiB @ 00 01:33:40.487, mean 00 00:00:00.017
step 312500: train loss 2.439619, val loss 2.4401944, mem 1.8 GiB @ 00 01:33:49.631, mean 00 00:00:00.018
step 313000: train loss 2.4350023, val loss 2.4342606, mem 1.8 GiB @ 00 01:33:58.886, mean 00 00:00:00.018
step 313500: train loss 2.4510732, val loss 2.437338, mem 1.8 GiB @ 00 01:34:07.905, mean 00 00:00:00.018
step 314000: train loss 2.4380753, val loss 2.4517314, mem 1.8 GiB @ 00 01:34:17.033, mean 00 00:00:00.018
step 314500: train loss 2.440372, val loss 2.4328933, mem 1.8 GiB @ 00 01:34:26.249, mean 00 00:00:00.018
step 315000: train loss 2.438684, val loss 2.4446712, mem 1.8 GiB @ 00 01:34:35.319, mean 00 00:00:00.018
step 315500: train loss 2.4343712, val loss 2.4389482, mem 1.8 GiB @ 00 01:34:44.466, mean 00 00:00:00.018
step 316000: train loss 2.4493341, val loss 2.453184, mem 1.8 GiB @ 00 01:34:53.687, mean 00 00:00:00.018
step 316500: train loss 2.4420295, val loss 2.4386802, mem 1.8 GiB @ 00 01:35:02.889, mean 00 00:00:00.018
step 317000: train loss 2.4443822, val loss 2.4338913, mem 1.8 GiB @ 00 01:35:11.936, mean 00 00:00:00.018
step 317500: train loss 2.4345539, val loss 2.4277682, mem 1.8 GiB @ 00 01:35:21.084, mean 00 00:00:00.018
step 318000: train loss 2.448233, val loss 2.4442809, mem 1.8 GiB @ 00 01:35:30.574, mean 00 00:00:00.018
step 318500: train loss 2.4410906, val loss 2.4420981, mem 1.8 GiB @ 00 01:35:39.928, mean 00 00:00:00.018
step 319000: train loss 2.4443424, val loss 2.4302993, mem 1.8 GiB @ 00 01:35:49.104, mean 00 00:00:00.018
step 319500: train loss 2.446393, val loss 2.441452, mem 1.8 GiB @ 00 01:35:58.403, mean 00 00:00:00.018
step 320000: train loss 2.4503496, val loss 2.4373574, mem 1.8 GiB @ 00 01:36:07.685, mean 00 00:00:00.018
step 320500: train loss 2.4339416, val loss 2.4392054, mem 1.8 GiB @ 00 01:36:17.187, mean 00 00:00:00.019
step 321000: train loss 2.4366395, val loss 2.4251125, mem 1.8 GiB @ 00 01:36:26.543, mean 00 00:00:00.018
step 321500: train loss 2.4349153, val loss 2.4335637, mem 1.8 GiB @ 00 01:36:35.656, mean 00 00:00:00.018
step 322000: train loss 2.4512765, val loss 2.4433956, mem 1.8 GiB @ 00 01:36:44.951, mean 00 00:00:00.018
step 322500: train loss 2.4413161, val loss 2.4348395, mem 1.8 GiB @ 00 01:36:54.008, mean 00 00:00:00.018
step 323000: train loss 2.4341264, val loss 2.4412436, mem 1.8 GiB @ 00 01:37:03.513, mean 00 00:00:00.019
step 323500: train loss 2.4345622, val loss 2.431275, mem 1.8 GiB @ 00 01:37:12.499, mean 00 00:00:00.017
step 324000: train loss 2.432215, val loss 2.420378, mem 1.8 GiB @ 00 01:37:21.217, mean 00 00:00:00.017
step 324500: train loss 2.4243083, val loss 2.417802, mem 1.8 GiB @ 00 01:37:30.024, mean 00 00:00:00.017
step 325000: train loss 2.4346843, val loss 2.4273462, mem 1.8 GiB @ 00 01:37:39.036, mean 00 00:00:00.018
step 325500: train loss 2.4244695, val loss 2.4063392, mem 1.8 GiB @ 00 01:37:48.181, mean 00 00:00:00.018
step 326000: train loss 2.4223616, val loss 2.4371176, mem 1.8 GiB @ 00 01:37:57.224, mean 00 00:00:00.018
step 326500: train loss 2.4381638, val loss 2.4546845, mem 1.8 GiB @ 00 01:38:06.126, mean 00 00:00:00.017
step 327000: train loss 2.4318724, val loss 2.4192364, mem 1.8 GiB @ 00 01:38:14.943, mean 00 00:00:00.017
step 327500: train loss 2.4380782, val loss 2.4305928, mem 1.8 GiB @ 00 01:38:23.706, mean 00 00:00:00.017
step 328000: train loss 2.4299731, val loss 2.4463315, mem 1.8 GiB @ 00 01:38:32.475, mean 00 00:00:00.017
step 328500: train loss 2.4263108, val loss 2.4262087, mem 1.8 GiB @ 00 01:38:41.238, mean 00 00:00:00.017
step 329000: train loss 2.4191794, val loss 2.4130962, mem 1.8 GiB @ 00 01:38:50.106, mean 00 00:00:00.017
step 329500: train loss 2.4348552, val loss 2.4280515, mem 1.8 GiB @ 00 01:38:59.223, mean 00 00:00:00.018
step 330000: train loss 2.4295897, val loss 2.4214635, mem 1.8 GiB @ 00 01:39:08.354, mean 00 00:00:00.018
step 330500: train loss 2.42987, val loss 2.4410412, mem 1.8 GiB @ 00 01:39:17.517, mean 00 00:00:00.018
step 331000: train loss 2.4240541, val loss 2.4203851, mem 1.8 GiB @ 00 01:39:26.680, mean 00 00:00:00.018
step 331500: train loss 2.4434175, val loss 2.4256203, mem 1.8 GiB @ 00 01:39:35.860, mean 00 00:00:00.018
step 332000: train loss 2.422635, val loss 2.4199014, mem 1.8 GiB @ 00 01:39:45.025, mean 00 00:00:00.018
step 332500: train loss 2.4335017, val loss 2.4358044, mem 1.8 GiB @ 00 01:39:54.171, mean 00 00:00:00.018
step 333000: train loss 2.4305584, val loss 2.4298432, mem 1.8 GiB @ 00 01:40:03.228, mean 00 00:00:00.018
step 333500: train loss 2.4380465, val loss 2.4241312, mem 1.8 GiB @ 00 01:40:12.127, mean 00 00:00:00.017
step 334000: train loss 2.4202397, val loss 2.4223871, mem 1.8 GiB @ 00 01:40:20.927, mean 00 00:00:00.017
step 334500: train loss 2.4128385, val loss 2.4151232, mem 1.8 GiB @ 00 01:40:29.739, mean 00 00:00:00.017
step 335000: train loss 2.4262135, val loss 2.4343655, mem 1.8 GiB @ 00 01:40:38.810, mean 00 00:00:00.018
step 335500: train loss 2.4198608, val loss 2.428112, mem 1.8 GiB @ 00 01:40:47.694, mean 00 00:00:00.017
step 336000: train loss 2.4342926, val loss 2.4344656, mem 1.8 GiB @ 00 01:40:56.404, mean 00 00:00:00.017
step 336500: train loss 2.4267435, val loss 2.4210875, mem 1.8 GiB @ 00 01:41:05.191, mean 00 00:00:00.017
step 337000: train loss 2.4455545, val loss 2.4247575, mem 1.8 GiB @ 00 01:41:14.131, mean 00 00:00:00.017
step 337500: train loss 2.4160745, val loss 2.4380069, mem 1.8 GiB @ 00 01:41:23.285, mean 00 00:00:00.018
step 338000: train loss 2.421518, val loss 2.4098623, mem 1.8 GiB @ 00 01:41:32.311, mean 00 00:00:00.018
step 338500: train loss 2.415923, val loss 2.4140372, mem 1.8 GiB @ 00 01:41:41.359, mean 00 00:00:00.018
step 339000: train loss 2.435879, val loss 2.427478, mem 1.8 GiB @ 00 01:41:50.504, mean 00 00:00:00.018
step 339500: train loss 2.4219196, val loss 2.4103038, mem 1.8 GiB @ 00 01:41:59.646, mean 00 00:00:00.018
step 340000: train loss 2.652295, val loss 2.6822727, mem 1.8 GiB @ 00 01:42:08.916, mean 00 00:00:00.018
step 340500: train loss 2.7898743, val loss 2.7690716, mem 1.8 GiB @ 00 01:42:18.042, mean 00 00:00:00.018
step 341000: train loss 2.7199295, val loss 2.7232447, mem 1.8 GiB @ 00 01:42:26.885, mean 00 00:00:00.017
step 341500: train loss 2.676015, val loss 2.7101068, mem 1.8 GiB @ 00 01:42:35.724, mean 00 00:00:00.017
step 342000: train loss 2.6557055, val loss 2.669767, mem 1.8 GiB @ 00 01:42:44.735, mean 00 00:00:00.018
step 342500: train loss 2.626028, val loss 2.6465673, mem 1.8 GiB @ 00 01:42:53.763, mean 00 00:00:00.018
step 343000: train loss 2.5983799, val loss 2.6087084, mem 1.8 GiB @ 00 01:43:02.908, mean 00 00:00:00.018
step 343500: train loss 2.5772839, val loss 2.5800054, mem 1.8 GiB @ 00 01:43:12.053, mean 00 00:00:00.018
step 344000: train loss 2.5628796, val loss 2.5641806, mem 1.8 GiB @ 00 01:43:21.204, mean 00 00:00:00.018
step 344500: train loss 2.5434506, val loss 2.5455234, mem 1.8 GiB @ 00 01:43:30.244, mean 00 00:00:00.018
step 345000: train loss 2.527355, val loss 2.5372653, mem 1.8 GiB @ 00 01:43:39.419, mean 00 00:00:00.018
step 345500: train loss 2.510504, val loss 2.545458, mem 1.8 GiB @ 00 01:43:48.616, mean 00 00:00:00.018
step 346000: train loss 2.5145416, val loss 2.5152042, mem 1.8 GiB @ 00 01:43:57.658, mean 00 00:00:00.018
step 346500: train loss 2.5008016, val loss 2.4992008, mem 1.8 GiB @ 00 01:44:06.809, mean 00 00:00:00.018
step 347000: train loss 2.505157, val loss 2.5046694, mem 1.8 GiB @ 00 01:44:15.766, mean 00 00:00:00.017
step 347500: train loss 2.4828694, val loss 2.4956229, mem 1.8 GiB @ 00 01:44:24.482, mean 00 00:00:00.017
step 348000: train loss 2.4716868, val loss 2.4826021, mem 1.8 GiB @ 00 01:44:33.379, mean 00 00:00:00.017
step 348500: train loss 2.4738767, val loss 2.4863489, mem 1.8 GiB @ 00 01:44:42.389, mean 00 00:00:00.018
step 349000: train loss 2.4797103, val loss 2.4753182, mem 1.8 GiB @ 00 01:44:51.216, mean 00 00:00:00.017
step 349500: train loss 2.487958, val loss 2.4780962, mem 1.8 GiB @ 00 01:45:00.014, mean 00 00:00:00.017
step 350000: train loss 2.4612567, val loss 2.4783213, mem 1.8 GiB @ 00 01:45:08.786, mean 00 00:00:00.017
step 350500: train loss 2.4696252, val loss 2.4540868, mem 1.8 GiB @ 00 01:45:17.586, mean 00 00:00:00.017
step 351000: train loss 2.4587543, val loss 2.4572601, mem 1.8 GiB @ 00 01:45:26.822, mean 00 00:00:00.018
step 351500: train loss 2.4680867, val loss 2.4577105, mem 1.8 GiB @ 00 01:45:36.223, mean 00 00:00:00.018
step 352000: train loss 2.4464958, val loss 2.4457538, mem 1.8 GiB @ 00 01:45:45.431, mean 00 00:00:00.018
step 352500: train loss 2.4521053, val loss 2.4668896, mem 1.8 GiB @ 00 01:45:54.593, mean 00 00:00:00.018
step 353000: train loss 2.4511774, val loss 2.4608147, mem 1.8 GiB @ 00 01:46:04.049, mean 00 00:00:00.018
step 353500: train loss 2.4566035, val loss 2.4562044, mem 1.8 GiB @ 00 01:46:13.348, mean 00 00:00:00.018
step 354000: train loss 2.4548883, val loss 2.4581301, mem 1.8 GiB @ 00 01:46:22.624, mean 00 00:00:00.018
step 354500: train loss 2.4652588, val loss 2.4546378, mem 1.8 GiB @ 00 01:46:31.912, mean 00 00:00:00.018
step 355000: train loss 2.4522305, val loss 2.452346, mem 1.8 GiB @ 00 01:46:41.202, mean 00 00:00:00.018
step 355500: train loss 2.434949, val loss 2.4499092, mem 1.8 GiB @ 00 01:46:50.561, mean 00 00:00:00.018
step 356000: train loss 2.4552615, val loss 2.4408555, mem 1.8 GiB @ 00 01:46:59.765, mean 00 00:00:00.018
step 356500: train loss 2.432456, val loss 2.4355164, mem 1.8 GiB @ 00 01:47:09.176, mean 00 00:00:00.018
step 357000: train loss 2.444455, val loss 2.4449995, mem 1.8 GiB @ 00 01:47:18.197, mean 00 00:00:00.018
step 357500: train loss 2.4449456, val loss 2.4391365, mem 1.8 GiB @ 00 01:47:27.304, mean 00 00:00:00.018
step 358000: train loss 2.4336784, val loss 2.4518769, mem 1.8 GiB @ 00 01:47:36.408, mean 00 00:00:00.018
step 358500: train loss 2.4418964, val loss 2.4358761, mem 1.8 GiB @ 00 01:47:45.469, mean 00 00:00:00.018
step 359000: train loss 2.4558136, val loss 2.440374, mem 1.8 GiB @ 00 01:47:54.690, mean 00 00:00:00.018
step 359500: train loss 2.4257638, val loss 2.44186, mem 1.8 GiB @ 00 01:48:03.831, mean 00 00:00:00.018
step 360000: train loss 2.4286563, val loss 2.441392, mem 1.8 GiB @ 00 01:48:12.970, mean 00 00:00:00.018
step 360500: train loss 2.4381099, val loss 2.436402, mem 1.8 GiB @ 00 01:48:22.016, mean 00 00:00:00.018
step 361000: train loss 2.4241579, val loss 2.4252224, mem 1.8 GiB @ 00 01:48:31.172, mean 00 00:00:00.018
step 361500: train loss 2.4413903, val loss 2.4077175, mem 1.8 GiB @ 00 01:48:40.331, mean 00 00:00:00.018
step 362000: train loss 2.4362407, val loss 2.4189215, mem 1.8 GiB @ 00 01:48:49.498, mean 00 00:00:00.018
step 362500: train loss 2.4384396, val loss 2.430536, mem 1.8 GiB @ 00 01:48:58.659, mean 00 00:00:00.018
step 363000: train loss 2.4485288, val loss 2.4239888, mem 1.8 GiB @ 00 01:49:07.826, mean 00 00:00:00.018
step 363500: train loss 2.4422164, val loss 2.438103, mem 1.8 GiB @ 00 01:49:16.910, mean 00 00:00:00.018
step 364000: train loss 2.4435146, val loss 2.4371295, mem 1.8 GiB @ 00 01:49:25.709, mean 00 00:00:00.017
step 364500: train loss 2.4258716, val loss 2.4377074, mem 1.8 GiB @ 00 01:49:34.392, mean 00 00:00:00.017
step 365000: train loss 2.4192746, val loss 2.4291909, mem 1.8 GiB @ 00 01:49:43.246, mean 00 00:00:00.017
step 365500: train loss 2.4386652, val loss 2.4285774, mem 1.8 GiB @ 00 01:49:52.119, mean 00 00:00:00.017
step 366000: train loss 2.4407601, val loss 2.441177, mem 1.8 GiB @ 00 01:50:00.885, mean 00 00:00:00.017
step 366500: train loss 2.416405, val loss 2.4223332, mem 1.8 GiB @ 00 01:50:09.675, mean 00 00:00:00.017
step 367000: train loss 2.4387212, val loss 2.4265435, mem 1.8 GiB @ 00 01:50:18.498, mean 00 00:00:00.017
step 367500: train loss 2.4383786, val loss 2.4349, mem 1.8 GiB @ 00 01:50:27.257, mean 00 00:00:00.017
step 368000: train loss 2.4678714, val loss 2.4248204, mem 1.8 GiB @ 00 01:50:36.018, mean 00 00:00:00.017
step 368500: train loss 2.4239264, val loss 2.4271057, mem 1.8 GiB @ 00 01:50:44.855, mean 00 00:00:00.017
step 369000: train loss 2.4417288, val loss 2.4165223, mem 1.8 GiB @ 00 01:50:53.663, mean 00 00:00:00.017
step 369500: train loss 2.424456, val loss 2.4187992, mem 1.8 GiB @ 00 01:51:02.594, mean 00 00:00:00.017
step 370000: train loss 2.4251, val loss 2.4222875, mem 1.8 GiB @ 00 01:51:11.592, mean 00 00:00:00.017
step 370500: train loss 2.4180276, val loss 2.4239607, mem 1.8 GiB @ 00 01:51:20.365, mean 00 00:00:00.017
step 371000: train loss 2.4383798, val loss 2.4350317, mem 1.8 GiB @ 00 01:51:29.106, mean 00 00:00:00.017
step 371500: train loss 2.4398546, val loss 2.4260116, mem 1.8 GiB @ 00 01:51:37.876, mean 00 00:00:00.017
step 372000: train loss 2.4368112, val loss 2.4323637, mem 1.8 GiB @ 00 01:51:46.705, mean 00 00:00:00.017
step 372500: train loss 2.435869, val loss 2.4217334, mem 1.8 GiB @ 00 01:51:55.977, mean 00 00:00:00.018
step 373000: train loss 2.4316614, val loss 2.4218917, mem 1.8 GiB @ 00 01:52:05.110, mean 00 00:00:00.018
step 373500: train loss 2.4206614, val loss 2.428714, mem 1.8 GiB @ 00 01:52:14.231, mean 00 00:00:00.018
step 374000: train loss 2.4330847, val loss 2.40299, mem 1.8 GiB @ 00 01:52:23.261, mean 00 00:00:00.018
step 374500: train loss 2.418499, val loss 2.4116082, mem 1.8 GiB @ 00 01:52:32.410, mean 00 00:00:00.018
step 375000: train loss 2.4226122, val loss 2.4235678, mem 1.8 GiB @ 00 01:52:41.673, mean 00 00:00:00.018
step 375500: train loss 2.4187577, val loss 2.4194658, mem 1.8 GiB @ 00 01:52:50.845, mean 00 00:00:00.018
step 376000: train loss 2.4141326, val loss 2.4147115, mem 1.8 GiB @ 00 01:52:59.560, mean 00 00:00:00.017
step 376500: train loss 2.430538, val loss 2.4121327, mem 1.8 GiB @ 00 01:53:08.276, mean 00 00:00:00.017
step 377000: train loss 2.4519615, val loss 2.4226959, mem 1.8 GiB @ 00 01:53:17.005, mean 00 00:00:00.017
step 377500: train loss 2.4224124, val loss 2.4019668, mem 1.8 GiB @ 00 01:53:25.948, mean 00 00:00:00.017
step 378000: train loss 2.4212193, val loss 2.4116938, mem 1.8 GiB @ 00 01:53:34.857, mean 00 00:00:00.017
step 378500: train loss 2.424057, val loss 2.4263124, mem 1.8 GiB @ 00 01:53:43.650, mean 00 00:00:00.017
step 379000: train loss 2.4228716, val loss 2.4053323, mem 1.8 GiB @ 00 01:53:52.528, mean 00 00:00:00.017
step 379500: train loss 2.4134047, val loss 2.4375808, mem 1.8 GiB @ 00 01:54:01.357, mean 00 00:00:00.017
step 380000: train loss 2.4309914, val loss 2.4138079, mem 1.8 GiB @ 00 01:54:10.289, mean 00 00:00:00.017
step 380500: train loss 2.4326022, val loss 2.4211283, mem 1.8 GiB @ 00 01:54:19.349, mean 00 00:00:00.018
step 381000: train loss 2.4219337, val loss 2.4316504, mem 1.8 GiB @ 00 01:54:28.406, mean 00 00:00:00.018
step 381500: train loss 2.4249458, val loss 2.4061646, mem 1.8 GiB @ 00 01:54:37.407, mean 00 00:00:00.018
step 382000: train loss 2.412233, val loss 2.395365, mem 1.8 GiB @ 00 01:54:46.632, mean 00 00:00:00.018
step 382500: train loss 2.4123247, val loss 2.4008293, mem 1.8 GiB @ 00 01:54:56.039, mean 00 00:00:00.018
step 383000: train loss 2.4233522, val loss 2.4114609, mem 1.8 GiB @ 00 01:55:04.859, mean 00 00:00:00.017
step 383500: train loss 2.3935232, val loss 2.4115074, mem 1.8 GiB @ 00 01:55:13.674, mean 00 00:00:00.017
step 384000: train loss 2.4053967, val loss 2.41368, mem 1.8 GiB @ 00 01:55:22.662, mean 00 00:00:00.017
step 384500: train loss 2.422283, val loss 2.4269755, mem 1.8 GiB @ 00 01:55:31.786, mean 00 00:00:00.018
step 385000: train loss 2.419432, val loss 2.403616, mem 1.8 GiB @ 00 01:55:40.916, mean 00 00:00:00.018
step 385500: train loss 2.413404, val loss 2.389704, mem 1.8 GiB @ 00 01:55:50.069, mean 00 00:00:00.018
step 386000: train loss 2.4151437, val loss 2.4142435, mem 1.8 GiB @ 00 01:55:59.209, mean 00 00:00:00.018
step 386500: train loss 2.4271908, val loss 2.4151316, mem 1.8 GiB @ 00 01:56:08.321, mean 00 00:00:00.018
step 387000: train loss 2.4165332, val loss 2.423218, mem 1.8 GiB @ 00 01:56:17.447, mean 00 00:00:00.018
step 387500: train loss 2.4076636, val loss 2.412603, mem 1.8 GiB @ 00 01:56:26.581, mean 00 00:00:00.018
step 388000: train loss 2.4042504, val loss 2.3971028, mem 1.8 GiB @ 00 01:56:35.733, mean 00 00:00:00.018
step 388500: train loss 2.419165, val loss 2.40996, mem 1.8 GiB @ 00 01:56:44.864, mean 00 00:00:00.018
step 389000: train loss 2.414425, val loss 2.3995209, mem 1.8 GiB @ 00 01:56:54.027, mean 00 00:00:00.018
step 389500: train loss 2.4185703, val loss 2.4084232, mem 1.8 GiB @ 00 01:57:03.177, mean 00 00:00:00.018
step 390000: train loss 2.4128482, val loss 2.3903246, mem 1.8 GiB @ 00 01:57:12.360, mean 00 00:00:00.018
step 390500: train loss 2.400638, val loss 2.4276297, mem 1.8 GiB @ 00 01:57:21.497, mean 00 00:00:00.018
step 391000: train loss 2.412008, val loss 2.4019094, mem 1.8 GiB @ 00 01:57:30.523, mean 00 00:00:00.018
step 391500: train loss 2.4059381, val loss 2.4047198, mem 1.8 GiB @ 00 01:57:39.651, mean 00 00:00:00.018
step 392000: train loss 2.414154, val loss 2.41641, mem 1.8 GiB @ 00 01:57:48.755, mean 00 00:00:00.018
step 392500: train loss 2.406269, val loss 2.4017627, mem 1.8 GiB @ 00 01:57:57.880, mean 00 00:00:00.018
step 393000: train loss 2.4152787, val loss 2.4168036, mem 1.8 GiB @ 00 01:58:06.889, mean 00 00:00:00.018
step 393500: train loss 2.4025655, val loss 2.4080863, mem 1.8 GiB @ 00 01:58:16.054, mean 00 00:00:00.018
step 394000: train loss 2.41546, val loss 2.4059029, mem 1.8 GiB @ 00 01:58:25.196, mean 00 00:00:00.018
step 394500: train loss 2.4065962, val loss 2.403012, mem 1.8 GiB @ 00 01:58:34.357, mean 00 00:00:00.018
step 395000: train loss 2.4524796, val loss 2.4001129, mem 1.8 GiB @ 00 01:58:43.688, mean 00 00:00:00.018
step 395500: train loss 2.409124, val loss 2.383219, mem 1.8 GiB @ 00 01:58:52.846, mean 00 00:00:00.018
step 396000: train loss 2.3872268, val loss 2.3950167, mem 1.8 GiB @ 00 01:59:02.149, mean 00 00:00:00.018
step 396500: train loss 2.4028306, val loss 2.3995686, mem 1.8 GiB @ 00 01:59:11.504, mean 00 00:00:00.018
step 397000: train loss 2.3996212, val loss 2.3991694, mem 1.8 GiB @ 00 01:59:20.998, mean 00 00:00:00.018
step 397500: train loss 2.4242122, val loss 2.4042957, mem 1.8 GiB @ 00 01:59:30.451, mean 00 00:00:00.018
step 398000: train loss 2.3924987, val loss 2.4111295, mem 1.8 GiB @ 00 01:59:39.741, mean 00 00:00:00.018
step 398500: train loss 2.4192376, val loss 2.4141915, mem 1.8 GiB @ 00 01:59:48.948, mean 00 00:00:00.018
step 399000: train loss 2.3974004, val loss 2.4030101, mem 1.8 GiB @ 00 01:59:58.389, mean 00 00:00:00.018
step 399500: train loss 2.4125965, val loss 2.4000788, mem 1.8 GiB @ 00 02:00:07.557, mean 00 00:00:00.018
step 400000: train loss 2.4468613, val loss 2.4148867, mem 1.8 GiB @ 00 02:00:16.729, mean 00 00:00:00.018
step 400500: train loss 2.4123094, val loss 2.396364, mem 1.8 GiB @ 00 02:00:25.769, mean 00 00:00:00.018
step 401000: train loss 2.398114, val loss 2.4085965, mem 1.8 GiB @ 00 02:00:34.926, mean 00 00:00:00.018
step 401500: train loss 2.39101, val loss 2.3990896, mem 1.8 GiB @ 00 02:00:44.102, mean 00 00:00:00.018
step 402000: train loss 2.3826494, val loss 2.3985615, mem 1.8 GiB @ 00 02:00:53.167, mean 00 00:00:00.018
step 402500: train loss 2.3917272, val loss 2.4007378, mem 1.8 GiB @ 00 02:01:02.538, mean 00 00:00:00.018
step 403000: train loss 2.3997507, val loss 2.386026, mem 1.8 GiB @ 00 02:01:11.732, mean 00 00:00:00.018
step 403500: train loss 2.4025493, val loss 2.397488, mem 1.8 GiB @ 00 02:01:20.771, mean 00 00:00:00.018
step 404000: train loss 2.393354, val loss 2.411638, mem 1.8 GiB @ 00 02:01:29.910, mean 00 00:00:00.018
step 404500: train loss 2.3991313, val loss 2.3908591, mem 1.8 GiB @ 00 02:01:39.040, mean 00 00:00:00.018
step 405000: train loss 2.3968866, val loss 2.3968182, mem 1.8 GiB @ 00 02:01:48.205, mean 00 00:00:00.018
step 405500: train loss 2.409394, val loss 2.3976352, mem 1.8 GiB @ 00 02:01:57.403, mean 00 00:00:00.018
step 406000: train loss 2.4165103, val loss 2.3910544, mem 1.8 GiB @ 00 02:02:06.561, mean 00 00:00:00.018
step 406500: train loss 2.3946478, val loss 2.393726, mem 1.8 GiB @ 00 02:02:15.624, mean 00 00:00:00.018
step 407000: train loss 2.3907726, val loss 2.4040008, mem 1.8 GiB @ 00 02:02:24.767, mean 00 00:00:00.018
step 407500: train loss 2.4023483, val loss 2.4013793, mem 1.8 GiB @ 00 02:02:33.906, mean 00 00:00:00.018
step 408000: train loss 2.3950434, val loss 2.3875606, mem 1.8 GiB @ 00 02:02:42.757, mean 00 00:00:00.017
step 408500: train loss 2.378699, val loss 2.388443, mem 1.8 GiB @ 00 02:02:51.790, mean 00 00:00:00.018
step 409000: train loss 2.4001055, val loss 2.3878806, mem 1.8 GiB @ 00 02:03:01.140, mean 00 00:00:00.018
step 409500: train loss 2.393835, val loss 2.3996031, mem 1.8 GiB @ 00 02:03:10.290, mean 00 00:00:00.018
step 410000: train loss 2.396568, val loss 2.4047532, mem 1.8 GiB @ 00 02:03:19.449, mean 00 00:00:00.018
step 410500: train loss 2.4000516, val loss 2.3759887, mem 1.8 GiB @ 00 02:03:28.501, mean 00 00:00:00.018
step 411000: train loss 2.4076157, val loss 2.3848298, mem 1.8 GiB @ 00 02:03:37.268, mean 00 00:00:00.017
step 411500: train loss 2.4013216, val loss 2.3980997, mem 1.8 GiB @ 00 02:03:46.053, mean 00 00:00:00.017
step 412000: train loss 2.3799667, val loss 2.3816733, mem 1.8 GiB @ 00 02:03:54.876, mean 00 00:00:00.017
step 412500: train loss 2.3948534, val loss 2.392322, mem 1.8 GiB @ 00 02:04:03.818, mean 00 00:00:00.017
step 413000: train loss 2.4005518, val loss 2.3934658, mem 1.8 GiB @ 00 02:04:12.877, mean 00 00:00:00.018
step 413500: train loss 2.3846264, val loss 2.3932781, mem 1.8 GiB @ 00 02:04:22.014, mean 00 00:00:00.018
step 414000: train loss 2.4020753, val loss 2.3745437, mem 1.8 GiB @ 00 02:04:31.148, mean 00 00:00:00.018
step 414500: train loss 2.4026284, val loss 2.3903258, mem 1.8 GiB @ 00 02:04:40.307, mean 00 00:00:00.018
step 415000: train loss 2.4002054, val loss 2.3884127, mem 1.8 GiB @ 00 02:04:49.364, mean 00 00:00:00.018
step 415500: train loss 2.3899398, val loss 2.394304, mem 1.8 GiB @ 00 02:04:58.519, mean 00 00:00:00.018
step 416000: train loss 2.3742023, val loss 2.3850763, mem 1.8 GiB @ 00 02:05:07.647, mean 00 00:00:00.018
step 416500: train loss 2.391297, val loss 2.3954685, mem 1.8 GiB @ 00 02:05:16.807, mean 00 00:00:00.018
step 417000: train loss 2.392579, val loss 2.3893664, mem 1.8 GiB @ 00 02:05:25.963, mean 00 00:00:00.018
step 417500: train loss 2.3946335, val loss 2.3797314, mem 1.8 GiB @ 00 02:05:35.122, mean 00 00:00:00.018
step 418000: train loss 2.3697493, val loss 2.394789, mem 1.8 GiB @ 00 02:05:44.264, mean 00 00:00:00.018
step 418500: train loss 2.386455, val loss 2.3786616, mem 1.8 GiB @ 00 02:05:53.438, mean 00 00:00:00.018
step 419000: train loss 2.3827674, val loss 2.3750157, mem 1.8 GiB @ 00 02:06:02.409, mean 00 00:00:00.017
step 419500: train loss 2.4108465, val loss 2.3939955, mem 1.8 GiB @ 00 02:06:11.556, mean 00 00:00:00.018
step 420000: train loss 2.3860466, val loss 2.3903787, mem 1.8 GiB @ 00 02:06:20.776, mean 00 00:00:00.018
step 420500: train loss 2.3826714, val loss 2.3838928, mem 1.8 GiB @ 00 02:06:29.948, mean 00 00:00:00.018
step 421000: train loss 2.385026, val loss 2.3679717, mem 1.8 GiB @ 00 02:06:39.125, mean 00 00:00:00.018
step 421500: train loss 2.3826928, val loss 2.3955796, mem 1.8 GiB @ 00 02:06:48.346, mean 00 00:00:00.018
step 422000: train loss 2.3802092, val loss 2.384607, mem 1.8 GiB @ 00 02:06:57.462, mean 00 00:00:00.018
step 422500: train loss 2.3966827, val loss 2.3903196, mem 1.8 GiB @ 00 02:07:06.531, mean 00 00:00:00.018
step 423000: train loss 2.386037, val loss 2.3872313, mem 1.8 GiB @ 00 02:07:15.723, mean 00 00:00:00.018
step 423500: train loss 2.3959246, val loss 2.3666515, mem 1.8 GiB @ 00 02:07:24.913, mean 00 00:00:00.018
step 424000: train loss 2.3893595, val loss 2.3931353, mem 1.8 GiB @ 00 02:07:34.108, mean 00 00:00:00.018
step 424500: train loss 2.3890767, val loss 2.3920531, mem 1.8 GiB @ 00 02:07:43.283, mean 00 00:00:00.018
step 425000: train loss 2.3891098, val loss 2.3877804, mem 1.8 GiB @ 00 02:07:52.340, mean 00 00:00:00.018
step 425500: train loss 2.376239, val loss 2.3796756, mem 1.8 GiB @ 00 02:08:01.529, mean 00 00:00:00.018
step 426000: train loss 2.3887706, val loss 2.3899965, mem 1.8 GiB @ 00 02:08:10.677, mean 00 00:00:00.018
step 426500: train loss 2.3811224, val loss 2.400354, mem 1.8 GiB @ 00 02:08:19.740, mean 00 00:00:00.018
step 427000: train loss 2.377065, val loss 2.3996406, mem 1.8 GiB @ 00 02:08:29.007, mean 00 00:00:00.018
step 427500: train loss 2.3914866, val loss 2.385398, mem 1.8 GiB @ 00 02:08:38.193, mean 00 00:00:00.018
step 428000: train loss 2.376551, val loss 2.398249, mem 1.8 GiB @ 00 02:08:47.391, mean 00 00:00:00.018
step 428500: train loss 2.3793836, val loss 2.3826532, mem 1.8 GiB @ 00 02:08:56.577, mean 00 00:00:00.018
step 429000: train loss 2.3916812, val loss 2.3719275, mem 1.8 GiB @ 00 02:09:05.762, mean 00 00:00:00.018
step 429500: train loss 2.3777795, val loss 2.3618064, mem 1.8 GiB @ 00 02:09:14.809, mean 00 00:00:00.018
step 430000: train loss 2.3759856, val loss 2.3838453, mem 1.8 GiB @ 00 02:09:24.110, mean 00 00:00:00.018
step 430500: train loss 2.3741999, val loss 2.3733249, mem 1.8 GiB @ 00 02:09:33.206, mean 00 00:00:00.018
step 431000: train loss 2.3713117, val loss 2.3839552, mem 1.8 GiB @ 00 02:09:42.519, mean 00 00:00:00.018
step 431500: train loss 2.4036944, val loss 2.3732474, mem 1.8 GiB @ 00 02:09:51.775, mean 00 00:00:00.018
step 432000: train loss 2.358284, val loss 2.3643808, mem 1.8 GiB @ 00 02:10:01.092, mean 00 00:00:00.018
step 432500: train loss 2.3862655, val loss 2.3744354, mem 1.8 GiB @ 00 02:10:10.354, mean 00 00:00:00.018
step 433000: train loss 2.3753412, val loss 2.3803775, mem 1.8 GiB @ 00 02:10:19.424, mean 00 00:00:00.018
step 433500: train loss 2.3923316, val loss 2.3785357, mem 1.8 GiB @ 00 02:10:28.761, mean 00 00:00:00.018
step 434000: train loss 2.3752007, val loss 2.3761463, mem 1.8 GiB @ 00 02:10:38.241, mean 00 00:00:00.018
step 434500: train loss 2.3819542, val loss 2.370355, mem 1.8 GiB @ 00 02:10:47.673, mean 00 00:00:00.018
step 435000: train loss 2.3801258, val loss 2.389269, mem 1.8 GiB @ 00 02:10:56.940, mean 00 00:00:00.018
step 435500: train loss 2.392801, val loss 2.3861885, mem 1.8 GiB @ 00 02:11:06.078, mean 00 00:00:00.018
step 436000: train loss 2.3774984, val loss 2.3798604, mem 1.8 GiB @ 00 02:11:15.153, mean 00 00:00:00.018
step 436500: train loss 2.3802838, val loss 2.3839114, mem 1.8 GiB @ 00 02:11:23.948, mean 00 00:00:00.017
step 437000: train loss 2.3683894, val loss 2.3849006, mem 1.8 GiB @ 00 02:11:32.752, mean 00 00:00:00.017
step 437500: train loss 2.3789823, val loss 2.3888671, mem 1.8 GiB @ 00 02:11:41.826, mean 00 00:00:00.018
step 438000: train loss 2.3863165, val loss 2.3594747, mem 1.8 GiB @ 00 02:11:50.922, mean 00 00:00:00.018
step 438500: train loss 2.3683233, val loss 2.386778, mem 1.8 GiB @ 00 02:12:00.316, mean 00 00:00:00.018
step 439000: train loss 2.376559, val loss 2.370619, mem 1.8 GiB @ 00 02:12:09.536, mean 00 00:00:00.018
step 439500: train loss 2.4594512, val loss 2.3664064, mem 1.8 GiB @ 00 02:12:18.700, mean 00 00:00:00.018
step 440000: train loss 2.3859165, val loss 2.380868, mem 1.8 GiB @ 00 02:12:27.767, mean 00 00:00:00.018
step 440500: train loss 2.3757248, val loss 2.3757632, mem 1.8 GiB @ 00 02:12:36.994, mean 00 00:00:00.018
step 441000: train loss 2.383643, val loss 2.3719497, mem 1.8 GiB @ 00 02:12:46.483, mean 00 00:00:00.018
step 441500: train loss 2.3750272, val loss 2.3757021, mem 1.8 GiB @ 00 02:12:55.830, mean 00 00:00:00.018
step 442000: train loss 2.38463, val loss 2.3615637, mem 1.8 GiB @ 00 02:13:04.915, mean 00 00:00:00.018
step 442500: train loss 2.361793, val loss 2.3855786, mem 1.8 GiB @ 00 02:13:14.519, mean 00 00:00:00.019
step 443000: train loss 2.3753753, val loss 2.3702614, mem 1.8 GiB @ 00 02:13:23.788, mean 00 00:00:00.018
step 443500: train loss 2.3751671, val loss 2.3741052, mem 1.8 GiB @ 00 02:13:32.953, mean 00 00:00:00.018
step 444000: train loss 2.380937, val loss 2.383485, mem 1.8 GiB @ 00 02:13:42.138, mean 00 00:00:00.018
step 444500: train loss 2.3716888, val loss 2.3734887, mem 1.8 GiB @ 00 02:13:51.455, mean 00 00:00:00.018
step 445000: train loss 2.362425, val loss 2.3777597, mem 1.8 GiB @ 00 02:14:00.679, mean 00 00:00:00.018
step 445500: train loss 2.3807151, val loss 2.3743951, mem 1.8 GiB @ 00 02:14:09.879, mean 00 00:00:00.018
step 446000: train loss 2.3723612, val loss 2.3769546, mem 1.8 GiB @ 00 02:14:18.960, mean 00 00:00:00.018
step 446500: train loss 2.3795846, val loss 2.3701952, mem 1.8 GiB @ 00 02:14:28.128, mean 00 00:00:00.018
step 447000: train loss 2.3840108, val loss 2.362172, mem 1.8 GiB @ 00 02:14:37.313, mean 00 00:00:00.018
step 447500: train loss 2.3747187, val loss 2.3592546, mem 1.8 GiB @ 00 02:14:46.477, mean 00 00:00:00.018
step 448000: train loss 2.3841767, val loss 2.3779569, mem 1.8 GiB @ 00 02:14:55.648, mean 00 00:00:00.018
step 448500: train loss 2.3643084, val loss 2.3558083, mem 1.8 GiB @ 00 02:15:04.801, mean 00 00:00:00.018
step 449000: train loss 2.4554086, val loss 2.3611245, mem 1.8 GiB @ 00 02:15:13.907, mean 00 00:00:00.018
step 449500: train loss 2.364983, val loss 2.378378, mem 1.8 GiB @ 00 02:15:23.297, mean 00 00:00:00.018
step 450000: train loss 2.37746, val loss 2.363567, mem 1.8 GiB @ 00 02:15:32.523, mean 00 00:00:00.018
step 450500: train loss 2.3833902, val loss 2.3702283, mem 1.8 GiB @ 00 02:15:41.745, mean 00 00:00:00.018
step 451000: train loss 2.3824365, val loss 2.3654528, mem 1.8 GiB @ 00 02:15:51.087, mean 00 00:00:00.018
step 451500: train loss 2.3724146, val loss 2.3965228, mem 1.8 GiB @ 00 02:16:00.268, mean 00 00:00:00.018
step 452000: train loss 2.4250047, val loss 2.3681169, mem 1.8 GiB @ 00 02:16:09.455, mean 00 00:00:00.018
step 452500: train loss 2.3670893, val loss 2.3736572, mem 1.8 GiB @ 00 02:16:18.637, mean 00 00:00:00.018
step 453000: train loss 2.3795638, val loss 2.3649864, mem 1.8 GiB @ 00 02:16:27.817, mean 00 00:00:00.018
step 453500: train loss 2.3723862, val loss 2.3749583, mem 1.8 GiB @ 00 02:16:36.995, mean 00 00:00:00.018
step 454000: train loss 2.3825226, val loss 2.3513906, mem 1.8 GiB @ 00 02:16:46.181, mean 00 00:00:00.018
step 454500: train loss 2.387439, val loss 2.3738463, mem 1.8 GiB @ 00 02:16:55.265, mean 00 00:00:00.018
step 455000: train loss 2.3685465, val loss 2.373995, mem 1.8 GiB @ 00 02:17:04.426, mean 00 00:00:00.018
step 455500: train loss 2.3533616, val loss 2.3728466, mem 1.8 GiB @ 00 02:17:13.601, mean 00 00:00:00.018
step 456000: train loss 2.3567426, val loss 2.3734798, mem 1.8 GiB @ 00 02:17:22.762, mean 00 00:00:00.018
step 456500: train loss 2.3645902, val loss 2.3434207, mem 1.8 GiB @ 00 02:17:32.014, mean 00 00:00:00.018
step 457000: train loss 2.3543143, val loss 2.3657029, mem 1.8 GiB @ 00 02:17:41.183, mean 00 00:00:00.018
step 457500: train loss 2.371081, val loss 2.361699, mem 1.8 GiB @ 00 02:17:50.674, mean 00 00:00:00.018
step 458000: train loss 2.3650606, val loss 2.378439, mem 1.8 GiB @ 00 02:17:59.861, mean 00 00:00:00.018
step 458500: train loss 2.3601646, val loss 2.3866324, mem 1.8 GiB @ 00 02:18:08.946, mean 00 00:00:00.018
step 459000: train loss 2.3596425, val loss 2.373352, mem 1.8 GiB @ 00 02:18:18.125, mean 00 00:00:00.018
step 459500: train loss 2.344636, val loss 2.3654506, mem 1.8 GiB @ 00 02:18:27.317, mean 00 00:00:00.018
step 460000: train loss 2.3857627, val loss 2.3755834, mem 1.8 GiB @ 00 02:18:36.550, mean 00 00:00:00.018
step 460500: train loss 2.4014077, val loss 2.360458, mem 1.8 GiB @ 00 02:18:46.062, mean 00 00:00:00.019
step 461000: train loss 2.3776186, val loss 2.370172, mem 1.8 GiB @ 00 02:18:55.214, mean 00 00:00:00.018
step 461500: train loss 2.36866, val loss 2.3600593, mem 1.8 GiB @ 00 02:19:04.771, mean 00 00:00:00.019
step 462000: train loss 2.3676417, val loss 2.3634334, mem 1.8 GiB @ 00 02:19:14.058, mean 00 00:00:00.018
step 462500: train loss 2.3542812, val loss 2.367238, mem 1.8 GiB @ 00 02:19:23.231, mean 00 00:00:00.018
step 463000: train loss 2.362465, val loss 2.3652005, mem 1.8 GiB @ 00 02:19:32.326, mean 00 00:00:00.018
step 463500: train loss 2.3566709, val loss 2.3647776, mem 1.8 GiB @ 00 02:19:41.524, mean 00 00:00:00.018
step 464000: train loss 2.3791728, val loss 2.3577034, mem 1.8 GiB @ 00 02:19:50.719, mean 00 00:00:00.018
step 464500: train loss 2.3632848, val loss 2.3586533, mem 1.8 GiB @ 00 02:19:59.889, mean 00 00:00:00.018
step 465000: train loss 2.3614843, val loss 2.353092, mem 1.8 GiB @ 00 02:20:09.266, mean 00 00:00:00.018
step 465500: train loss 2.3554018, val loss 2.357572, mem 1.8 GiB @ 00 02:20:18.830, mean 00 00:00:00.019
step 466000: train loss 2.3650265, val loss 2.372995, mem 1.8 GiB @ 00 02:20:28.166, mean 00 00:00:00.018
step 466500: train loss 2.3804584, val loss 2.3633828, mem 1.8 GiB @ 00 02:20:37.454, mean 00 00:00:00.018
step 467000: train loss 2.370531, val loss 2.3539412, mem 1.8 GiB @ 00 02:20:46.752, mean 00 00:00:00.018
step 467500: train loss 2.3639522, val loss 2.373466, mem 1.8 GiB @ 00 02:20:56.118, mean 00 00:00:00.018
step 468000: train loss 2.3788934, val loss 2.3680565, mem 1.8 GiB @ 00 02:21:05.355, mean 00 00:00:00.018
step 468500: train loss 2.372972, val loss 2.373527, mem 1.8 GiB @ 00 02:21:14.521, mean 00 00:00:00.018
step 469000: train loss 2.370032, val loss 2.354494, mem 1.8 GiB @ 00 02:21:23.999, mean 00 00:00:00.018
step 469500: train loss 2.3656874, val loss 2.3634064, mem 1.8 GiB @ 00 02:21:33.376, mean 00 00:00:00.018
step 470000: train loss 2.3642733, val loss 2.36408, mem 1.8 GiB @ 00 02:21:42.669, mean 00 00:00:00.018
step 470500: train loss 2.3651583, val loss 2.3587265, mem 1.8 GiB @ 00 02:21:52.004, mean 00 00:00:00.018
step 471000: train loss 2.3650782, val loss 2.3550112, mem 1.8 GiB @ 00 02:22:01.390, mean 00 00:00:00.018
step 471500: train loss 2.3562682, val loss 2.3592665, mem 1.8 GiB @ 00 02:22:10.503, mean 00 00:00:00.018
step 472000: train loss 2.3608005, val loss 2.3585377, mem 1.8 GiB @ 00 02:22:19.839, mean 00 00:00:00.018
step 472500: train loss 2.3637173, val loss 2.351146, mem 1.8 GiB @ 00 02:22:29.425, mean 00 00:00:00.019
step 473000: train loss 2.3559585, val loss 2.3577933, mem 1.8 GiB @ 00 02:22:39.049, mean 00 00:00:00.019
step 473500: train loss 2.3632016, val loss 2.3730283, mem 1.8 GiB @ 00 02:22:48.142, mean 00 00:00:00.018
step 474000: train loss 2.3683305, val loss 2.35291, mem 1.8 GiB @ 00 02:22:57.384, mean 00 00:00:00.018
step 474500: train loss 2.3559275, val loss 2.363527, mem 1.8 GiB @ 00 02:23:06.666, mean 00 00:00:00.018
step 475000: train loss 2.3739898, val loss 2.3709545, mem 1.8 GiB @ 00 02:23:15.924, mean 00 00:00:00.018
step 475500: train loss 2.3554213, val loss 2.3687272, mem 1.8 GiB @ 00 02:23:25.127, mean 00 00:00:00.018
step 476000: train loss 2.3653874, val loss 2.3623044, mem 1.8 GiB @ 00 02:23:34.305, mean 00 00:00:00.018
step 476500: train loss 2.3549144, val loss 2.3533032, mem 1.8 GiB @ 00 02:23:43.485, mean 00 00:00:00.018
step 477000: train loss 2.3636403, val loss 2.3538759, mem 1.8 GiB @ 00 02:23:52.689, mean 00 00:00:00.018
step 477500: train loss 2.3706546, val loss 2.3558376, mem 1.8 GiB @ 00 02:24:01.859, mean 00 00:00:00.018
step 478000: train loss 2.3600304, val loss 2.3801532, mem 1.8 GiB @ 00 02:24:11.010, mean 00 00:00:00.018
step 478500: train loss 2.3637145, val loss 2.3495271, mem 1.8 GiB @ 00 02:24:20.216, mean 00 00:00:00.018
step 479000: train loss 2.3592846, val loss 2.3518353, mem 1.8 GiB @ 00 02:24:29.313, mean 00 00:00:00.018
step 479500: train loss 2.3607047, val loss 2.3497307, mem 1.8 GiB @ 00 02:24:38.177, mean 00 00:00:00.017
step 480000: train loss 2.371363, val loss 2.3537543, mem 1.8 GiB @ 00 02:24:46.968, mean 00 00:00:00.017
step 480500: train loss 2.381707, val loss 2.362558, mem 1.8 GiB @ 00 02:24:55.907, mean 00 00:00:00.017
step 481000: train loss 2.358783, val loss 2.3522975, mem 1.8 GiB @ 00 02:25:05.065, mean 00 00:00:00.018
step 481500: train loss 2.3591268, val loss 2.3684855, mem 1.8 GiB @ 00 02:25:14.282, mean 00 00:00:00.018
step 482000: train loss 2.3410273, val loss 2.351635, mem 1.8 GiB @ 00 02:25:23.029, mean 00 00:00:00.017
step 482500: train loss 2.3520207, val loss 2.3638039, mem 1.8 GiB @ 00 02:25:32.120, mean 00 00:00:00.018
step 483000: train loss 2.3728619, val loss 2.3505511, mem 1.8 GiB @ 00 02:25:41.276, mean 00 00:00:00.018
step 483500: train loss 2.343745, val loss 2.3474526, mem 1.8 GiB @ 00 02:25:50.537, mean 00 00:00:00.018
step 484000: train loss 2.3527293, val loss 2.3622677, mem 1.8 GiB @ 00 02:25:59.501, mean 00 00:00:00.017
step 484500: train loss 2.3547528, val loss 2.3506584, mem 1.8 GiB @ 00 02:26:08.895, mean 00 00:00:00.018
step 485000: train loss 2.3636892, val loss 2.3520553, mem 1.8 GiB @ 00 02:26:18.134, mean 00 00:00:00.018
step 485500: train loss 2.3798091, val loss 2.3510208, mem 1.8 GiB @ 00 02:26:27.285, mean 00 00:00:00.018
step 486000: train loss 2.354472, val loss 2.3511236, mem 1.8 GiB @ 00 02:26:36.410, mean 00 00:00:00.018
step 486500: train loss 2.3634796, val loss 2.3553338, mem 1.8 GiB @ 00 02:26:45.580, mean 00 00:00:00.018
step 487000: train loss 2.3576303, val loss 2.3537252, mem 1.8 GiB @ 00 02:26:54.756, mean 00 00:00:00.018
step 487500: train loss 2.347268, val loss 2.3446317, mem 1.8 GiB @ 00 02:27:03.923, mean 00 00:00:00.018
step 488000: train loss 2.3641076, val loss 2.3620434, mem 1.8 GiB @ 00 02:27:13.126, mean 00 00:00:00.018
step 488500: train loss 2.3716695, val loss 2.3633919, mem 1.8 GiB @ 00 02:27:22.231, mean 00 00:00:00.018
step 489000: train loss 2.3392625, val loss 2.3528214, mem 1.8 GiB @ 00 02:27:31.424, mean 00 00:00:00.018
step 489500: train loss 2.354816, val loss 2.3431456, mem 1.8 GiB @ 00 02:27:40.606, mean 00 00:00:00.018
step 490000: train loss 2.348204, val loss 2.3653224, mem 1.8 GiB @ 00 02:27:49.813, mean 00 00:00:00.018
step 490500: train loss 2.3558083, val loss 2.3484302, mem 1.8 GiB @ 00 02:27:58.993, mean 00 00:00:00.018
step 491000: train loss 2.3457336, val loss 2.3513455, mem 1.8 GiB @ 00 02:28:07.954, mean 00 00:00:00.017
step 491500: train loss 2.3521547, val loss 2.3593612, mem 1.8 GiB @ 00 02:28:17.118, mean 00 00:00:00.018
step 492000: train loss 2.3431354, val loss 2.3375738, mem 1.8 GiB @ 00 02:28:26.297, mean 00 00:00:00.018
step 492500: train loss 2.3468869, val loss 2.3502817, mem 1.8 GiB @ 00 02:28:35.489, mean 00 00:00:00.018
step 493000: train loss 2.3578033, val loss 2.3548772, mem 1.8 GiB @ 00 02:28:44.675, mean 00 00:00:00.018
step 493500: train loss 2.345487, val loss 2.336624, mem 1.8 GiB @ 00 02:28:53.738, mean 00 00:00:00.018
step 494000: train loss 2.3508835, val loss 2.3561502, mem 1.8 GiB @ 00 02:29:02.899, mean 00 00:00:00.018
step 494500: train loss 2.358552, val loss 2.3490806, mem 1.8 GiB @ 00 02:29:12.331, mean 00 00:00:00.018
step 495000: train loss 2.3648794, val loss 2.3426409, mem 1.8 GiB @ 00 02:29:21.508, mean 00 00:00:00.018
step 495500: train loss 2.3516612, val loss 2.3386152, mem 1.8 GiB @ 00 02:29:30.563, mean 00 00:00:00.018
step 496000: train loss 2.35885, val loss 2.3550785, mem 1.8 GiB @ 00 02:29:39.719, mean 00 00:00:00.018
step 496500: train loss 2.3461769, val loss 2.3491158, mem 1.8 GiB @ 00 02:29:48.886, mean 00 00:00:00.018
step 497000: train loss 2.3594, val loss 2.3526895, mem 1.8 GiB @ 00 02:29:57.949, mean 00 00:00:00.018
step 497500: train loss 2.3481705, val loss 2.3384616, mem 1.8 GiB @ 00 02:30:07.082, mean 00 00:00:00.018
step 498000: train loss 2.350544, val loss 2.3400764, mem 1.8 GiB @ 00 02:30:16.225, mean 00 00:00:00.018
step 498500: train loss 2.3550496, val loss 2.346412, mem 1.8 GiB @ 00 02:30:25.379, mean 00 00:00:00.018
step 499000: train loss 2.3503609, val loss 2.3523016, mem 1.8 GiB @ 00 02:30:34.519, mean 00 00:00:00.018
step 499500: train loss 2.3481781, val loss 2.3625906, mem 1.8 GiB @ 00 02:30:43.659, mean 00 00:00:00.018
step 500000: train loss 2.3510149, val loss 2.350693, mem 1.8 GiB @ 00 02:30:52.624, mean 00 00:00:00.017
step 500500: train loss 2.369787, val loss 2.3490334, mem 1.8 GiB @ 00 02:31:01.549, mean 00 00:00:00.017
step 501000: train loss 2.3673894, val loss 2.3506083, mem 1.8 GiB @ 00 02:31:10.546, mean 00 00:00:00.017
step 501500: train loss 2.3537996, val loss 2.33525, mem 1.8 GiB @ 00 02:31:19.349, mean 00 00:00:00.017
step 502000: train loss 2.3483624, val loss 2.3440685, mem 1.8 GiB @ 00 02:31:28.439, mean 00 00:00:00.018
step 502500: train loss 2.345315, val loss 2.332948, mem 1.8 GiB @ 00 02:31:37.593, mean 00 00:00:00.018
step 503000: train loss 2.3526871, val loss 2.3296204, mem 1.8 GiB @ 00 02:31:46.638, mean 00 00:00:00.018
step 503500: train loss 2.3378382, val loss 2.3479302, mem 1.8 GiB @ 00 02:31:55.811, mean 00 00:00:00.018
step 504000: train loss 2.3668284, val loss 2.3434362, mem 1.8 GiB @ 00 02:32:04.955, mean 00 00:00:00.018
step 504500: train loss 2.3469913, val loss 2.3384125, mem 1.8 GiB @ 00 02:32:14.132, mean 00 00:00:00.018
step 505000: train loss 2.3487778, val loss 2.3341064, mem 1.8 GiB @ 00 02:32:23.192, mean 00 00:00:00.018
step 505500: train loss 2.3636475, val loss 2.3397374, mem 1.8 GiB @ 00 02:32:32.377, mean 00 00:00:00.018
step 506000: train loss 2.344055, val loss 2.3380704, mem 1.8 GiB @ 00 02:32:41.543, mean 00 00:00:00.018
step 506500: train loss 2.3416677, val loss 2.3394883, mem 1.8 GiB @ 00 02:32:50.695, mean 00 00:00:00.018
step 507000: train loss 2.3448026, val loss 2.3287714, mem 1.8 GiB @ 00 02:32:59.839, mean 00 00:00:00.018
step 507500: train loss 2.348776, val loss 2.34898, mem 1.8 GiB @ 00 02:33:08.893, mean 00 00:00:00.018
step 508000: train loss 2.3507745, val loss 2.3358774, mem 1.8 GiB @ 00 02:33:18.055, mean 00 00:00:00.018
step 508500: train loss 2.3597946, val loss 2.335337, mem 1.8 GiB @ 00 02:33:27.197, mean 00 00:00:00.018
step 509000: train loss 2.3437927, val loss 2.3260546, mem 1.8 GiB @ 00 02:33:36.339, mean 00 00:00:00.018
step 509500: train loss 2.3483064, val loss 2.3413935, mem 1.8 GiB @ 00 02:33:45.469, mean 00 00:00:00.018
step 510000: train loss 2.3486583, val loss 2.3480072, mem 1.8 GiB @ 00 02:33:54.520, mean 00 00:00:00.018
step 510500: train loss 2.3534381, val loss 2.3291647, mem 1.8 GiB @ 00 02:34:03.689, mean 00 00:00:00.018
step 511000: train loss 2.3540115, val loss 2.3523347, mem 1.8 GiB @ 00 02:34:12.738, mean 00 00:00:00.018
step 511500: train loss 2.3418753, val loss 2.3499079, mem 1.8 GiB @ 00 02:34:21.866, mean 00 00:00:00.018
step 512000: train loss 2.3333204, val loss 2.3405404, mem 1.8 GiB @ 00 02:34:31.024, mean 00 00:00:00.018
step 512500: train loss 2.3456047, val loss 2.3485515, mem 1.8 GiB @ 00 02:34:40.164, mean 00 00:00:00.018
step 513000: train loss 2.3358233, val loss 2.3473854, mem 1.8 GiB @ 00 02:34:49.194, mean 00 00:00:00.018
step 513500: train loss 2.341502, val loss 2.3586295, mem 1.8 GiB @ 00 02:34:58.329, mean 00 00:00:00.018
step 514000: train loss 2.3517418, val loss 2.3602312, mem 1.8 GiB @ 00 02:35:07.460, mean 00 00:00:00.018
step 514500: train loss 2.3526785, val loss 2.3444133, mem 1.8 GiB @ 00 02:35:16.627, mean 00 00:00:00.018
step 515000: train loss 2.3651946, val loss 2.3604565, mem 1.8 GiB @ 00 02:35:25.686, mean 00 00:00:00.018
step 515500: train loss 2.3498354, val loss 2.3605607, mem 1.8 GiB @ 00 02:35:34.820, mean 00 00:00:00.018
step 516000: train loss 2.3498614, val loss 2.3424528, mem 1.8 GiB @ 00 02:35:43.963, mean 00 00:00:00.018
step 516500: train loss 2.346652, val loss 2.3394887, mem 1.8 GiB @ 00 02:35:53.095, mean 00 00:00:00.018
step 517000: train loss 2.3301485, val loss 2.350963, mem 1.8 GiB @ 00 02:36:02.134, mean 00 00:00:00.018
step 517500: train loss 2.3411343, val loss 2.3532724, mem 1.8 GiB @ 00 02:36:11.266, mean 00 00:00:00.018
step 518000: train loss 2.354331, val loss 2.3324544, mem 1.8 GiB @ 00 02:36:20.409, mean 00 00:00:00.018
step 518500: train loss 2.3544064, val loss 2.3530188, mem 1.8 GiB @ 00 02:36:29.541, mean 00 00:00:00.018
step 519000: train loss 2.3419483, val loss 2.3420913, mem 1.8 GiB @ 00 02:36:38.704, mean 00 00:00:00.018
step 519500: train loss 2.3332355, val loss 2.3519952, mem 1.8 GiB @ 00 02:36:47.838, mean 00 00:00:00.018
step 520000: train loss 2.3516273, val loss 2.3526306, mem 1.8 GiB @ 00 02:36:56.973, mean 00 00:00:00.018
step 520500: train loss 2.3409646, val loss 2.3361604, mem 1.8 GiB @ 00 02:37:06.104, mean 00 00:00:00.018
step 521000: train loss 2.3405492, val loss 2.3503668, mem 1.8 GiB @ 00 02:37:15.141, mean 00 00:00:00.018
step 521500: train loss 2.347478, val loss 2.3375955, mem 1.8 GiB @ 00 02:37:24.261, mean 00 00:00:00.018
step 522000: train loss 2.3402627, val loss 2.3540635, mem 1.8 GiB @ 00 02:37:33.394, mean 00 00:00:00.018
step 522500: train loss 2.4108365, val loss 2.3483794, mem 1.8 GiB @ 00 02:37:42.527, mean 00 00:00:00.018
step 523000: train loss 2.3310792, val loss 2.3211222, mem 1.8 GiB @ 00 02:37:51.676, mean 00 00:00:00.018
step 523500: train loss 2.3390007, val loss 2.340547, mem 1.8 GiB @ 00 02:38:00.823, mean 00 00:00:00.018
step 524000: train loss 2.3373404, val loss 2.3405776, mem 1.8 GiB @ 00 02:38:09.873, mean 00 00:00:00.018
step 524500: train loss 2.3431487, val loss 2.3428378, mem 1.8 GiB @ 00 02:38:19.014, mean 00 00:00:00.018
step 525000: train loss 2.3310544, val loss 2.3447895, mem 1.8 GiB @ 00 02:38:28.157, mean 00 00:00:00.018
step 525500: train loss 2.3315222, val loss 2.342296, mem 1.8 GiB @ 00 02:38:37.210, mean 00 00:00:00.018
step 526000: train loss 2.3445225, val loss 2.3333216, mem 1.8 GiB @ 00 02:38:46.354, mean 00 00:00:00.018
step 526500: train loss 2.3395946, val loss 2.3530505, mem 1.8 GiB @ 00 02:38:55.525, mean 00 00:00:00.018
step 527000: train loss 2.3208537, val loss 2.3526297, mem 1.8 GiB @ 00 02:39:04.659, mean 00 00:00:00.018
step 527500: train loss 2.4080188, val loss 2.3472197, mem 1.8 GiB @ 00 02:39:13.803, mean 00 00:00:00.018
step 528000: train loss 2.342384, val loss 2.3425224, mem 1.8 GiB @ 00 02:39:22.948, mean 00 00:00:00.018
step 528500: train loss 2.3530867, val loss 2.3466098, mem 1.8 GiB @ 00 02:39:32.003, mean 00 00:00:00.018
step 529000: train loss 2.3381453, val loss 2.3327246, mem 1.8 GiB @ 00 02:39:41.152, mean 00 00:00:00.018
step 529500: train loss 2.332357, val loss 2.3417711, mem 1.8 GiB @ 00 02:39:50.314, mean 00 00:00:00.018
step 530000: train loss 2.3478088, val loss 2.3466043, mem 1.8 GiB @ 00 02:39:59.339, mean 00 00:00:00.018
step 530500: train loss 2.3422341, val loss 2.3540564, mem 1.8 GiB @ 00 02:40:08.474, mean 00 00:00:00.018
step 531000: train loss 2.3352487, val loss 2.3395123, mem 1.8 GiB @ 00 02:40:17.648, mean 00 00:00:00.018
step 531500: train loss 2.3374016, val loss 2.331628, mem 1.8 GiB @ 00 02:40:26.810, mean 00 00:00:00.018
step 532000: train loss 2.3464797, val loss 2.3289495, mem 1.8 GiB @ 00 02:40:35.851, mean 00 00:00:00.018
step 532500: train loss 2.321446, val loss 2.3340356, mem 1.8 GiB @ 00 02:40:45.008, mean 00 00:00:00.018
step 533000: train loss 2.358253, val loss 2.338134, mem 1.8 GiB @ 00 02:40:54.145, mean 00 00:00:00.018
step 533500: train loss 2.3402748, val loss 2.3361847, mem 1.8 GiB @ 00 02:41:03.298, mean 00 00:00:00.018
step 534000: train loss 2.3331685, val loss 2.327053, mem 1.8 GiB @ 00 02:41:12.341, mean 00 00:00:00.018
step 534500: train loss 2.3339922, val loss 2.3486657, mem 1.8 GiB @ 00 02:41:21.477, mean 00 00:00:00.018
step 535000: train loss 2.3343015, val loss 2.3388588, mem 1.8 GiB @ 00 02:41:30.645, mean 00 00:00:00.018
step 535500: train loss 2.323881, val loss 2.3252475, mem 1.8 GiB @ 00 02:41:39.769, mean 00 00:00:00.018
step 536000: train loss 2.3477106, val loss 2.3292398, mem 1.8 GiB @ 00 02:41:48.910, mean 00 00:00:00.018
step 536500: train loss 2.3457186, val loss 2.3385577, mem 1.8 GiB @ 00 02:41:57.948, mean 00 00:00:00.018
step 537000: train loss 2.346818, val loss 2.3239636, mem 1.8 GiB @ 00 02:42:07.097, mean 00 00:00:00.018
step 537500: train loss 2.3270364, val loss 2.348026, mem 1.8 GiB @ 00 02:42:16.233, mean 00 00:00:00.018
step 538000: train loss 2.335682, val loss 2.339398, mem 1.8 GiB @ 00 02:42:25.351, mean 00 00:00:00.018
step 538500: train loss 2.3603888, val loss 2.3330445, mem 1.8 GiB @ 00 02:42:34.509, mean 00 00:00:00.018
step 539000: train loss 2.347923, val loss 2.3438914, mem 1.8 GiB @ 00 02:42:43.643, mean 00 00:00:00.018
step 539500: train loss 2.3365238, val loss 2.3227603, mem 1.8 GiB @ 00 02:42:52.760, mean 00 00:00:00.018
step 540000: train loss 2.343493, val loss 2.3421218, mem 1.8 GiB @ 00 02:43:01.907, mean 00 00:00:00.018
step 540500: train loss 2.3468316, val loss 2.3456323, mem 1.8 GiB @ 00 02:43:11.063, mean 00 00:00:00.018
step 541000: train loss 2.347065, val loss 2.3302932, mem 1.8 GiB @ 00 02:43:20.204, mean 00 00:00:00.018
step 541500: train loss 2.3474638, val loss 2.333723, mem 1.8 GiB @ 00 02:43:29.349, mean 00 00:00:00.018
step 542000: train loss 2.324858, val loss 2.3266835, mem 1.8 GiB @ 00 02:43:38.488, mean 00 00:00:00.018
step 542500: train loss 2.343362, val loss 2.321592, mem 1.8 GiB @ 00 02:43:47.546, mean 00 00:00:00.018
step 543000: train loss 2.326336, val loss 2.3274972, mem 1.8 GiB @ 00 02:43:56.707, mean 00 00:00:00.018
step 543500: train loss 2.3368099, val loss 2.341783, mem 1.8 GiB @ 00 02:44:05.843, mean 00 00:00:00.018
step 544000: train loss 2.3247554, val loss 2.3463454, mem 1.8 GiB @ 00 02:44:14.900, mean 00 00:00:00.018
step 544500: train loss 2.3214695, val loss 2.3249547, mem 1.8 GiB @ 00 02:44:24.042, mean 00 00:00:00.018
step 545000: train loss 2.328066, val loss 2.336189, mem 1.8 GiB @ 00 02:44:33.188, mean 00 00:00:00.018
step 545500: train loss 2.322199, val loss 2.3529277, mem 1.8 GiB @ 00 02:44:42.331, mean 00 00:00:00.018
step 546000: train loss 2.3416889, val loss 2.342873, mem 1.8 GiB @ 00 02:44:51.480, mean 00 00:00:00.018
step 546500: train loss 2.3258338, val loss 2.3349712, mem 1.8 GiB @ 00 02:45:00.640, mean 00 00:00:00.018
step 547000: train loss 2.3235977, val loss 2.3302248, mem 1.8 GiB @ 00 02:45:09.787, mean 00 00:00:00.018
step 547500: train loss 2.3208544, val loss 2.32694, mem 1.8 GiB @ 00 02:45:18.913, mean 00 00:00:00.018
step 548000: train loss 2.333699, val loss 2.3369596, mem 1.8 GiB @ 00 02:45:28.071, mean 00 00:00:00.018
step 548500: train loss 2.3206275, val loss 2.347431, mem 1.8 GiB @ 00 02:45:37.216, mean 00 00:00:00.018
step 549000: train loss 2.3423638, val loss 2.3370743, mem 1.8 GiB @ 00 02:45:46.358, mean 00 00:00:00.018
step 549500: train loss 2.3322568, val loss 2.3368742, mem 1.8 GiB @ 00 02:45:55.381, mean 00 00:00:00.018
step 550000: train loss 2.3300378, val loss 2.337343, mem 1.8 GiB @ 00 02:46:04.519, mean 00 00:00:00.018
step 550500: train loss 2.3336627, val loss 2.3303823, mem 1.8 GiB @ 00 02:46:13.652, mean 00 00:00:00.018
step 551000: train loss 2.3473783, val loss 2.3308103, mem 1.8 GiB @ 00 02:46:22.807, mean 00 00:00:00.018
step 551500: train loss 2.3347373, val loss 2.3314583, mem 1.8 GiB @ 00 02:46:31.938, mean 00 00:00:00.018
step 552000: train loss 2.340103, val loss 2.3277876, mem 1.8 GiB @ 00 02:46:41.148, mean 00 00:00:00.018
step 552500: train loss 2.324435, val loss 2.3330834, mem 1.8 GiB @ 00 02:46:50.392, mean 00 00:00:00.018
step 553000: train loss 2.3328147, val loss 2.333977, mem 1.8 GiB @ 00 02:46:59.180, mean 00 00:00:00.017
step 553500: train loss 2.3383696, val loss 2.3375275, mem 1.8 GiB @ 00 02:47:07.864, mean 00 00:00:00.017
step 554000: train loss 2.3384478, val loss 2.3143854, mem 1.8 GiB @ 00 02:47:16.625, mean 00 00:00:00.017
step 554500: train loss 2.3358839, val loss 2.326465, mem 1.8 GiB @ 00 02:47:25.417, mean 00 00:00:00.017
step 555000: train loss 2.3210788, val loss 2.3317544, mem 1.8 GiB @ 00 02:47:34.577, mean 00 00:00:00.018
step 555500: train loss 2.3346448, val loss 2.3358033, mem 1.8 GiB @ 00 02:47:43.382, mean 00 00:00:00.017
step 556000: train loss 2.3346643, val loss 2.3391814, mem 1.8 GiB @ 00 02:47:52.192, mean 00 00:00:00.017
step 556500: train loss 2.3357937, val loss 2.3371942, mem 1.8 GiB @ 00 02:48:00.888, mean 00 00:00:00.017
step 557000: train loss 2.3357422, val loss 2.3316374, mem 1.8 GiB @ 00 02:48:09.697, mean 00 00:00:00.017
step 557500: train loss 2.3190622, val loss 2.331285, mem 1.8 GiB @ 00 02:48:18.488, mean 00 00:00:00.017
step 558000: train loss 2.3207788, val loss 2.325431, mem 1.8 GiB @ 00 02:48:27.302, mean 00 00:00:00.017
step 558500: train loss 2.3363914, val loss 2.3338547, mem 1.8 GiB @ 00 02:48:35.992, mean 00 00:00:00.017
step 559000: train loss 2.3210857, val loss 2.3151124, mem 1.8 GiB @ 00 02:48:44.762, mean 00 00:00:00.017
step 559500: train loss 2.333575, val loss 2.3373618, mem 1.8 GiB @ 00 02:48:53.564, mean 00 00:00:00.017
step 560000: train loss 2.3326318, val loss 2.339546, mem 1.8 GiB @ 00 02:49:02.330, mean 00 00:00:00.017
step 560500: train loss 2.32983, val loss 2.3306496, mem 1.8 GiB @ 00 02:49:11.011, mean 00 00:00:00.017
step 561000: train loss 2.3212588, val loss 2.3426528, mem 1.8 GiB @ 00 02:49:19.824, mean 00 00:00:00.017
step 561500: train loss 2.339025, val loss 2.3246245, mem 1.8 GiB @ 00 02:49:28.612, mean 00 00:00:00.017
step 562000: train loss 2.3266425, val loss 2.3267515, mem 1.8 GiB @ 00 02:49:37.404, mean 00 00:00:00.017
step 562500: train loss 2.346658, val loss 2.3280072, mem 1.8 GiB @ 00 02:49:46.414, mean 00 00:00:00.018
step 563000: train loss 2.3272426, val loss 2.334537, mem 1.8 GiB @ 00 02:49:55.578, mean 00 00:00:00.018
step 563500: train loss 2.3276389, val loss 2.31589, mem 1.8 GiB @ 00 02:50:04.646, mean 00 00:00:00.018
step 564000: train loss 2.321059, val loss 2.3141496, mem 1.8 GiB @ 00 02:50:13.806, mean 00 00:00:00.018
step 564500: train loss 2.315784, val loss 2.3282883, mem 1.8 GiB @ 00 02:50:22.964, mean 00 00:00:00.018
step 565000: train loss 2.3299615, val loss 2.322458, mem 1.8 GiB @ 00 02:50:32.108, mean 00 00:00:00.018
step 565500: train loss 2.3085997, val loss 2.3193398, mem 1.8 GiB @ 00 02:50:41.266, mean 00 00:00:00.018
step 566000: train loss 2.3267152, val loss 2.3296494, mem 1.8 GiB @ 00 02:50:50.309, mean 00 00:00:00.018
step 566500: train loss 2.325362, val loss 2.3339767, mem 1.8 GiB @ 00 02:50:59.461, mean 00 00:00:00.018
step 567000: train loss 2.3268611, val loss 2.3248718, mem 1.8 GiB @ 00 02:51:08.621, mean 00 00:00:00.018
step 567500: train loss 2.3288002, val loss 2.331527, mem 1.8 GiB @ 00 02:51:17.681, mean 00 00:00:00.018
step 568000: train loss 2.3257892, val loss 2.324899, mem 1.8 GiB @ 00 02:51:26.831, mean 00 00:00:00.018
step 568500: train loss 2.3450959, val loss 2.3302362, mem 1.8 GiB @ 00 02:51:35.964, mean 00 00:00:00.018
step 569000: train loss 2.3178155, val loss 2.3005102, mem 1.8 GiB @ 00 02:51:45.121, mean 00 00:00:00.018
step 569500: train loss 2.3131895, val loss 2.324363, mem 1.8 GiB @ 00 02:51:54.265, mean 00 00:00:00.018
step 570000: train loss 2.3838615, val loss 2.3120196, mem 1.8 GiB @ 00 02:52:03.394, mean 00 00:00:00.018
step 570500: train loss 2.3266819, val loss 2.32223, mem 1.8 GiB @ 00 02:52:12.534, mean 00 00:00:00.018
step 571000: train loss 2.3264625, val loss 2.330164, mem 1.8 GiB @ 00 02:52:21.675, mean 00 00:00:00.018
step 571500: train loss 2.3085089, val loss 2.327514, mem 1.8 GiB @ 00 02:52:30.734, mean 00 00:00:00.018
step 572000: train loss 2.324154, val loss 2.3430388, mem 1.8 GiB @ 00 02:52:39.873, mean 00 00:00:00.018
step 572500: train loss 2.325331, val loss 2.3268912, mem 1.8 GiB @ 00 02:52:49.030, mean 00 00:00:00.018
step 573000: train loss 2.3239121, val loss 2.3210802, mem 1.8 GiB @ 00 02:52:58.186, mean 00 00:00:00.018
step 573500: train loss 2.3314044, val loss 2.3110595, mem 1.8 GiB @ 00 02:53:07.311, mean 00 00:00:00.018
step 574000: train loss 2.3236904, val loss 2.3232157, mem 1.8 GiB @ 00 02:53:16.435, mean 00 00:00:00.018
step 574500: train loss 2.4090478, val loss 2.334555, mem 1.8 GiB @ 00 02:53:25.579, mean 00 00:00:00.018
step 575000: train loss 2.3346038, val loss 2.3083637, mem 1.8 GiB @ 00 02:53:34.760, mean 00 00:00:00.018
step 575500: train loss 2.3124063, val loss 2.3172061, mem 1.8 GiB @ 00 02:53:43.901, mean 00 00:00:00.018
step 576000: train loss 2.3172288, val loss 2.3280401, mem 1.8 GiB @ 00 02:53:53.054, mean 00 00:00:00.018
step 576500: train loss 2.3280644, val loss 2.3213747, mem 1.8 GiB @ 00 02:54:02.121, mean 00 00:00:00.018
step 577000: train loss 2.3424764, val loss 2.319179, mem 1.8 GiB @ 00 02:54:11.262, mean 00 00:00:00.018
step 577500: train loss 2.3179393, val loss 2.3278594, mem 1.8 GiB @ 00 02:54:20.414, mean 00 00:00:00.018
step 578000: train loss 2.3220694, val loss 2.3265288, mem 1.8 GiB @ 00 02:54:29.536, mean 00 00:00:00.018
step 578500: train loss 2.3195157, val loss 2.3299918, mem 1.8 GiB @ 00 02:54:38.692, mean 00 00:00:00.018
step 579000: train loss 2.3248498, val loss 2.3131719, mem 1.8 GiB @ 00 02:54:47.747, mean 00 00:00:00.018
step 579500: train loss 2.3145638, val loss 2.329055, mem 1.8 GiB @ 00 02:54:56.886, mean 00 00:00:00.018
step 580000: train loss 2.3228743, val loss 2.3176746, mem 1.8 GiB @ 00 02:55:06.021, mean 00 00:00:00.018
step 580500: train loss 2.3272295, val loss 2.3184454, mem 1.8 GiB @ 00 02:55:15.149, mean 00 00:00:00.018
step 581000: train loss 2.3195713, val loss 2.3201163, mem 1.8 GiB @ 00 02:55:24.196, mean 00 00:00:00.018
step 581500: train loss 2.3270311, val loss 2.3218355, mem 1.8 GiB @ 00 02:55:33.349, mean 00 00:00:00.018
step 582000: train loss 2.3309793, val loss 2.3233135, mem 1.8 GiB @ 00 02:55:42.489, mean 00 00:00:00.018
step 582500: train loss 2.3224242, val loss 2.3143625, mem 1.8 GiB @ 00 02:55:51.553, mean 00 00:00:00.018
step 583000: train loss 2.3096883, val loss 2.3168516, mem 1.8 GiB @ 00 02:56:00.682, mean 00 00:00:00.018
step 583500: train loss 2.3252676, val loss 2.3145359, mem 1.8 GiB @ 00 02:56:09.839, mean 00 00:00:00.018
step 584000: train loss 2.3229096, val loss 2.3116112, mem 1.8 GiB @ 00 02:56:18.949, mean 00 00:00:00.018
step 584500: train loss 2.3236234, val loss 2.3170018, mem 1.8 GiB @ 00 02:56:28.012, mean 00 00:00:00.018
step 585000: train loss 2.3235657, val loss 2.3272545, mem 1.8 GiB @ 00 02:56:37.166, mean 00 00:00:00.018
step 585500: train loss 2.347879, val loss 2.31938, mem 1.8 GiB @ 00 02:56:46.298, mean 00 00:00:00.018
step 586000: train loss 2.3286688, val loss 2.3359675, mem 1.8 GiB @ 00 02:56:55.438, mean 00 00:00:00.018
step 586500: train loss 2.323351, val loss 2.3054464, mem 1.8 GiB @ 00 02:57:04.610, mean 00 00:00:00.018
step 587000: train loss 2.3103914, val loss 2.3184018, mem 1.8 GiB @ 00 02:57:13.745, mean 00 00:00:00.018
step 587500: train loss 2.3030713, val loss 2.3148727, mem 1.8 GiB @ 00 02:57:22.923, mean 00 00:00:00.018
step 588000: train loss 2.3185172, val loss 2.3175092, mem 1.8 GiB @ 00 02:57:32.089, mean 00 00:00:00.018
step 588500: train loss 2.313155, val loss 2.3246512, mem 1.8 GiB @ 00 02:57:41.267, mean 00 00:00:00.018
step 589000: train loss 2.3185701, val loss 2.3346858, mem 1.8 GiB @ 00 02:57:50.444, mean 00 00:00:00.018
step 589500: train loss 2.344179, val loss 2.3256993, mem 1.8 GiB @ 00 02:57:59.482, mean 00 00:00:00.018
step 590000: train loss 2.329632, val loss 2.3299994, mem 1.8 GiB @ 00 02:58:08.594, mean 00 00:00:00.018
step 590500: train loss 2.3234274, val loss 2.3092484, mem 1.8 GiB @ 00 02:58:17.728, mean 00 00:00:00.018
step 591000: train loss 2.3244472, val loss 2.3174407, mem 1.8 GiB @ 00 02:58:26.873, mean 00 00:00:00.018
step 591500: train loss 2.3203762, val loss 2.327473, mem 1.8 GiB @ 00 02:58:35.931, mean 00 00:00:00.018
step 592000: train loss 2.3255925, val loss 2.3044422, mem 1.8 GiB @ 00 02:58:45.068, mean 00 00:00:00.018
step 592500: train loss 2.2960868, val loss 2.312803, mem 1.8 GiB @ 00 02:58:54.229, mean 00 00:00:00.018
step 593000: train loss 2.3243637, val loss 2.317543, mem 1.8 GiB @ 00 02:59:03.338, mean 00 00:00:00.018
step 593500: train loss 2.3254132, val loss 2.3203993, mem 1.8 GiB @ 00 02:59:12.474, mean 00 00:00:00.018
step 594000: train loss 2.312744, val loss 2.2972994, mem 1.8 GiB @ 00 02:59:21.605, mean 00 00:00:00.018
step 594500: train loss 2.3150928, val loss 2.3104165, mem 1.8 GiB @ 00 02:59:30.745, mean 00 00:00:00.018
step 595000: train loss 2.3074439, val loss 2.3044872, mem 1.8 GiB @ 00 02:59:39.603, mean 00 00:00:00.017
step 595500: train loss 2.3048897, val loss 2.3276963, mem 1.8 GiB @ 00 02:59:48.408, mean 00 00:00:00.017
step 596000: train loss 2.3183203, val loss 2.3135316, mem 1.8 GiB @ 00 02:59:57.086, mean 00 00:00:00.017
step 596500: train loss 2.3234549, val loss 2.3071833, mem 1.8 GiB @ 00 03:00:05.859, mean 00 00:00:00.017
step 597000: train loss 2.3089044, val loss 2.303861, mem 1.8 GiB @ 00 03:00:14.627, mean 00 00:00:00.017
step 597500: train loss 2.3297462, val loss 2.3188994, mem 1.8 GiB @ 00 03:00:23.404, mean 00 00:00:00.017
step 598000: train loss 2.3181086, val loss 2.3061166, mem 1.8 GiB @ 00 03:00:32.211, mean 00 00:00:00.017
step 598500: train loss 2.3126132, val loss 2.303684, mem 1.8 GiB @ 00 03:00:41.034, mean 00 00:00:00.017
step 599000: train loss 2.3093224, val loss 2.3253648, mem 1.8 GiB @ 00 03:00:49.842, mean 00 00:00:00.017
step 599500: train loss 2.3066359, val loss 2.3260183, mem 1.8 GiB @ 00 03:00:58.608, mean 00 00:00:00.017
step 600000: train loss 2.3149905, val loss 2.3172703, mem 1.8 GiB @ 00 03:01:07.391, mean 00 00:00:00.017
step 600500: train loss 2.3153174, val loss 2.3212404, mem 1.8 GiB @ 00 03:01:16.308, mean 00 00:00:00.017
step 601000: train loss 2.307925, val loss 2.32034, mem 1.8 GiB @ 00 03:01:25.518, mean 00 00:00:00.018
step 601500: train loss 2.3106024, val loss 2.3148134, mem 1.8 GiB @ 00 03:01:34.744, mean 00 00:00:00.018
step 602000: train loss 2.3182251, val loss 2.3197787, mem 1.8 GiB @ 00 03:01:43.577, mean 00 00:00:00.017
step 602500: train loss 2.3393493, val loss 2.3089056, mem 1.8 GiB @ 00 03:01:52.264, mean 00 00:00:00.017
step 603000: train loss 2.3428147, val loss 2.3002663, mem 1.8 GiB @ 00 03:02:01.055, mean 00 00:00:00.017
step 603500: train loss 2.335127, val loss 2.3103633, mem 1.8 GiB @ 00 03:02:09.880, mean 00 00:00:00.017
step 604000: train loss 2.2985513, val loss 2.3319366, mem 1.8 GiB @ 00 03:02:18.543, mean 00 00:00:00.017
step 604500: train loss 2.307102, val loss 2.3111167, mem 1.8 GiB @ 00 03:02:27.320, mean 00 00:00:00.017
step 605000: train loss 2.3065076, val loss 2.3183773, mem 1.8 GiB @ 00 03:02:36.124, mean 00 00:00:00.017
step 605500: train loss 2.3184266, val loss 2.3027754, mem 1.8 GiB @ 00 03:02:44.910, mean 00 00:00:00.017
step 606000: train loss 2.3202963, val loss 2.3187096, mem 1.8 GiB @ 00 03:02:53.588, mean 00 00:00:00.017
step 606500: train loss 2.3159473, val loss 2.3104608, mem 1.8 GiB @ 00 03:03:02.410, mean 00 00:00:00.017
step 607000: train loss 2.3194795, val loss 2.3050017, mem 1.8 GiB @ 00 03:03:11.464, mean 00 00:00:00.018
step 607500: train loss 2.3068535, val loss 2.3184056, mem 1.8 GiB @ 00 03:03:20.528, mean 00 00:00:00.018
step 608000: train loss 2.3040216, val loss 2.3088067, mem 1.8 GiB @ 00 03:03:29.305, mean 00 00:00:00.017
step 608500: train loss 2.3053863, val loss 2.3326135, mem 1.8 GiB @ 00 03:03:38.095, mean 00 00:00:00.017
step 609000: train loss 2.3312109, val loss 2.3181162, mem 1.8 GiB @ 00 03:03:47.132, mean 00 00:00:00.018
step 609500: train loss 2.3071477, val loss 2.3088384, mem 1.8 GiB @ 00 03:03:56.379, mean 00 00:00:00.018
step 610000: train loss 2.3158095, val loss 2.3033392, mem 1.8 GiB @ 00 03:04:05.192, mean 00 00:00:00.017
step 610500: train loss 2.3258178, val loss 2.3068554, mem 1.8 GiB @ 00 03:04:14.001, mean 00 00:00:00.017
step 611000: train loss 2.314968, val loss 2.3130035, mem 1.8 GiB @ 00 03:04:22.972, mean 00 00:00:00.017
step 611500: train loss 2.309017, val loss 2.3014596, mem 1.8 GiB @ 00 03:04:32.145, mean 00 00:00:00.018
step 612000: train loss 2.3103518, val loss 2.3073437, mem 1.8 GiB @ 00 03:04:40.830, mean 00 00:00:00.017
step 612500: train loss 2.3092923, val loss 2.3030002, mem 1.8 GiB @ 00 03:04:49.799, mean 00 00:00:00.017
step 613000: train loss 2.312412, val loss 2.3167007, mem 1.8 GiB @ 00 03:04:59.047, mean 00 00:00:00.018
step 613500: train loss 2.3100448, val loss 2.3007827, mem 1.8 GiB @ 00 03:05:08.287, mean 00 00:00:00.018
step 614000: train loss 2.3063247, val loss 2.3153574, mem 1.8 GiB @ 00 03:05:17.370, mean 00 00:00:00.018
step 614500: train loss 2.2924669, val loss 2.3184755, mem 1.8 GiB @ 00 03:05:26.620, mean 00 00:00:00.018
step 615000: train loss 2.3132744, val loss 2.3108294, mem 1.8 GiB @ 00 03:05:35.409, mean 00 00:00:00.017
step 615500: train loss 2.302254, val loss 2.3076525, mem 1.8 GiB @ 00 03:05:44.130, mean 00 00:00:00.017
step 616000: train loss 2.3179903, val loss 2.3017159, mem 1.8 GiB @ 00 03:05:52.957, mean 00 00:00:00.017
step 616500: train loss 2.3036864, val loss 2.316827, mem 1.8 GiB @ 00 03:06:01.810, mean 00 00:00:00.017
step 617000: train loss 2.2931473, val loss 2.305501, mem 1.8 GiB @ 00 03:06:10.971, mean 00 00:00:00.018
step 617500: train loss 2.3006804, val loss 2.3106997, mem 1.8 GiB @ 00 03:06:20.039, mean 00 00:00:00.018
step 618000: train loss 2.2971127, val loss 2.3167589, mem 1.8 GiB @ 00 03:06:29.210, mean 00 00:00:00.018
step 618500: train loss 2.3252127, val loss 2.3070557, mem 1.8 GiB @ 00 03:06:38.363, mean 00 00:00:00.018
step 619000: train loss 2.29744, val loss 2.3150108, mem 1.8 GiB @ 00 03:06:47.530, mean 00 00:00:00.018
step 619500: train loss 2.3070602, val loss 2.310112, mem 1.8 GiB @ 00 03:06:56.688, mean 00 00:00:00.018
step 620000: train loss 2.3087325, val loss 2.3162189, mem 1.8 GiB @ 00 03:07:05.746, mean 00 00:00:00.018
step 620500: train loss 2.3180459, val loss 2.3092062, mem 1.8 GiB @ 00 03:07:14.885, mean 00 00:00:00.018
step 621000: train loss 2.3027465, val loss 2.311192, mem 1.8 GiB @ 00 03:07:24.032, mean 00 00:00:00.018
step 621500: train loss 2.3246386, val loss 2.311622, mem 1.8 GiB @ 00 03:07:33.190, mean 00 00:00:00.018
step 622000: train loss 2.3117485, val loss 2.312063, mem 1.8 GiB @ 00 03:07:42.346, mean 00 00:00:00.018
step 622500: train loss 2.313538, val loss 2.3044667, mem 1.8 GiB @ 00 03:07:51.516, mean 00 00:00:00.018
step 623000: train loss 2.3097947, val loss 2.3140657, mem 1.8 GiB @ 00 03:08:00.661, mean 00 00:00:00.018
step 623500: train loss 2.3038003, val loss 2.3131764, mem 1.8 GiB @ 00 03:08:09.544, mean 00 00:00:00.017
step 624000: train loss 2.3060522, val loss 2.3046165, mem 1.8 GiB @ 00 03:08:18.313, mean 00 00:00:00.017
step 624500: train loss 2.3101997, val loss 2.3137896, mem 1.8 GiB @ 00 03:08:27.133, mean 00 00:00:00.017
step 625000: train loss 2.3300827, val loss 2.3093305, mem 1.8 GiB @ 00 03:08:35.890, mean 00 00:00:00.017
step 625500: train loss 2.2939572, val loss 2.3078089, mem 1.8 GiB @ 00 03:08:44.834, mean 00 00:00:00.017
step 626000: train loss 2.3052123, val loss 2.3061159, mem 1.8 GiB @ 00 03:08:53.935, mean 00 00:00:00.018
step 626500: train loss 2.3008606, val loss 2.3087168, mem 1.8 GiB @ 00 03:09:03.170, mean 00 00:00:00.018
step 627000: train loss 2.324859, val loss 2.30585, mem 1.8 GiB @ 00 03:09:11.919, mean 00 00:00:00.017
step 627500: train loss 2.3105087, val loss 2.2999065, mem 1.8 GiB @ 00 03:09:20.676, mean 00 00:00:00.017
step 628000: train loss 2.2951322, val loss 2.3009887, mem 1.8 GiB @ 00 03:09:29.361, mean 00 00:00:00.017
step 628500: train loss 2.3047512, val loss 2.2914093, mem 1.8 GiB @ 00 03:09:38.139, mean 00 00:00:00.017
step 629000: train loss 2.3078976, val loss 2.3013716, mem 1.8 GiB @ 00 03:09:46.922, mean 00 00:00:00.017
step 629500: train loss 2.2813153, val loss 2.313811, mem 1.8 GiB @ 00 03:09:55.692, mean 00 00:00:00.017
step 630000: train loss 2.3110461, val loss 2.3243973, mem 1.8 GiB @ 00 03:10:04.474, mean 00 00:00:00.017
step 630500: train loss 2.3025467, val loss 2.3010418, mem 1.8 GiB @ 00 03:10:13.166, mean 00 00:00:00.017
step 631000: train loss 2.3007183, val loss 2.3165462, mem 1.8 GiB @ 00 03:10:21.977, mean 00 00:00:00.017
step 631500: train loss 2.3043745, val loss 2.29828, mem 1.8 GiB @ 00 03:10:30.706, mean 00 00:00:00.017
step 632000: train loss 2.2972264, val loss 2.300483, mem 1.8 GiB @ 00 03:10:39.356, mean 00 00:00:00.017
step 632500: train loss 2.2786138, val loss 2.3210952, mem 1.8 GiB @ 00 03:10:48.161, mean 00 00:00:00.017
step 633000: train loss 2.2968988, val loss 2.2977245, mem 1.8 GiB @ 00 03:10:56.907, mean 00 00:00:00.017
step 633500: train loss 2.3158169, val loss 2.303964, mem 1.8 GiB @ 00 03:11:05.641, mean 00 00:00:00.017
step 634000: train loss 2.315812, val loss 2.3110852, mem 1.8 GiB @ 00 03:11:14.416, mean 00 00:00:00.017
step 634500: train loss 2.3261993, val loss 2.3018687, mem 1.8 GiB @ 00 03:11:23.178, mean 00 00:00:00.017
step 635000: train loss 2.3028014, val loss 2.3075945, mem 1.8 GiB @ 00 03:11:31.936, mean 00 00:00:00.017
step 635500: train loss 2.317659, val loss 2.2906113, mem 1.8 GiB @ 00 03:11:40.715, mean 00 00:00:00.017
step 636000: train loss 2.3073537, val loss 2.3047054, mem 1.8 GiB @ 00 03:11:49.467, mean 00 00:00:00.017
step 636500: train loss 2.3035944, val loss 2.3036857, mem 1.8 GiB @ 00 03:11:58.152, mean 00 00:00:00.017
step 637000: train loss 2.3140788, val loss 2.3071005, mem 1.8 GiB @ 00 03:12:06.954, mean 00 00:00:00.017
step 637500: train loss 2.3049545, val loss 2.291559, mem 1.8 GiB @ 00 03:12:15.739, mean 00 00:00:00.017
step 638000: train loss 2.320001, val loss 2.295299, mem 1.8 GiB @ 00 03:12:24.555, mean 00 00:00:00.017
step 638500: train loss 2.2960584, val loss 2.2939699, mem 1.8 GiB @ 00 03:12:33.318, mean 00 00:00:00.017
step 639000: train loss 2.305879, val loss 2.2971702, mem 1.8 GiB @ 00 03:12:41.997, mean 00 00:00:00.017
step 639500: train loss 2.3101234, val loss 2.2995012, mem 1.8 GiB @ 00 03:12:50.768, mean 00 00:00:00.017
step 640000: train loss 2.3077893, val loss 2.2822864, mem 1.8 GiB @ 00 03:12:59.385, mean 00 00:00:00.017
step 640500: train loss 2.2902634, val loss 2.2963655, mem 1.8 GiB @ 00 03:13:08.158, mean 00 00:00:00.017
step 641000: train loss 2.3070679, val loss 2.3011456, mem 1.8 GiB @ 00 03:13:16.923, mean 00 00:00:00.017
step 641500: train loss 2.3016183, val loss 2.2909226, mem 1.8 GiB @ 00 03:13:25.725, mean 00 00:00:00.017
step 642000: train loss 2.2972481, val loss 2.296335, mem 1.8 GiB @ 00 03:13:34.520, mean 00 00:00:00.017
step 642500: train loss 2.314465, val loss 2.3176634, mem 1.8 GiB @ 00 03:13:43.196, mean 00 00:00:00.017
step 643000: train loss 2.2964988, val loss 2.2860997, mem 1.8 GiB @ 00 03:13:51.978, mean 00 00:00:00.017
step 643500: train loss 2.2831419, val loss 2.3115747, mem 1.8 GiB @ 00 03:14:00.606, mean 00 00:00:00.017
step 644000: train loss 2.293556, val loss 2.308411, mem 1.8 GiB @ 00 03:14:09.443, mean 00 00:00:00.017
step 644500: train loss 2.303934, val loss 2.327956, mem 1.8 GiB @ 00 03:14:18.603, mean 00 00:00:00.018
step 645000: train loss 2.291738, val loss 2.2955785, mem 1.8 GiB @ 00 03:14:27.653, mean 00 00:00:00.018
step 645500: train loss 2.2928119, val loss 2.3052053, mem 1.8 GiB @ 00 03:14:36.805, mean 00 00:00:00.018
step 646000: train loss 2.3015556, val loss 2.3016086, mem 1.8 GiB @ 00 03:14:45.999, mean 00 00:00:00.018
step 646500: train loss 2.2863533, val loss 2.3039863, mem 1.8 GiB @ 00 03:14:55.155, mean 00 00:00:00.018
step 647000: train loss 2.3058753, val loss 2.306852, mem 1.8 GiB @ 00 03:15:04.287, mean 00 00:00:00.018
step 647500: train loss 2.3031561, val loss 2.2938018, mem 1.8 GiB @ 00 03:15:13.437, mean 00 00:00:00.018
step 648000: train loss 2.3014235, val loss 2.3098958, mem 1.8 GiB @ 00 03:15:22.584, mean 00 00:00:00.018
step 648500: train loss 2.293663, val loss 2.297832, mem 1.8 GiB @ 00 03:15:31.760, mean 00 00:00:00.018
step 649000: train loss 2.3008022, val loss 2.3083782, mem 1.8 GiB @ 00 03:15:40.928, mean 00 00:00:00.018
step 649500: train loss 2.3086586, val loss 2.292764, mem 1.8 GiB @ 00 03:15:50.103, mean 00 00:00:00.018
step 650000: train loss 2.2909122, val loss 2.3004694, mem 1.8 GiB @ 00 03:15:59.255, mean 00 00:00:00.018
step 650500: train loss 2.308872, val loss 2.2987018, mem 1.8 GiB @ 00 03:16:08.342, mean 00 00:00:00.018
step 651000: train loss 2.2859778, val loss 2.300415, mem 1.8 GiB @ 00 03:16:17.502, mean 00 00:00:00.018
step 651500: train loss 2.2984788, val loss 2.3072267, mem 1.8 GiB @ 00 03:16:26.639, mean 00 00:00:00.018
step 652000: train loss 2.2977326, val loss 2.3144586, mem 1.8 GiB @ 00 03:16:35.817, mean 00 00:00:00.018
step 652500: train loss 2.3008084, val loss 2.3212404, mem 1.8 GiB @ 00 03:16:44.979, mean 00 00:00:00.018
step 653000: train loss 2.29159, val loss 2.2947986, mem 1.8 GiB @ 00 03:16:54.133, mean 00 00:00:00.018
step 653500: train loss 2.2882485, val loss 2.29617, mem 1.8 GiB @ 00 03:17:03.267, mean 00 00:00:00.018
step 654000: train loss 2.3412645, val loss 2.3077228, mem 1.8 GiB @ 00 03:17:12.328, mean 00 00:00:00.018
step 654500: train loss 2.287687, val loss 2.3050256, mem 1.8 GiB @ 00 03:17:21.476, mean 00 00:00:00.018
step 655000: train loss 2.305784, val loss 2.2951255, mem 1.8 GiB @ 00 03:17:30.608, mean 00 00:00:00.018
step 655500: train loss 2.2845242, val loss 2.3060708, mem 1.8 GiB @ 00 03:17:39.733, mean 00 00:00:00.018
step 656000: train loss 2.3028417, val loss 2.305944, mem 1.8 GiB @ 00 03:17:48.871, mean 00 00:00:00.018
step 656500: train loss 2.2959988, val loss 2.2750258, mem 1.8 GiB @ 00 03:17:57.901, mean 00 00:00:00.018
step 657000: train loss 2.2953055, val loss 2.2953768, mem 1.8 GiB @ 00 03:18:07.022, mean 00 00:00:00.018
step 657500: train loss 2.3043542, val loss 2.2842615, mem 1.8 GiB @ 00 03:18:16.140, mean 00 00:00:00.018
step 658000: train loss 2.3066568, val loss 2.300196, mem 1.8 GiB @ 00 03:18:25.279, mean 00 00:00:00.018
step 658500: train loss 2.2992141, val loss 2.2956378, mem 1.8 GiB @ 00 03:18:34.423, mean 00 00:00:00.018
step 659000: train loss 2.3146415, val loss 2.3028378, mem 1.8 GiB @ 00 03:18:43.452, mean 00 00:00:00.018
step 659500: train loss 2.2823896, val loss 2.3087718, mem 1.8 GiB @ 00 03:18:52.584, mean 00 00:00:00.018
step 660000: train loss 2.319877, val loss 2.3032854, mem 1.8 GiB @ 00 03:19:01.743, mean 00 00:00:00.018
step 660500: train loss 2.318558, val loss 2.3171718, mem 1.8 GiB @ 00 03:19:10.904, mean 00 00:00:00.018
step 661000: train loss 2.2823446, val loss 2.3118055, mem 1.8 GiB @ 00 03:19:20.060, mean 00 00:00:00.018
step 661500: train loss 2.2905521, val loss 2.303266, mem 1.8 GiB @ 00 03:19:29.106, mean 00 00:00:00.018
step 662000: train loss 2.2983565, val loss 2.3160448, mem 1.8 GiB @ 00 03:19:38.274, mean 00 00:00:00.018
step 662500: train loss 2.3005319, val loss 2.2939534, mem 1.8 GiB @ 00 03:19:47.417, mean 00 00:00:00.018
step 663000: train loss 2.30373, val loss 2.310516, mem 1.8 GiB @ 00 03:19:56.550, mean 00 00:00:00.018
step 663500: train loss 2.2834153, val loss 2.309705, mem 1.8 GiB @ 00 03:20:05.612, mean 00 00:00:00.018
step 664000: train loss 2.3042352, val loss 2.299471, mem 1.8 GiB @ 00 03:20:14.764, mean 00 00:00:00.018
step 664500: train loss 2.2868762, val loss 2.2988455, mem 1.8 GiB @ 00 03:20:23.891, mean 00 00:00:00.018
step 665000: train loss 2.2969077, val loss 2.2887242, mem 1.8 GiB @ 00 03:20:33.038, mean 00 00:00:00.018
step 665500: train loss 2.298933, val loss 2.2972004, mem 1.8 GiB @ 00 03:20:42.102, mean 00 00:00:00.018
step 666000: train loss 2.3008516, val loss 2.3005362, mem 1.8 GiB @ 00 03:20:51.258, mean 00 00:00:00.018
step 666500: train loss 2.3094218, val loss 2.2960315, mem 1.8 GiB @ 00 03:21:00.391, mean 00 00:00:00.018
step 667000: train loss 2.295313, val loss 2.2890468, mem 1.8 GiB @ 00 03:21:09.430, mean 00 00:00:00.018
step 667500: train loss 2.306277, val loss 2.304493, mem 1.8 GiB @ 00 03:21:18.563, mean 00 00:00:00.018
step 668000: train loss 2.2994401, val loss 2.2929947, mem 1.8 GiB @ 00 03:21:27.716, mean 00 00:00:00.018
step 668500: train loss 2.2888303, val loss 2.2970815, mem 1.8 GiB @ 00 03:21:36.859, mean 00 00:00:00.018
step 669000: train loss 2.2838101, val loss 2.2999568, mem 1.8 GiB @ 00 03:21:46.015, mean 00 00:00:00.018
step 669500: train loss 2.2923691, val loss 2.3023589, mem 1.8 GiB @ 00 03:21:55.136, mean 00 00:00:00.018
step 670000: train loss 2.307009, val loss 2.296037, mem 1.8 GiB @ 00 03:22:04.293, mean 00 00:00:00.018
step 670500: train loss 2.2960227, val loss 2.302245, mem 1.8 GiB @ 00 03:22:13.438, mean 00 00:00:00.018
step 671000: train loss 2.2833288, val loss 2.3113935, mem 1.8 GiB @ 00 03:22:22.607, mean 00 00:00:00.018
step 671500: train loss 2.3103912, val loss 2.2967393, mem 1.8 GiB @ 00 03:22:31.763, mean 00 00:00:00.018
step 672000: train loss 2.3020658, val loss 2.293443, mem 1.8 GiB @ 00 03:22:40.920, mean 00 00:00:00.018
step 672500: train loss 2.2997828, val loss 2.2968237, mem 1.8 GiB @ 00 03:22:50.062, mean 00 00:00:00.018
step 673000: train loss 2.2885, val loss 2.292739, mem 1.8 GiB @ 00 03:22:59.202, mean 00 00:00:00.018
step 673500: train loss 2.2883458, val loss 2.3050754, mem 1.8 GiB @ 00 03:23:08.369, mean 00 00:00:00.018
step 674000: train loss 2.271993, val loss 2.2909682, mem 1.8 GiB @ 00 03:23:17.420, mean 00 00:00:00.018
step 674500: train loss 2.2908704, val loss 2.2836957, mem 1.8 GiB @ 00 03:23:26.563, mean 00 00:00:00.018
step 675000: train loss 2.285819, val loss 2.3075142, mem 1.8 GiB @ 00 03:23:35.706, mean 00 00:00:00.018
step 675500: train loss 2.2884932, val loss 2.2979875, mem 1.8 GiB @ 00 03:23:44.860, mean 00 00:00:00.018
step 676000: train loss 2.3031259, val loss 2.3030589, mem 1.8 GiB @ 00 03:23:54.037, mean 00 00:00:00.018
step 676500: train loss 2.2996423, val loss 2.3043609, mem 1.8 GiB @ 00 03:24:03.089, mean 00 00:00:00.018
step 677000: train loss 2.2839782, val loss 2.298454, mem 1.8 GiB @ 00 03:24:12.226, mean 00 00:00:00.018
step 677500: train loss 2.2807262, val loss 2.2949276, mem 1.8 GiB @ 00 03:24:21.364, mean 00 00:00:00.018
step 678000: train loss 2.3125625, val loss 2.3002346, mem 1.8 GiB @ 00 03:24:30.511, mean 00 00:00:00.018
step 678500: train loss 2.2800064, val loss 2.2856731, mem 1.8 GiB @ 00 03:24:39.672, mean 00 00:00:00.018
step 679000: train loss 2.2860303, val loss 2.2891037, mem 1.8 GiB @ 00 03:24:48.718, mean 00 00:00:00.018
step 679500: train loss 2.2850828, val loss 2.2957366, mem 1.8 GiB @ 00 03:24:57.861, mean 00 00:00:00.018
step 680000: train loss 2.2993853, val loss 2.305786, mem 1.8 GiB @ 00 03:25:07.011, mean 00 00:00:00.018
step 680500: train loss 2.2989807, val loss 2.2722704, mem 1.8 GiB @ 00 03:25:16.046, mean 00 00:00:00.018
step 681000: train loss 2.2857225, val loss 2.28624, mem 1.8 GiB @ 00 03:25:25.216, mean 00 00:00:00.018
step 681500: train loss 2.289432, val loss 2.2885315, mem 1.8 GiB @ 00 03:25:34.369, mean 00 00:00:00.018
step 682000: train loss 2.2909584, val loss 2.3071756, mem 1.8 GiB @ 00 03:25:43.398, mean 00 00:00:00.018
step 682500: train loss 2.2878983, val loss 2.305722, mem 1.8 GiB @ 00 03:25:52.568, mean 00 00:00:00.018
step 683000: train loss 2.3009741, val loss 2.292141, mem 1.8 GiB @ 00 03:26:01.624, mean 00 00:00:00.018
step 683500: train loss 2.3032393, val loss 2.30667, mem 1.8 GiB @ 00 03:26:10.776, mean 00 00:00:00.018
step 684000: train loss 2.286202, val loss 2.294317, mem 1.8 GiB @ 00 03:26:19.932, mean 00 00:00:00.018
step 684500: train loss 2.2949548, val loss 2.2835343, mem 1.8 GiB @ 00 03:26:29.089, mean 00 00:00:00.018
step 685000: train loss 2.2917407, val loss 2.2841213, mem 1.8 GiB @ 00 03:26:38.145, mean 00 00:00:00.018
step 685500: train loss 2.2914917, val loss 2.2891164, mem 1.8 GiB @ 00 03:26:47.297, mean 00 00:00:00.018
step 686000: train loss 2.298737, val loss 2.2934813, mem 1.8 GiB @ 00 03:26:56.455, mean 00 00:00:00.018
step 686500: train loss 2.2933493, val loss 2.298618, mem 1.8 GiB @ 00 03:27:05.622, mean 00 00:00:00.018
step 687000: train loss 2.291691, val loss 2.2908494, mem 1.8 GiB @ 00 03:27:14.775, mean 00 00:00:00.018
step 687500: train loss 2.2931561, val loss 2.3000464, mem 1.8 GiB @ 00 03:27:23.933, mean 00 00:00:00.018
step 688000: train loss 2.295091, val loss 2.292425, mem 1.8 GiB @ 00 03:27:33.097, mean 00 00:00:00.018
step 688500: train loss 2.2872913, val loss 2.302908, mem 1.8 GiB @ 00 03:27:42.279, mean 00 00:00:00.018
step 689000: train loss 2.2854645, val loss 2.286331, mem 1.8 GiB @ 00 03:27:51.446, mean 00 00:00:00.018
step 689500: train loss 2.2855272, val loss 2.2917972, mem 1.8 GiB @ 00 03:28:00.586, mean 00 00:00:00.018
step 690000: train loss 2.2867677, val loss 2.2940617, mem 1.8 GiB @ 00 03:28:09.749, mean 00 00:00:00.018
step 690500: train loss 2.2818375, val loss 2.2915401, mem 1.8 GiB @ 00 03:28:18.900, mean 00 00:00:00.018
step 691000: train loss 2.2932453, val loss 2.3011715, mem 1.8 GiB @ 00 03:28:27.848, mean 00 00:00:00.017
step 691500: train loss 2.2898128, val loss 2.3084676, mem 1.8 GiB @ 00 03:28:36.563, mean 00 00:00:00.017
step 692000: train loss 2.2925372, val loss 2.3020697, mem 1.8 GiB @ 00 03:28:45.464, mean 00 00:00:00.017
step 692500: train loss 2.2905102, val loss 2.2882373, mem 1.8 GiB @ 00 03:28:54.690, mean 00 00:00:00.018
step 693000: train loss 2.2873197, val loss 2.3019736, mem 1.8 GiB @ 00 03:29:03.498, mean 00 00:00:00.017
step 693500: train loss 2.2921371, val loss 2.279833, mem 1.8 GiB @ 00 03:29:12.297, mean 00 00:00:00.017
step 694000: train loss 2.280135, val loss 2.285626, mem 1.8 GiB @ 00 03:29:21.245, mean 00 00:00:00.017
step 694500: train loss 2.2711344, val loss 2.2838259, mem 1.8 GiB @ 00 03:29:29.984, mean 00 00:00:00.017
step 695000: train loss 2.286898, val loss 2.306506, mem 1.8 GiB @ 00 03:29:39.042, mean 00 00:00:00.018
step 695500: train loss 2.3049734, val loss 2.2814143, mem 1.8 GiB @ 00 03:29:48.010, mean 00 00:00:00.017
step 696000: train loss 2.2913823, val loss 2.2889993, mem 1.8 GiB @ 00 03:29:56.806, mean 00 00:00:00.017
step 696500: train loss 2.291847, val loss 2.2947402, mem 1.8 GiB @ 00 03:30:05.509, mean 00 00:00:00.017
step 697000: train loss 2.2907627, val loss 2.2930977, mem 1.8 GiB @ 00 03:30:14.368, mean 00 00:00:00.017
step 697500: train loss 2.2883854, val loss 2.2885995, mem 1.8 GiB @ 00 03:30:23.175, mean 00 00:00:00.017
step 698000: train loss 2.2794936, val loss 2.2900174, mem 1.8 GiB @ 00 03:30:31.975, mean 00 00:00:00.017
step 698500: train loss 2.2944753, val loss 2.2851098, mem 1.8 GiB @ 00 03:30:40.764, mean 00 00:00:00.017
step 699000: train loss 2.2925293, val loss 2.3037977, mem 1.8 GiB @ 00 03:30:49.615, mean 00 00:00:00.017
step 699500: train loss 2.2794123, val loss 2.292784, mem 1.8 GiB @ 00 03:30:58.756, mean 00 00:00:00.018
step 700000: train loss 2.2841105, val loss 2.288748, mem 1.8 GiB @ 00 03:31:07.956, mean 00 00:00:00.018
step 700500: train loss 2.276564, val loss 2.2946615, mem 1.8 GiB @ 00 03:31:17.133, mean 00 00:00:00.018
step 701000: train loss 2.292443, val loss 2.2831316, mem 1.8 GiB @ 00 03:31:26.306, mean 00 00:00:00.018
step 701500: train loss 2.2803562, val loss 2.285081, mem 1.8 GiB @ 00 03:31:35.532, mean 00 00:00:00.018
step 702000: train loss 2.2947283, val loss 2.298304, mem 1.8 GiB @ 00 03:31:44.897, mean 00 00:00:00.018
step 702500: train loss 2.2763805, val loss 2.2861364, mem 1.8 GiB @ 00 03:31:54.061, mean 00 00:00:00.018
step 703000: train loss 2.286979, val loss 2.2887847, mem 1.8 GiB @ 00 03:32:03.224, mean 00 00:00:00.018
step 703500: train loss 2.2872958, val loss 2.2769544, mem 1.8 GiB @ 00 03:32:12.300, mean 00 00:00:00.018
step 704000: train loss 2.2773714, val loss 2.2868588, mem 1.8 GiB @ 00 03:32:21.629, mean 00 00:00:00.018
step 704500: train loss 2.2951915, val loss 2.2851398, mem 1.8 GiB @ 00 03:32:30.811, mean 00 00:00:00.018
step 705000: train loss 2.2621007, val loss 2.28557, mem 1.8 GiB @ 00 03:32:40.084, mean 00 00:00:00.018
step 705500: train loss 2.2861671, val loss 2.3001208, mem 1.8 GiB @ 00 03:32:49.249, mean 00 00:00:00.018
step 706000: train loss 2.283359, val loss 2.2966146, mem 1.8 GiB @ 00 03:32:58.452, mean 00 00:00:00.018
step 706500: train loss 2.2831786, val loss 2.2987251, mem 1.8 GiB @ 00 03:33:07.665, mean 00 00:00:00.018
step 707000: train loss 2.2829163, val loss 2.2856371, mem 1.8 GiB @ 00 03:33:16.852, mean 00 00:00:00.018
step 707500: train loss 2.2773166, val loss 2.2847893, mem 1.8 GiB @ 00 03:33:26.032, mean 00 00:00:00.018
step 708000: train loss 2.283919, val loss 2.2961278, mem 1.8 GiB @ 00 03:33:35.222, mean 00 00:00:00.018
step 708500: train loss 2.276219, val loss 2.294294, mem 1.8 GiB @ 00 03:33:44.389, mean 00 00:00:00.018
step 709000: train loss 2.3018255, val loss 2.2860231, mem 1.8 GiB @ 00 03:33:53.580, mean 00 00:00:00.018
step 709500: train loss 2.2688081, val loss 2.291689, mem 1.8 GiB @ 00 03:34:02.893, mean 00 00:00:00.018
step 710000: train loss 2.2850723, val loss 2.296421, mem 1.8 GiB @ 00 03:34:12.095, mean 00 00:00:00.018
step 710500: train loss 2.2746515, val loss 2.2823522, mem 1.8 GiB @ 00 03:34:21.269, mean 00 00:00:00.018
step 711000: train loss 2.2862856, val loss 2.2877457, mem 1.8 GiB @ 00 03:34:30.440, mean 00 00:00:00.018
step 711500: train loss 2.2905393, val loss 2.29381, mem 1.8 GiB @ 00 03:34:39.652, mean 00 00:00:00.018
step 712000: train loss 2.2800016, val loss 2.2795594, mem 1.8 GiB @ 00 03:34:48.859, mean 00 00:00:00.018
step 712500: train loss 2.277238, val loss 2.2871072, mem 1.8 GiB @ 00 03:34:57.952, mean 00 00:00:00.018
step 713000: train loss 2.2693377, val loss 2.2779965, mem 1.8 GiB @ 00 03:35:07.165, mean 00 00:00:00.018
step 713500: train loss 2.2872405, val loss 2.2817144, mem 1.8 GiB @ 00 03:35:16.387, mean 00 00:00:00.018
step 714000: train loss 2.2922618, val loss 2.2914429, mem 1.8 GiB @ 00 03:35:25.688, mean 00 00:00:00.018
step 714500: train loss 2.2933896, val loss 2.2807534, mem 1.8 GiB @ 00 03:35:35.104, mean 00 00:00:00.018
step 715000: train loss 2.2808402, val loss 2.2941284, mem 1.8 GiB @ 00 03:35:44.564, mean 00 00:00:00.018
step 715500: train loss 2.2790077, val loss 2.28591, mem 1.8 GiB @ 00 03:35:53.650, mean 00 00:00:00.018
step 716000: train loss 2.2738504, val loss 2.2732847, mem 1.8 GiB @ 00 03:36:02.981, mean 00 00:00:00.018
step 716500: train loss 2.2799387, val loss 2.300602, mem 1.8 GiB @ 00 03:36:12.200, mean 00 00:00:00.018
step 717000: train loss 2.2858553, val loss 2.287386, mem 1.8 GiB @ 00 03:36:21.364, mean 00 00:00:00.018
step 717500: train loss 2.2737494, val loss 2.284016, mem 1.8 GiB @ 00 03:36:30.529, mean 00 00:00:00.018
step 718000: train loss 2.2752552, val loss 2.274066, mem 1.8 GiB @ 00 03:36:39.876, mean 00 00:00:00.018
step 718500: train loss 2.2778475, val loss 2.2785661, mem 1.8 GiB @ 00 03:36:49.069, mean 00 00:00:00.018
step 719000: train loss 2.2829196, val loss 2.283661, mem 1.8 GiB @ 00 03:36:58.241, mean 00 00:00:00.018
step 719500: train loss 2.2734134, val loss 2.302142, mem 1.8 GiB @ 00 03:37:07.343, mean 00 00:00:00.018
step 720000: train loss 2.2660172, val loss 2.285894, mem 1.8 GiB @ 00 03:37:16.538, mean 00 00:00:00.018
step 720500: train loss 2.2785535, val loss 2.2837067, mem 1.8 GiB @ 00 03:37:25.733, mean 00 00:00:00.018
step 721000: train loss 2.2746513, val loss 2.281136, mem 1.8 GiB @ 00 03:37:34.816, mean 00 00:00:00.018
step 721500: train loss 2.2784693, val loss 2.2754922, mem 1.8 GiB @ 00 03:37:43.957, mean 00 00:00:00.018
step 722000: train loss 2.2816796, val loss 2.2986722, mem 1.8 GiB @ 00 03:37:53.142, mean 00 00:00:00.018
step 722500: train loss 2.280635, val loss 2.2891529, mem 1.8 GiB @ 00 03:38:02.324, mean 00 00:00:00.018
step 723000: train loss 2.2857888, val loss 2.2665756, mem 1.8 GiB @ 00 03:38:11.500, mean 00 00:00:00.018
step 723500: train loss 2.2739663, val loss 2.2796414, mem 1.8 GiB @ 00 03:38:20.667, mean 00 00:00:00.018
step 724000: train loss 2.270349, val loss 2.2811027, mem 1.8 GiB @ 00 03:38:29.796, mean 00 00:00:00.018
step 724500: train loss 2.2920463, val loss 2.2869825, mem 1.8 GiB @ 00 03:38:39.272, mean 00 00:00:00.018
step 725000: train loss 2.2825773, val loss 2.270819, mem 1.8 GiB @ 00 03:38:48.465, mean 00 00:00:00.018
step 725500: train loss 2.2807813, val loss 2.2907033, mem 1.8 GiB @ 00 03:38:57.547, mean 00 00:00:00.018
step 726000: train loss 2.278381, val loss 2.2942564, mem 1.8 GiB @ 00 03:39:06.729, mean 00 00:00:00.018
step 726500: train loss 2.2717578, val loss 2.2723758, mem 1.8 GiB @ 00 03:39:15.969, mean 00 00:00:00.018
step 727000: train loss 2.2886586, val loss 2.2884555, mem 1.8 GiB @ 00 03:39:25.148, mean 00 00:00:00.018
step 727500: train loss 2.2868357, val loss 2.2886403, mem 1.8 GiB @ 00 03:39:34.352, mean 00 00:00:00.018
step 728000: train loss 2.2888658, val loss 2.2835512, mem 1.8 GiB @ 00 03:39:43.424, mean 00 00:00:00.018
step 728500: train loss 2.2793245, val loss 2.28752, mem 1.8 GiB @ 00 03:39:52.629, mean 00 00:00:00.018
step 729000: train loss 2.2814343, val loss 2.267717, mem 1.8 GiB @ 00 03:40:02.055, mean 00 00:00:00.018
step 729500: train loss 2.2655323, val loss 2.2795937, mem 1.8 GiB @ 00 03:40:11.189, mean 00 00:00:00.018
step 730000: train loss 2.2747645, val loss 2.2718275, mem 1.8 GiB @ 00 03:40:20.600, mean 00 00:00:00.018
step 730500: train loss 2.271119, val loss 2.2874398, mem 1.8 GiB @ 00 03:40:29.942, mean 00 00:00:00.018
step 731000: train loss 2.2972238, val loss 2.2857137, mem 1.8 GiB @ 00 03:40:39.146, mean 00 00:00:00.018
step 731500: train loss 2.2691667, val loss 2.295633, mem 1.8 GiB @ 00 03:40:48.318, mean 00 00:00:00.018
step 732000: train loss 2.272052, val loss 2.2829409, mem 1.8 GiB @ 00 03:40:57.396, mean 00 00:00:00.018
step 732500: train loss 2.2625363, val loss 2.2715473, mem 1.8 GiB @ 00 03:41:06.563, mean 00 00:00:00.018
step 733000: train loss 2.287413, val loss 2.2865708, mem 1.8 GiB @ 00 03:41:15.708, mean 00 00:00:00.018
step 733500: train loss 2.2647488, val loss 2.271611, mem 1.8 GiB @ 00 03:41:24.884, mean 00 00:00:00.018
step 734000: train loss 2.26851, val loss 2.2694094, mem 1.8 GiB @ 00 03:41:33.999, mean 00 00:00:00.018
step 734500: train loss 2.262117, val loss 2.2992897, mem 1.8 GiB @ 00 03:41:43.164, mean 00 00:00:00.018
step 735000: train loss 2.2783203, val loss 2.2841508, mem 1.8 GiB @ 00 03:41:52.370, mean 00 00:00:00.018
step 735500: train loss 2.2684016, val loss 2.288933, mem 1.8 GiB @ 00 03:42:01.523, mean 00 00:00:00.018
step 736000: train loss 2.2695038, val loss 2.2817583, mem 1.8 GiB @ 00 03:42:10.610, mean 00 00:00:00.018
step 736500: train loss 2.272045, val loss 2.282048, mem 1.8 GiB @ 00 03:42:19.775, mean 00 00:00:00.018
step 737000: train loss 2.275808, val loss 2.2975066, mem 1.8 GiB @ 00 03:42:28.907, mean 00 00:00:00.018
step 737500: train loss 2.2809563, val loss 2.2786052, mem 1.8 GiB @ 00 03:42:38.069, mean 00 00:00:00.018
step 738000: train loss 2.2704315, val loss 2.2867298, mem 1.8 GiB @ 00 03:42:47.269, mean 00 00:00:00.018
step 738500: train loss 2.2684157, val loss 2.271068, mem 1.8 GiB @ 00 03:42:56.443, mean 00 00:00:00.018
step 739000: train loss 2.2928166, val loss 2.2637858, mem 1.8 GiB @ 00 03:43:05.827, mean 00 00:00:00.018
step 739500: train loss 2.2660582, val loss 2.2862308, mem 1.8 GiB @ 00 03:43:15.441, mean 00 00:00:00.019
step 740000: train loss 2.2753632, val loss 2.2803824, mem 1.8 GiB @ 00 03:43:24.801, mean 00 00:00:00.018
step 740500: train loss 2.2768476, val loss 2.2901382, mem 1.8 GiB @ 00 03:43:34.264, mean 00 00:00:00.018
step 741000: train loss 2.277609, val loss 2.2745354, mem 1.8 GiB @ 00 03:43:43.533, mean 00 00:00:00.018
step 741500: train loss 2.2668009, val loss 2.2896898, mem 1.8 GiB @ 00 03:43:52.714, mean 00 00:00:00.018
step 742000: train loss 2.2701964, val loss 2.2699099, mem 1.8 GiB @ 00 03:44:01.538, mean 00 00:00:00.017
step 742500: train loss 2.2698042, val loss 2.2818162, mem 1.8 GiB @ 00 03:44:10.617, mean 00 00:00:00.018
step 743000: train loss 2.275697, val loss 2.2781198, mem 1.8 GiB @ 00 03:44:19.477, mean 00 00:00:00.017
step 743500: train loss 2.2795098, val loss 2.2972524, mem 1.8 GiB @ 00 03:44:28.460, mean 00 00:00:00.017
step 744000: train loss 2.289917, val loss 2.2585845, mem 1.8 GiB @ 00 03:44:37.642, mean 00 00:00:00.018
step 744500: train loss 2.2615392, val loss 2.2811704, mem 1.8 GiB @ 00 03:44:46.712, mean 00 00:00:00.018
step 745000: train loss 2.2783606, val loss 2.2777061, mem 1.8 GiB @ 00 03:44:55.882, mean 00 00:00:00.018
step 745500: train loss 2.2742956, val loss 2.2850003, mem 1.8 GiB @ 00 03:45:05.061, mean 00 00:00:00.018
step 746000: train loss 2.2849493, val loss 2.279858, mem 1.8 GiB @ 00 03:45:14.268, mean 00 00:00:00.018
step 746500: train loss 2.2771714, val loss 2.272195, mem 1.8 GiB @ 00 03:45:23.489, mean 00 00:00:00.018
step 747000: train loss 2.2713976, val loss 2.2745354, mem 1.8 GiB @ 00 03:45:32.694, mean 00 00:00:00.018
step 747500: train loss 2.2877634, val loss 2.2841635, mem 1.8 GiB @ 00 03:45:41.990, mean 00 00:00:00.018
step 748000: train loss 2.276159, val loss 2.2848625, mem 1.8 GiB @ 00 03:45:51.327, mean 00 00:00:00.018
step 748500: train loss 2.2772744, val loss 2.279282, mem 1.8 GiB @ 00 03:46:00.472, mean 00 00:00:00.018
step 749000: train loss 2.2655225, val loss 2.2646341, mem 1.8 GiB @ 00 03:46:09.811, mean 00 00:00:00.018
step 749500: train loss 2.2797596, val loss 2.281843, mem 1.8 GiB @ 00 03:46:18.986, mean 00 00:00:00.018
step 750000: train loss 2.2654428, val loss 2.2846377, mem 1.8 GiB @ 00 03:46:28.166, mean 00 00:00:00.018
step 750500: train loss 2.2645056, val loss 2.272044, mem 1.8 GiB @ 00 03:46:37.220, mean 00 00:00:00.018
step 751000: train loss 2.274614, val loss 2.2704537, mem 1.8 GiB @ 00 03:46:46.417, mean 00 00:00:00.018
step 751500: train loss 2.2721612, val loss 2.281265, mem 1.8 GiB @ 00 03:46:55.773, mean 00 00:00:00.018
step 752000: train loss 2.270774, val loss 2.262547, mem 1.8 GiB @ 00 03:47:04.946, mean 00 00:00:00.018
step 752500: train loss 2.269947, val loss 2.2754729, mem 1.8 GiB @ 00 03:47:14.122, mean 00 00:00:00.018
step 753000: train loss 2.2712798, val loss 2.2691274, mem 1.8 GiB @ 00 03:47:23.158, mean 00 00:00:00.018
step 753500: train loss 2.271963, val loss 2.280351, mem 1.8 GiB @ 00 03:47:32.247, mean 00 00:00:00.018
step 754000: train loss 2.2663126, val loss 2.2841818, mem 1.8 GiB @ 00 03:47:41.125, mean 00 00:00:00.017
step 754500: train loss 2.2719343, val loss 2.292506, mem 1.8 GiB @ 00 03:47:50.286, mean 00 00:00:00.018
step 755000: train loss 2.2694187, val loss 2.2811596, mem 1.8 GiB @ 00 03:47:59.588, mean 00 00:00:00.018
step 755500: train loss 2.276486, val loss 2.2813108, mem 1.8 GiB @ 00 03:48:08.764, mean 00 00:00:00.018
step 756000: train loss 2.2897606, val loss 2.2725146, mem 1.8 GiB @ 00 03:48:18.078, mean 00 00:00:00.018
step 756500: train loss 2.2771683, val loss 2.2869933, mem 1.8 GiB @ 00 03:48:26.959, mean 00 00:00:00.017
step 757000: train loss 2.2693853, val loss 2.2837682, mem 1.8 GiB @ 00 03:48:35.806, mean 00 00:00:00.017
step 757500: train loss 2.2726145, val loss 2.2731907, mem 1.8 GiB @ 00 03:48:45.061, mean 00 00:00:00.018
step 758000: train loss 2.2784917, val loss 2.2760386, mem 1.8 GiB @ 00 03:48:54.147, mean 00 00:00:00.018
step 758500: train loss 2.2771497, val loss 2.2616374, mem 1.8 GiB @ 00 03:49:03.322, mean 00 00:00:00.018
step 759000: train loss 2.265287, val loss 2.2745721, mem 1.8 GiB @ 00 03:49:12.348, mean 00 00:00:00.018
step 759500: train loss 2.270125, val loss 2.271908, mem 1.8 GiB @ 00 03:49:21.361, mean 00 00:00:00.018
step 760000: train loss 2.2677634, val loss 2.268547, mem 1.8 GiB @ 00 03:49:30.195, mean 00 00:00:00.017
step 760500: train loss 2.263109, val loss 2.276107, mem 1.8 GiB @ 00 03:49:39.250, mean 00 00:00:00.018
step 761000: train loss 2.274537, val loss 2.2884128, mem 1.8 GiB @ 00 03:49:48.148, mean 00 00:00:00.017
step 761500: train loss 2.2612472, val loss 2.28975, mem 1.8 GiB @ 00 03:49:56.908, mean 00 00:00:00.017
step 762000: train loss 2.269177, val loss 2.2798705, mem 1.8 GiB @ 00 03:50:05.882, mean 00 00:00:00.017
step 762500: train loss 2.2704477, val loss 2.2754211, mem 1.8 GiB @ 00 03:50:15.045, mean 00 00:00:00.018
step 763000: train loss 2.2664897, val loss 2.2622516, mem 1.8 GiB @ 00 03:50:24.213, mean 00 00:00:00.018
step 763500: train loss 2.2675166, val loss 2.2726767, mem 1.8 GiB @ 00 03:50:33.341, mean 00 00:00:00.018
step 764000: train loss 2.253797, val loss 2.2797506, mem 1.8 GiB @ 00 03:50:42.515, mean 00 00:00:00.018
step 764500: train loss 2.2543678, val loss 2.2799828, mem 1.8 GiB @ 00 03:50:51.679, mean 00 00:00:00.018
step 765000: train loss 2.2723315, val loss 2.2845416, mem 1.8 GiB @ 00 03:51:00.857, mean 00 00:00:00.018
step 765500: train loss 2.2738805, val loss 2.2778718, mem 1.8 GiB @ 00 03:51:10.006, mean 00 00:00:00.018
step 766000: train loss 2.2698054, val loss 2.2805912, mem 1.8 GiB @ 00 03:51:19.056, mean 00 00:00:00.018
step 766500: train loss 2.2782342, val loss 2.2694044, mem 1.8 GiB @ 00 03:51:28.187, mean 00 00:00:00.018
step 767000: train loss 2.2539225, val loss 2.2776327, mem 1.8 GiB @ 00 03:51:37.392, mean 00 00:00:00.018
step 767500: train loss 2.2559621, val loss 2.275125, mem 1.8 GiB @ 00 03:51:46.642, mean 00 00:00:00.018
step 768000: train loss 2.2656794, val loss 2.2832248, mem 1.8 GiB @ 00 03:51:56.108, mean 00 00:00:00.018
step 768500: train loss 2.2548637, val loss 2.266884, mem 1.8 GiB @ 00 03:52:05.278, mean 00 00:00:00.018
step 769000: train loss 2.2730832, val loss 2.2738352, mem 1.8 GiB @ 00 03:52:14.445, mean 00 00:00:00.018
step 769500: train loss 2.258777, val loss 2.265753, mem 1.8 GiB @ 00 03:52:23.674, mean 00 00:00:00.018
step 770000: train loss 2.2711082, val loss 2.2738736, mem 1.8 GiB @ 00 03:52:32.830, mean 00 00:00:00.018
step 770500: train loss 2.2717962, val loss 2.2644463, mem 1.8 GiB @ 00 03:52:42.027, mean 00 00:00:00.018
step 771000: train loss 2.2588818, val loss 2.2741215, mem 1.8 GiB @ 00 03:52:51.236, mean 00 00:00:00.018
step 771500: train loss 2.2595847, val loss 2.2838638, mem 1.8 GiB @ 00 03:53:00.409, mean 00 00:00:00.018
step 772000: train loss 2.2663329, val loss 2.2475514, mem 1.8 GiB @ 00 03:53:09.773, mean 00 00:00:00.018
step 772500: train loss 2.2686563, val loss 2.2722306, mem 1.8 GiB @ 00 03:53:18.905, mean 00 00:00:00.018
step 773000: train loss 2.2681715, val loss 2.2660003, mem 1.8 GiB @ 00 03:53:28.179, mean 00 00:00:00.018
step 773500: train loss 2.2607749, val loss 2.2808654, mem 1.8 GiB @ 00 03:53:37.348, mean 00 00:00:00.018
step 774000: train loss 2.2677312, val loss 2.2731903, mem 1.8 GiB @ 00 03:53:46.571, mean 00 00:00:00.018
step 774500: train loss 2.270269, val loss 2.2774978, mem 1.8 GiB @ 00 03:53:55.889, mean 00 00:00:00.018
step 775000: train loss 2.256171, val loss 2.2779896, mem 1.8 GiB @ 00 03:54:05.130, mean 00 00:00:00.018
step 775500: train loss 2.266305, val loss 2.275317, mem 1.8 GiB @ 00 03:54:14.153, mean 00 00:00:00.018
step 776000: train loss 2.2850509, val loss 2.2705505, mem 1.8 GiB @ 00 03:54:23.179, mean 00 00:00:00.018
step 776500: train loss 2.2680154, val loss 2.2743013, mem 1.8 GiB @ 00 03:54:31.913, mean 00 00:00:00.017
step 777000: train loss 2.265637, val loss 2.2610173, mem 1.8 GiB @ 00 03:54:41.094, mean 00 00:00:00.018
step 777500: train loss 2.2546008, val loss 2.2700722, mem 1.8 GiB @ 00 03:54:50.370, mean 00 00:00:00.018
step 778000: train loss 2.2596235, val loss 2.2733533, mem 1.8 GiB @ 00 03:54:59.375, mean 00 00:00:00.018
step 778500: train loss 2.25916, val loss 2.265778, mem 1.8 GiB @ 00 03:55:08.534, mean 00 00:00:00.018
step 779000: train loss 2.2596781, val loss 2.2797916, mem 1.8 GiB @ 00 03:55:17.332, mean 00 00:00:00.017
step 779500: train loss 2.264418, val loss 2.2731698, mem 1.8 GiB @ 00 03:55:26.036, mean 00 00:00:00.017
step 780000: train loss 2.2572834, val loss 2.280378, mem 1.8 GiB @ 00 03:55:34.882, mean 00 00:00:00.017
step 780500: train loss 2.262652, val loss 2.28351, mem 1.8 GiB @ 00 03:55:44.279, mean 00 00:00:00.018
step 781000: train loss 2.264645, val loss 2.2827494, mem 1.8 GiB @ 00 03:55:53.498, mean 00 00:00:00.018
step 781500: train loss 2.272588, val loss 2.2632658, mem 1.8 GiB @ 00 03:56:02.644, mean 00 00:00:00.018
step 782000: train loss 2.2745814, val loss 2.267678, mem 1.8 GiB @ 00 03:56:11.858, mean 00 00:00:00.018
step 782500: train loss 2.2697625, val loss 2.2611127, mem 1.8 GiB @ 00 03:56:21.053, mean 00 00:00:00.018
step 783000: train loss 2.2566314, val loss 2.2825265, mem 1.8 GiB @ 00 03:56:30.433, mean 00 00:00:00.018
step 783500: train loss 2.2654617, val loss 2.2641466, mem 1.8 GiB @ 00 03:56:39.735, mean 00 00:00:00.018
step 784000: train loss 2.2589793, val loss 2.271189, mem 1.8 GiB @ 00 03:56:48.890, mean 00 00:00:00.018
step 784500: train loss 2.272329, val loss 2.2717323, mem 1.8 GiB @ 00 03:56:57.986, mean 00 00:00:00.018
step 785000: train loss 2.26377, val loss 2.285863, mem 1.8 GiB @ 00 03:57:07.161, mean 00 00:00:00.018
step 785500: train loss 2.2537668, val loss 2.2623732, mem 1.8 GiB @ 00 03:57:16.335, mean 00 00:00:00.018
step 786000: train loss 2.2650406, val loss 2.271193, mem 1.8 GiB @ 00 03:57:25.516, mean 00 00:00:00.018
step 786500: train loss 2.257682, val loss 2.2651317, mem 1.8 GiB @ 00 03:57:34.675, mean 00 00:00:00.018
step 787000: train loss 2.2693696, val loss 2.2614977, mem 1.8 GiB @ 00 03:57:43.736, mean 00 00:00:00.018
step 787500: train loss 2.2569332, val loss 2.2835157, mem 1.8 GiB @ 00 03:57:53.036, mean 00 00:00:00.018
step 788000: train loss 2.2722719, val loss 2.2712944, mem 1.8 GiB @ 00 03:58:02.307, mean 00 00:00:00.018
step 788500: train loss 2.250539, val loss 2.2709343, mem 1.8 GiB @ 00 03:58:11.611, mean 00 00:00:00.018
step 789000: train loss 2.254615, val loss 2.2835813, mem 1.8 GiB @ 00 03:58:20.793, mean 00 00:00:00.018
step 789500: train loss 2.2484221, val loss 2.277223, mem 1.8 GiB @ 00 03:58:29.979, mean 00 00:00:00.018
step 790000: train loss 2.2823858, val loss 2.2516553, mem 1.8 GiB @ 00 03:58:39.216, mean 00 00:00:00.018
step 790500: train loss 2.2660606, val loss 2.2604473, mem 1.8 GiB @ 00 03:58:48.648, mean 00 00:00:00.018
step 791000: train loss 2.2599797, val loss 2.267194, mem 1.8 GiB @ 00 03:58:58.098, mean 00 00:00:00.018
step 791500: train loss 2.2566845, val loss 2.254516, mem 1.8 GiB @ 00 03:59:07.566, mean 00 00:00:00.018
step 792000: train loss 2.271807, val loss 2.2721725, mem 1.8 GiB @ 00 03:59:16.953, mean 00 00:00:00.018
step 792500: train loss 2.240728, val loss 2.2676554, mem 1.8 GiB @ 00 03:59:26.309, mean 00 00:00:00.018
step 793000: train loss 2.2701333, val loss 2.2725563, mem 1.8 GiB @ 00 03:59:35.493, mean 00 00:00:00.018
step 793500: train loss 2.2611818, val loss 2.2649438, mem 1.8 GiB @ 00 03:59:44.739, mean 00 00:00:00.018
step 794000: train loss 2.2669704, val loss 2.2673125, mem 1.8 GiB @ 00 03:59:54.054, mean 00 00:00:00.018
step 794500: train loss 2.2505233, val loss 2.2699735, mem 1.8 GiB @ 00 04:00:03.355, mean 00 00:00:00.018
step 795000: train loss 2.2670212, val loss 2.2743337, mem 1.8 GiB @ 00 04:00:12.385, mean 00 00:00:00.018
step 795500: train loss 2.2594805, val loss 2.2805312, mem 1.8 GiB @ 00 04:00:21.557, mean 00 00:00:00.018
step 796000: train loss 2.2637522, val loss 2.2662606, mem 1.8 GiB @ 00 04:00:30.699, mean 00 00:00:00.018
step 796500: train loss 2.2605262, val loss 2.2520766, mem 1.8 GiB @ 00 04:00:40.045, mean 00 00:00:00.018
step 797000: train loss 2.2617164, val loss 2.272452, mem 1.8 GiB @ 00 04:00:49.303, mean 00 00:00:00.018
step 797500: train loss 2.2704358, val loss 2.2776563, mem 1.8 GiB @ 00 04:00:58.703, mean 00 00:00:00.018
step 798000: train loss 2.2739692, val loss 2.2720668, mem 1.8 GiB @ 00 04:01:08.184, mean 00 00:00:00.018
step 798500: train loss 2.2547083, val loss 2.2767522, mem 1.8 GiB @ 00 04:01:17.184, mean 00 00:00:00.017
step 799000: train loss 2.2586133, val loss 2.263393, mem 1.8 GiB @ 00 04:01:26.106, mean 00 00:00:00.017
step 799500: train loss 2.2691176, val loss 2.262372, mem 1.8 GiB @ 00 04:01:35.429, mean 00 00:00:00.018
step 800000: train loss 2.2579806, val loss 2.272627, mem 1.8 GiB @ 00 04:01:44.759, mean 00 00:00:00.018
step 800500: train loss 2.260896, val loss 2.27365, mem 1.8 GiB @ 00 04:01:53.628, mean 00 00:00:00.017
step 801000: train loss 2.2724276, val loss 2.2703278, mem 1.8 GiB @ 00 04:02:02.588, mean 00 00:00:00.017
step 801500: train loss 2.259636, val loss 2.27173, mem 1.8 GiB @ 00 04:02:11.865, mean 00 00:00:00.018
step 802000: train loss 2.2498376, val loss 2.2748392, mem 1.8 GiB @ 00 04:02:21.099, mean 00 00:00:00.018
step 802500: train loss 2.2718647, val loss 2.2868962, mem 1.8 GiB @ 00 04:02:30.222, mean 00 00:00:00.018
step 803000: train loss 2.2519665, val loss 2.2616284, mem 1.8 GiB @ 00 04:02:39.556, mean 00 00:00:00.018
step 803500: train loss 2.258449, val loss 2.2731223, mem 1.8 GiB @ 00 04:02:48.752, mean 00 00:00:00.018
step 804000: train loss 2.2588272, val loss 2.2874625, mem 1.8 GiB @ 00 04:02:57.952, mean 00 00:00:00.018
step 804500: train loss 2.2663305, val loss 2.2723925, mem 1.8 GiB @ 00 04:03:07.122, mean 00 00:00:00.018
step 805000: train loss 2.2589731, val loss 2.2698345, mem 1.8 GiB @ 00 04:03:16.596, mean 00 00:00:00.018
step 805500: train loss 2.2546554, val loss 2.2688227, mem 1.8 GiB @ 00 04:03:25.798, mean 00 00:00:00.018
step 806000: train loss 2.2549472, val loss 2.2638092, mem 1.8 GiB @ 00 04:03:35.279, mean 00 00:00:00.018
step 806500: train loss 2.260287, val loss 2.2618225, mem 1.8 GiB @ 00 04:03:44.581, mean 00 00:00:00.018
step 807000: train loss 2.2561684, val loss 2.275528, mem 1.8 GiB @ 00 04:03:53.846, mean 00 00:00:00.018
step 807500: train loss 2.2463286, val loss 2.250335, mem 1.8 GiB @ 00 04:04:03.208, mean 00 00:00:00.018
step 808000: train loss 2.2713263, val loss 2.2470963, mem 1.8 GiB @ 00 04:04:12.360, mean 00 00:00:00.018
step 808500: train loss 2.2437515, val loss 2.2655692, mem 1.8 GiB @ 00 04:04:21.471, mean 00 00:00:00.018
step 809000: train loss 2.2649426, val loss 2.2700162, mem 1.8 GiB @ 00 04:04:31.038, mean 00 00:00:00.019
step 809500: train loss 2.2586303, val loss 2.259074, mem 1.8 GiB @ 00 04:04:39.877, mean 00 00:00:00.017
step 810000: train loss 2.2632892, val loss 2.2694445, mem 1.8 GiB @ 00 04:04:49.393, mean 00 00:00:00.019
step 810500: train loss 2.2622592, val loss 2.2651308, mem 1.8 GiB @ 00 04:04:58.472, mean 00 00:00:00.018
step 811000: train loss 2.2549088, val loss 2.2684174, mem 1.8 GiB @ 00 04:05:07.664, mean 00 00:00:00.018
step 811500: train loss 2.2672865, val loss 2.2716424, mem 1.8 GiB @ 00 04:05:16.807, mean 00 00:00:00.018
step 812000: train loss 2.2607095, val loss 2.2679136, mem 1.8 GiB @ 00 04:05:25.653, mean 00 00:00:00.017
step 812500: train loss 2.245868, val loss 2.2738576, mem 1.8 GiB @ 00 04:05:34.441, mean 00 00:00:00.017
step 813000: train loss 2.2547417, val loss 2.2485924, mem 1.8 GiB @ 00 04:05:43.419, mean 00 00:00:00.017
step 813500: train loss 2.2635982, val loss 2.2676277, mem 1.8 GiB @ 00 04:05:52.396, mean 00 00:00:00.017
step 814000: train loss 2.2627103, val loss 2.2710397, mem 1.8 GiB @ 00 04:06:02.026, mean 00 00:00:00.019
step 814500: train loss 2.2676177, val loss 2.2682319, mem 1.8 GiB @ 00 04:06:11.412, mean 00 00:00:00.018
step 815000: train loss 2.2666519, val loss 2.273413, mem 1.8 GiB @ 00 04:06:20.356, mean 00 00:00:00.017
step 815500: train loss 2.2534642, val loss 2.2743554, mem 1.8 GiB @ 00 04:06:29.289, mean 00 00:00:00.017
step 816000: train loss 2.251959, val loss 2.259123, mem 1.8 GiB @ 00 04:06:38.236, mean 00 00:00:00.017
step 816500: train loss 2.261342, val loss 2.2799158, mem 1.8 GiB @ 00 04:06:47.050, mean 00 00:00:00.017
step 817000: train loss 2.257827, val loss 2.2760472, mem 1.8 GiB @ 00 04:06:55.815, mean 00 00:00:00.017
step 817500: train loss 2.2758272, val loss 2.2697694, mem 1.8 GiB @ 00 04:07:04.596, mean 00 00:00:00.017
step 818000: train loss 2.249361, val loss 2.2653575, mem 1.8 GiB @ 00 04:07:13.864, mean 00 00:00:00.018
step 818500: train loss 2.2437913, val loss 2.2573614, mem 1.8 GiB @ 00 04:07:23.143, mean 00 00:00:00.018
step 819000: train loss 2.2602346, val loss 2.2723856, mem 1.8 GiB @ 00 04:07:32.209, mean 00 00:00:00.018
step 819500: train loss 2.2714639, val loss 2.270462, mem 1.8 GiB @ 00 04:07:41.302, mean 00 00:00:00.018
step 820000: train loss 2.2605884, val loss 2.2583623, mem 1.8 GiB @ 00 04:07:50.692, mean 00 00:00:00.018
step 820500: train loss 2.2572227, val loss 2.2750385, mem 1.8 GiB @ 00 04:08:00.205, mean 00 00:00:00.019
step 821000: train loss 2.2729461, val loss 2.2540326, mem 1.8 GiB @ 00 04:08:09.458, mean 00 00:00:00.018
step 821500: train loss 2.274848, val loss 2.264112, mem 1.8 GiB @ 00 04:08:18.608, mean 00 00:00:00.018
step 822000: train loss 2.2601323, val loss 2.260506, mem 1.8 GiB @ 00 04:08:27.733, mean 00 00:00:00.018
step 822500: train loss 2.256376, val loss 2.2578714, mem 1.8 GiB @ 00 04:08:36.793, mean 00 00:00:00.018
step 823000: train loss 2.2517996, val loss 2.2539852, mem 1.8 GiB @ 00 04:08:45.956, mean 00 00:00:00.018
step 823500: train loss 2.2583141, val loss 2.264933, mem 1.8 GiB @ 00 04:08:55.431, mean 00 00:00:00.018
step 824000: train loss 2.2641556, val loss 2.2714398, mem 1.8 GiB @ 00 04:09:04.759, mean 00 00:00:00.018
step 824500: train loss 2.2563686, val loss 2.264822, mem 1.8 GiB @ 00 04:09:14.142, mean 00 00:00:00.018
step 825000: train loss 2.2624724, val loss 2.2638466, mem 1.8 GiB @ 00 04:09:23.330, mean 00 00:00:00.018
step 825500: train loss 2.243822, val loss 2.2563016, mem 1.8 GiB @ 00 04:09:32.831, mean 00 00:00:00.019
step 826000: train loss 2.2610912, val loss 2.2542326, mem 1.8 GiB @ 00 04:09:42.164, mean 00 00:00:00.018
step 826500: train loss 2.2593534, val loss 2.2519417, mem 1.8 GiB @ 00 04:09:51.397, mean 00 00:00:00.018
step 827000: train loss 2.2498162, val loss 2.2564394, mem 1.8 GiB @ 00 04:10:00.572, mean 00 00:00:00.018
step 827500: train loss 2.2571223, val loss 2.2790062, mem 1.8 GiB @ 00 04:10:09.801, mean 00 00:00:00.018
step 828000: train loss 2.2709255, val loss 2.2457812, mem 1.8 GiB @ 00 04:10:18.979, mean 00 00:00:00.018
step 828500: train loss 2.2573993, val loss 2.2598553, mem 1.8 GiB @ 00 04:10:28.168, mean 00 00:00:00.018
step 829000: train loss 2.2476897, val loss 2.257078, mem 1.8 GiB @ 00 04:10:37.514, mean 00 00:00:00.018
step 829500: train loss 2.2454093, val loss 2.2774427, mem 1.8 GiB @ 00 04:10:46.580, mean 00 00:00:00.018
step 830000: train loss 2.2774723, val loss 2.2355099, mem 1.8 GiB @ 00 04:10:55.720, mean 00 00:00:00.018
step 830500: train loss 2.25029, val loss 2.2741523, mem 1.8 GiB @ 00 04:11:04.895, mean 00 00:00:00.018
step 831000: train loss 2.2563164, val loss 2.2762036, mem 1.8 GiB @ 00 04:11:14.075, mean 00 00:00:00.018
step 831500: train loss 2.2475095, val loss 2.2477617, mem 1.8 GiB @ 00 04:11:23.158, mean 00 00:00:00.018
step 832000: train loss 2.2516103, val loss 2.2616997, mem 1.8 GiB @ 00 04:11:32.359, mean 00 00:00:00.018
step 832500: train loss 2.2518551, val loss 2.2510972, mem 1.8 GiB @ 00 04:11:41.520, mean 00 00:00:00.018
step 833000: train loss 2.251166, val loss 2.269026, mem 1.8 GiB @ 00 04:11:50.698, mean 00 00:00:00.018
step 833500: train loss 2.2626033, val loss 2.258627, mem 1.8 GiB @ 00 04:11:59.827, mean 00 00:00:00.018
step 834000: train loss 2.2508445, val loss 2.2665768, mem 1.8 GiB @ 00 04:12:09.019, mean 00 00:00:00.018
step 834500: train loss 2.2648215, val loss 2.259098, mem 1.8 GiB @ 00 04:12:18.181, mean 00 00:00:00.018
step 835000: train loss 2.2499275, val loss 2.269526, mem 1.8 GiB @ 00 04:12:27.323, mean 00 00:00:00.018
step 835500: train loss 2.2459323, val loss 2.2631285, mem 1.8 GiB @ 00 04:12:36.478, mean 00 00:00:00.018
step 836000: train loss 2.2590268, val loss 2.2593138, mem 1.8 GiB @ 00 04:12:45.666, mean 00 00:00:00.018
step 836500: train loss 2.254138, val loss 2.2593865, mem 1.8 GiB @ 00 04:12:54.932, mean 00 00:00:00.018
step 837000: train loss 2.2487197, val loss 2.2636805, mem 1.8 GiB @ 00 04:13:04.132, mean 00 00:00:00.018
step 837500: train loss 2.2517428, val loss 2.2733176, mem 1.8 GiB @ 00 04:13:13.521, mean 00 00:00:00.018
step 838000: train loss 2.2530243, val loss 2.258835, mem 1.8 GiB @ 00 04:13:22.693, mean 00 00:00:00.018
step 838500: train loss 2.2663558, val loss 2.2499, mem 1.8 GiB @ 00 04:13:31.838, mean 00 00:00:00.018
step 839000: train loss 2.2404592, val loss 2.2539678, mem 1.8 GiB @ 00 04:13:41.102, mean 00 00:00:00.018
step 839500: train loss 2.2447686, val loss 2.2487488, mem 1.8 GiB @ 00 04:13:50.303, mean 00 00:00:00.018
step 840000: train loss 2.2543457, val loss 2.274129, mem 1.8 GiB @ 00 04:13:59.481, mean 00 00:00:00.018
step 840500: train loss 2.2550128, val loss 2.2378263, mem 1.8 GiB @ 00 04:14:08.728, mean 00 00:00:00.018
step 841000: train loss 2.2502391, val loss 2.2542052, mem 1.8 GiB @ 00 04:14:18.087, mean 00 00:00:00.018
step 841500: train loss 2.2705362, val loss 2.2555346, mem 1.8 GiB @ 00 04:14:27.259, mean 00 00:00:00.018
step 842000: train loss 2.2501972, val loss 2.239789, mem 1.8 GiB @ 00 04:14:36.444, mean 00 00:00:00.018
step 842500: train loss 2.2597811, val loss 2.261317, mem 1.8 GiB @ 00 04:14:45.424, mean 00 00:00:00.017
step 843000: train loss 2.2653635, val loss 2.2616174, mem 1.8 GiB @ 00 04:14:54.657, mean 00 00:00:00.018
step 843500: train loss 2.2547512, val loss 2.2655556, mem 1.8 GiB @ 00 04:15:03.994, mean 00 00:00:00.018
step 844000: train loss 2.2332504, val loss 2.261637, mem 1.8 GiB @ 00 04:15:13.310, mean 00 00:00:00.018
step 844500: train loss 2.2568185, val loss 2.2564545, mem 1.8 GiB @ 00 04:15:22.685, mean 00 00:00:00.018
step 845000: train loss 2.2551782, val loss 2.263372, mem 1.8 GiB @ 00 04:15:31.861, mean 00 00:00:00.018
step 845500: train loss 2.248754, val loss 2.2714036, mem 1.8 GiB @ 00 04:15:41.042, mean 00 00:00:00.018
step 846000: train loss 2.2478988, val loss 2.2530258, mem 1.8 GiB @ 00 04:15:50.289, mean 00 00:00:00.018
step 846500: train loss 2.2484763, val loss 2.25623, mem 1.8 GiB @ 00 04:15:59.634, mean 00 00:00:00.018
step 847000: train loss 2.2687857, val loss 2.2718377, mem 1.8 GiB @ 00 04:16:08.937, mean 00 00:00:00.018
step 847500: train loss 2.2425854, val loss 2.2600305, mem 1.8 GiB @ 00 04:16:18.172, mean 00 00:00:00.018
step 848000: train loss 2.2447298, val loss 2.2601779, mem 1.8 GiB @ 00 04:16:27.368, mean 00 00:00:00.018
step 848500: train loss 2.2394783, val loss 2.267582, mem 1.8 GiB @ 00 04:16:36.239, mean 00 00:00:00.017
step 849000: train loss 2.2436252, val loss 2.2575734, mem 1.8 GiB @ 00 04:16:45.390, mean 00 00:00:00.018
step 849500: train loss 2.2422998, val loss 2.2468512, mem 1.8 GiB @ 00 04:16:54.558, mean 00 00:00:00.018
step 850000: train loss 2.2553694, val loss 2.270891, mem 1.8 GiB @ 00 04:17:03.539, mean 00 00:00:00.017
step 850500: train loss 2.2447731, val loss 2.2634041, mem 1.8 GiB @ 00 04:17:12.535, mean 00 00:00:00.017
step 851000: train loss 2.255388, val loss 2.2426426, mem 1.8 GiB @ 00 04:17:21.757, mean 00 00:00:00.018
step 851500: train loss 2.2543256, val loss 2.258324, mem 1.8 GiB @ 00 04:17:31.091, mean 00 00:00:00.018
step 852000: train loss 2.2430542, val loss 2.2525766, mem 1.8 GiB @ 00 04:17:40.447, mean 00 00:00:00.018
step 852500: train loss 2.2402914, val loss 2.2551544, mem 1.8 GiB @ 00 04:17:49.671, mean 00 00:00:00.018
step 853000: train loss 2.2462125, val loss 2.2548072, mem 1.8 GiB @ 00 04:17:58.827, mean 00 00:00:00.018
step 853500: train loss 2.2453153, val loss 2.275142, mem 1.8 GiB @ 00 04:18:08.065, mean 00 00:00:00.018
step 854000: train loss 2.2477012, val loss 2.2672782, mem 1.8 GiB @ 00 04:18:17.265, mean 00 00:00:00.018
step 854500: train loss 2.2549531, val loss 2.2645657, mem 1.8 GiB @ 00 04:18:26.457, mean 00 00:00:00.018
step 855000: train loss 2.249816, val loss 2.2555492, mem 1.8 GiB @ 00 04:18:35.654, mean 00 00:00:00.018
step 855500: train loss 2.2493026, val loss 2.2574549, mem 1.8 GiB @ 00 04:18:45.097, mean 00 00:00:00.018
step 856000: train loss 2.2332876, val loss 2.2603195, mem 1.8 GiB @ 00 04:18:54.340, mean 00 00:00:00.018
step 856500: train loss 2.2442312, val loss 2.2601118, mem 1.8 GiB @ 00 04:19:03.606, mean 00 00:00:00.018
step 857000: train loss 2.252209, val loss 2.24338, mem 1.8 GiB @ 00 04:19:12.773, mean 00 00:00:00.018
step 857500: train loss 2.2413855, val loss 2.244789, mem 1.8 GiB @ 00 04:19:21.945, mean 00 00:00:00.018
step 858000: train loss 2.254917, val loss 2.262808, mem 1.8 GiB @ 00 04:19:31.511, mean 00 00:00:00.019
step 858500: train loss 2.2413678, val loss 2.2657928, mem 1.8 GiB @ 00 04:19:40.914, mean 00 00:00:00.018
step 859000: train loss 2.2588735, val loss 2.2609715, mem 1.8 GiB @ 00 04:19:50.255, mean 00 00:00:00.018
step 859500: train loss 2.2393477, val loss 2.2475023, mem 1.8 GiB @ 00 04:19:59.557, mean 00 00:00:00.018
step 860000: train loss 2.2453237, val loss 2.2567503, mem 1.8 GiB @ 00 04:20:08.967, mean 00 00:00:00.018
step 860500: train loss 2.2542064, val loss 2.2565496, mem 1.8 GiB @ 00 04:20:18.386, mean 00 00:00:00.018
step 861000: train loss 2.246204, val loss 2.251514, mem 1.8 GiB @ 00 04:20:27.761, mean 00 00:00:00.018
step 861500: train loss 2.2659295, val loss 2.2529893, mem 1.8 GiB @ 00 04:20:36.659, mean 00 00:00:00.017
step 862000: train loss 2.2494278, val loss 2.2425716, mem 1.8 GiB @ 00 04:20:45.650, mean 00 00:00:00.017
step 862500: train loss 2.247173, val loss 2.2487428, mem 1.8 GiB @ 00 04:20:54.824, mean 00 00:00:00.018
step 863000: train loss 2.2479753, val loss 2.2572608, mem 1.8 GiB @ 00 04:21:04.017, mean 00 00:00:00.018
step 863500: train loss 2.2468019, val loss 2.2579987, mem 1.8 GiB @ 00 04:21:13.436, mean 00 00:00:00.018
step 864000: train loss 2.2453146, val loss 2.2636137, mem 1.8 GiB @ 00 04:21:22.657, mean 00 00:00:00.018
step 864500: train loss 2.2414, val loss 2.2625601, mem 1.8 GiB @ 00 04:21:31.423, mean 00 00:00:00.017
step 865000: train loss 2.246368, val loss 2.245829, mem 1.8 GiB @ 00 04:21:40.186, mean 00 00:00:00.017
step 865500: train loss 2.2396443, val loss 2.2556229, mem 1.8 GiB @ 00 04:21:49.395, mean 00 00:00:00.018
step 866000: train loss 2.2612107, val loss 2.2530603, mem 1.8 GiB @ 00 04:21:58.430, mean 00 00:00:00.018
step 866500: train loss 2.253103, val loss 2.251686, mem 1.8 GiB @ 00 04:22:07.207, mean 00 00:00:00.017
step 867000: train loss 2.2482042, val loss 2.2587085, mem 1.8 GiB @ 00 04:22:16.390, mean 00 00:00:00.018
step 867500: train loss 2.2332685, val loss 2.256372, mem 1.8 GiB @ 00 04:22:25.642, mean 00 00:00:00.018
step 868000: train loss 2.2546082, val loss 2.2552538, mem 1.8 GiB @ 00 04:22:35.054, mean 00 00:00:00.018
step 868500: train loss 2.238796, val loss 2.2517824, mem 1.8 GiB @ 00 04:22:44.397, mean 00 00:00:00.018
step 869000: train loss 2.2418735, val loss 2.2584565, mem 1.8 GiB @ 00 04:22:53.796, mean 00 00:00:00.018
step 869500: train loss 2.2600493, val loss 2.2449899, mem 1.8 GiB @ 00 04:23:03.031, mean 00 00:00:00.018
step 870000: train loss 2.2403073, val loss 2.2481508, mem 1.8 GiB @ 00 04:23:12.300, mean 00 00:00:00.018
step 870500: train loss 2.250251, val loss 2.2424898, mem 1.8 GiB @ 00 04:23:21.574, mean 00 00:00:00.018
step 871000: train loss 2.2509096, val loss 2.2461362, mem 1.8 GiB @ 00 04:23:30.956, mean 00 00:00:00.018
step 871500: train loss 2.2496538, val loss 2.2549698, mem 1.8 GiB @ 00 04:23:40.322, mean 00 00:00:00.018
step 872000: train loss 2.2567813, val loss 2.2520308, mem 1.8 GiB @ 00 04:23:49.489, mean 00 00:00:00.018
step 872500: train loss 2.252541, val loss 2.257628, mem 1.8 GiB @ 00 04:23:58.648, mean 00 00:00:00.018
step 873000: train loss 2.2536356, val loss 2.2463543, mem 1.8 GiB @ 00 04:24:07.824, mean 00 00:00:00.018
step 873500: train loss 2.2489777, val loss 2.2536669, mem 1.8 GiB @ 00 04:24:17.008, mean 00 00:00:00.018
step 874000: train loss 2.2518604, val loss 2.2542074, mem 1.8 GiB @ 00 04:24:26.163, mean 00 00:00:00.018
step 874500: train loss 2.2320533, val loss 2.2532053, mem 1.8 GiB @ 00 04:24:35.344, mean 00 00:00:00.018
step 875000: train loss 2.2435207, val loss 2.2460473, mem 1.8 GiB @ 00 04:24:44.562, mean 00 00:00:00.018
step 875500: train loss 2.220878, val loss 2.2438507, mem 1.8 GiB @ 00 04:24:53.849, mean 00 00:00:00.018
step 876000: train loss 2.239837, val loss 2.2481575, mem 1.8 GiB @ 00 04:25:03.207, mean 00 00:00:00.018
step 876500: train loss 2.2347174, val loss 2.2556188, mem 1.8 GiB @ 00 04:25:12.415, mean 00 00:00:00.018
step 877000: train loss 2.2331192, val loss 2.2363448, mem 1.8 GiB @ 00 04:25:21.584, mean 00 00:00:00.018
step 877500: train loss 2.251802, val loss 2.252009, mem 1.8 GiB @ 00 04:25:31.067, mean 00 00:00:00.018
step 878000: train loss 2.2421134, val loss 2.2524765, mem 1.8 GiB @ 00 04:25:40.385, mean 00 00:00:00.018
step 878500: train loss 2.2555754, val loss 2.2644415, mem 1.8 GiB @ 00 04:25:49.741, mean 00 00:00:00.018
step 879000: train loss 2.2397938, val loss 2.2377994, mem 1.8 GiB @ 00 04:25:59.188, mean 00 00:00:00.018
step 879500: train loss 2.2304294, val loss 2.247871, mem 1.8 GiB @ 00 04:26:08.482, mean 00 00:00:00.018
step 880000: train loss 2.2507303, val loss 2.2687926, mem 1.8 GiB @ 00 04:26:17.954, mean 00 00:00:00.018
step 880500: train loss 2.244281, val loss 2.2567918, mem 1.8 GiB @ 00 04:26:27.489, mean 00 00:00:00.019
step 881000: train loss 2.2277315, val loss 2.2465215, mem 1.8 GiB @ 00 04:26:36.793, mean 00 00:00:00.018
step 881500: train loss 2.2426233, val loss 2.2578244, mem 1.8 GiB @ 00 04:26:45.883, mean 00 00:00:00.018
step 882000: train loss 2.2319734, val loss 2.2358854, mem 1.8 GiB @ 00 04:26:55.078, mean 00 00:00:00.018
step 882500: train loss 2.2346785, val loss 2.2632113, mem 1.8 GiB @ 00 04:27:04.258, mean 00 00:00:00.018
step 883000: train loss 2.2518094, val loss 2.2501717, mem 1.8 GiB @ 00 04:27:13.458, mean 00 00:00:00.018
step 883500: train loss 2.249717, val loss 2.251826, mem 1.8 GiB @ 00 04:27:22.626, mean 00 00:00:00.018
step 884000: train loss 2.2519104, val loss 2.2505379, mem 1.8 GiB @ 00 04:27:31.779, mean 00 00:00:00.018
step 884500: train loss 2.2554505, val loss 2.260728, mem 1.8 GiB @ 00 04:27:40.830, mean 00 00:00:00.018
step 885000: train loss 2.2505383, val loss 2.250553, mem 1.8 GiB @ 00 04:27:50.002, mean 00 00:00:00.018
step 885500: train loss 2.2422178, val loss 2.2444506, mem 1.8 GiB @ 00 04:27:59.238, mean 00 00:00:00.018
step 886000: train loss 2.25808, val loss 2.267284, mem 1.8 GiB @ 00 04:28:08.607, mean 00 00:00:00.018
step 886500: train loss 2.2324777, val loss 2.261499, mem 1.8 GiB @ 00 04:28:17.806, mean 00 00:00:00.018
step 887000: train loss 2.2334914, val loss 2.2382152, mem 1.8 GiB @ 00 04:28:26.879, mean 00 00:00:00.018
step 887500: train loss 2.238273, val loss 2.2559175, mem 1.8 GiB @ 00 04:28:35.720, mean 00 00:00:00.017
step 888000: train loss 2.239764, val loss 2.2523947, mem 1.8 GiB @ 00 04:28:44.634, mean 00 00:00:00.017
step 888500: train loss 2.245789, val loss 2.257073, mem 1.8 GiB @ 00 04:28:53.548, mean 00 00:00:00.017
step 889000: train loss 2.2440355, val loss 2.2617953, mem 1.8 GiB @ 00 04:29:02.406, mean 00 00:00:00.017
step 889500: train loss 2.2451437, val loss 2.2459908, mem 1.8 GiB @ 00 04:29:11.218, mean 00 00:00:00.017
step 890000: train loss 2.2446358, val loss 2.2469945, mem 1.8 GiB @ 00 04:29:20.210, mean 00 00:00:00.017
step 890500: train loss 2.231224, val loss 2.2547402, mem 1.8 GiB @ 00 04:29:29.348, mean 00 00:00:00.018
step 891000: train loss 2.2285373, val loss 2.254766, mem 1.8 GiB @ 00 04:29:38.302, mean 00 00:00:00.017
step 891500: train loss 2.2477355, val loss 2.261395, mem 1.8 GiB @ 00 04:29:47.342, mean 00 00:00:00.018
step 892000: train loss 2.245404, val loss 2.2604854, mem 1.8 GiB @ 00 04:29:56.142, mean 00 00:00:00.017
step 892500: train loss 2.2439535, val loss 2.2563639, mem 1.8 GiB @ 00 04:30:04.918, mean 00 00:00:00.017
step 893000: train loss 2.2424095, val loss 2.2375934, mem 1.8 GiB @ 00 04:30:13.752, mean 00 00:00:00.017
step 893500: train loss 2.227346, val loss 2.2468626, mem 1.8 GiB @ 00 04:30:22.488, mean 00 00:00:00.017
step 894000: train loss 2.2461178, val loss 2.2529984, mem 1.8 GiB @ 00 04:30:31.530, mean 00 00:00:00.018
step 894500: train loss 2.2475193, val loss 2.262203, mem 1.8 GiB @ 00 04:30:40.308, mean 00 00:00:00.017
step 895000: train loss 2.2422082, val loss 2.2603934, mem 1.8 GiB @ 00 04:30:49.086, mean 00 00:00:00.017
step 895500: train loss 2.240865, val loss 2.2599273, mem 1.8 GiB @ 00 04:30:58.365, mean 00 00:00:00.018
step 896000: train loss 2.231234, val loss 2.2473233, mem 1.8 GiB @ 00 04:31:07.688, mean 00 00:00:00.018
step 896500: train loss 2.2451138, val loss 2.261338, mem 1.8 GiB @ 00 04:31:16.966, mean 00 00:00:00.018
step 897000: train loss 2.2581544, val loss 2.249606, mem 1.8 GiB @ 00 04:31:26.123, mean 00 00:00:00.018
step 897500: train loss 2.2387106, val loss 2.2546866, mem 1.8 GiB @ 00 04:31:35.192, mean 00 00:00:00.018
step 898000: train loss 2.2282033, val loss 2.2349796, mem 1.8 GiB @ 00 04:31:44.391, mean 00 00:00:00.018
step 898500: train loss 2.2495155, val loss 2.2475457, mem 1.8 GiB @ 00 04:31:53.565, mean 00 00:00:00.018
step 899000: train loss 2.2419505, val loss 2.254647, mem 1.8 GiB @ 00 04:32:02.733, mean 00 00:00:00.018
step 899500: train loss 2.243901, val loss 2.2521186, mem 1.8 GiB @ 00 04:32:11.899, mean 00 00:00:00.018
step 900000: train loss 2.232237, val loss 2.2484407, mem 1.8 GiB @ 00 04:32:21.038, mean 00 00:00:00.018
step 900500: train loss 2.235441, val loss 2.2341516, mem 1.8 GiB @ 00 04:32:30.050, mean 00 00:00:00.018
step 901000: train loss 2.2426603, val loss 2.256108, mem 1.8 GiB @ 00 04:32:39.108, mean 00 00:00:00.018
step 901500: train loss 2.2259796, val loss 2.239808, mem 1.8 GiB @ 00 04:32:48.320, mean 00 00:00:00.018
step 902000: train loss 2.250595, val loss 2.2483711, mem 1.8 GiB @ 00 04:32:57.690, mean 00 00:00:00.018
step 902500: train loss 2.2286785, val loss 2.244027, mem 1.8 GiB @ 00 04:33:06.782, mean 00 00:00:00.018
step 903000: train loss 2.2410064, val loss 2.258069, mem 1.8 GiB @ 00 04:33:15.951, mean 00 00:00:00.018
step 903500: train loss 2.2390676, val loss 2.231143, mem 1.8 GiB @ 00 04:33:25.223, mean 00 00:00:00.018
step 904000: train loss 2.22837, val loss 2.2656796, mem 1.8 GiB @ 00 04:33:34.467, mean 00 00:00:00.018
step 904500: train loss 2.2297902, val loss 2.2491207, mem 1.8 GiB @ 00 04:33:43.481, mean 00 00:00:00.018
step 905000: train loss 2.2355444, val loss 2.2532437, mem 1.8 GiB @ 00 04:33:52.191, mean 00 00:00:00.017
step 905500: train loss 2.2264853, val loss 2.2591155, mem 1.8 GiB @ 00 04:34:01.121, mean 00 00:00:00.017
step 906000: train loss 2.2436836, val loss 2.2395058, mem 1.8 GiB @ 00 04:34:10.271, mean 00 00:00:00.018
step 906500: train loss 2.2479496, val loss 2.2468793, mem 1.8 GiB @ 00 04:34:19.256, mean 00 00:00:00.017
step 907000: train loss 2.2446795, val loss 2.246893, mem 1.8 GiB @ 00 04:34:28.016, mean 00 00:00:00.017
step 907500: train loss 2.240365, val loss 2.2501416, mem 1.8 GiB @ 00 04:34:36.759, mean 00 00:00:00.017
step 908000: train loss 2.2350338, val loss 2.2511487, mem 1.8 GiB @ 00 04:34:45.863, mean 00 00:00:00.018
step 908500: train loss 2.2277744, val loss 2.2506413, mem 1.8 GiB @ 00 04:34:55.082, mean 00 00:00:00.018
step 909000: train loss 2.2382085, val loss 2.2367368, mem 1.8 GiB @ 00 04:35:04.251, mean 00 00:00:00.018
step 909500: train loss 2.2430854, val loss 2.2410119, mem 1.8 GiB @ 00 04:35:13.428, mean 00 00:00:00.018
step 910000: train loss 2.2398949, val loss 2.2379594, mem 1.8 GiB @ 00 04:35:22.581, mean 00 00:00:00.018
step 910500: train loss 2.2327561, val loss 2.2503977, mem 1.8 GiB @ 00 04:35:31.621, mean 00 00:00:00.018
step 911000: train loss 2.24023, val loss 2.2593367, mem 1.8 GiB @ 00 04:35:40.775, mean 00 00:00:00.018
step 911500: train loss 2.243674, val loss 2.2451925, mem 1.8 GiB @ 00 04:35:49.961, mean 00 00:00:00.018
step 912000: train loss 2.235088, val loss 2.2643561, mem 1.8 GiB @ 00 04:35:59.135, mean 00 00:00:00.018
step 912500: train loss 2.2409215, val loss 2.2439601, mem 1.8 GiB @ 00 04:36:08.212, mean 00 00:00:00.018
step 913000: train loss 2.2358358, val loss 2.2391157, mem 1.8 GiB @ 00 04:36:17.405, mean 00 00:00:00.018
step 913500: train loss 2.2451577, val loss 2.2443788, mem 1.8 GiB @ 00 04:36:26.458, mean 00 00:00:00.018
step 914000: train loss 2.237771, val loss 2.252447, mem 1.8 GiB @ 00 04:36:35.596, mean 00 00:00:00.018
step 914500: train loss 2.2403836, val loss 2.2526464, mem 1.8 GiB @ 00 04:36:44.736, mean 00 00:00:00.018
step 915000: train loss 2.2378147, val loss 2.2498941, mem 1.8 GiB @ 00 04:36:53.943, mean 00 00:00:00.018
step 915500: train loss 2.2377064, val loss 2.2443304, mem 1.8 GiB @ 00 04:37:02.697, mean 00 00:00:00.017
step 916000: train loss 2.231022, val loss 2.2305543, mem 1.8 GiB @ 00 04:37:11.482, mean 00 00:00:00.017
step 916500: train loss 2.2384915, val loss 2.2418804, mem 1.8 GiB @ 00 04:37:20.531, mean 00 00:00:00.018
step 917000: train loss 2.2406428, val loss 2.2527444, mem 1.8 GiB @ 00 04:37:29.713, mean 00 00:00:00.018
step 917500: train loss 2.2389808, val loss 2.2505786, mem 1.8 GiB @ 00 04:37:38.906, mean 00 00:00:00.018
step 918000: train loss 2.2369769, val loss 2.239864, mem 1.8 GiB @ 00 04:37:48.006, mean 00 00:00:00.018
step 918500: train loss 2.235453, val loss 2.2471187, mem 1.8 GiB @ 00 04:37:57.092, mean 00 00:00:00.018
step 919000: train loss 2.2312808, val loss 2.237793, mem 1.8 GiB @ 00 04:38:05.841, mean 00 00:00:00.017
step 919500: train loss 2.2349298, val loss 2.2634773, mem 1.8 GiB @ 00 04:38:14.553, mean 00 00:00:00.017
step 920000: train loss 2.2425153, val loss 2.2556734, mem 1.8 GiB @ 00 04:38:23.519, mean 00 00:00:00.017
step 920500: train loss 2.2436292, val loss 2.2515683, mem 1.8 GiB @ 00 04:38:32.357, mean 00 00:00:00.017
step 921000: train loss 2.2321813, val loss 2.253478, mem 1.8 GiB @ 00 04:38:41.387, mean 00 00:00:00.018
step 921500: train loss 2.2309332, val loss 2.2466738, mem 1.8 GiB @ 00 04:38:50.129, mean 00 00:00:00.017
step 922000: train loss 2.238431, val loss 2.2465973, mem 1.8 GiB @ 00 04:38:59.001, mean 00 00:00:00.017
step 922500: train loss 2.2376165, val loss 2.2475154, mem 1.8 GiB @ 00 04:39:08.333, mean 00 00:00:00.018
step 923000: train loss 2.2295177, val loss 2.247876, mem 1.8 GiB @ 00 04:39:17.002, mean 00 00:00:00.017
step 923500: train loss 2.2404532, val loss 2.2339292, mem 1.8 GiB @ 00 04:39:26.064, mean 00 00:00:00.018
step 924000: train loss 2.2289374, val loss 2.2431684, mem 1.8 GiB @ 00 04:39:35.239, mean 00 00:00:00.018
step 924500: train loss 2.2246664, val loss 2.2507198, mem 1.8 GiB @ 00 04:39:44.423, mean 00 00:00:00.018
step 925000: train loss 2.231667, val loss 2.2455802, mem 1.8 GiB @ 00 04:39:53.458, mean 00 00:00:00.018
step 925500: train loss 2.2248392, val loss 2.2449472, mem 1.8 GiB @ 00 04:40:02.211, mean 00 00:00:00.017
step 926000: train loss 2.2453234, val loss 2.2566974, mem 1.8 GiB @ 00 04:40:11.041, mean 00 00:00:00.017
step 926500: train loss 2.248863, val loss 2.2556875, mem 1.8 GiB @ 00 04:40:19.972, mean 00 00:00:00.017
step 927000: train loss 2.2526772, val loss 2.2531166, mem 1.8 GiB @ 00 04:40:28.994, mean 00 00:00:00.018
step 927500: train loss 2.236091, val loss 2.2520638, mem 1.8 GiB @ 00 04:40:38.110, mean 00 00:00:00.018
step 928000: train loss 2.2427285, val loss 2.2559812, mem 1.8 GiB @ 00 04:40:46.900, mean 00 00:00:00.017
step 928500: train loss 2.2220476, val loss 2.248759, mem 1.8 GiB @ 00 04:40:55.923, mean 00 00:00:00.018
step 929000: train loss 2.227358, val loss 2.248851, mem 1.8 GiB @ 00 04:41:05.070, mean 00 00:00:00.018
step 929500: train loss 2.239014, val loss 2.2435753, mem 1.8 GiB @ 00 04:41:14.195, mean 00 00:00:00.018
step 930000: train loss 2.2356915, val loss 2.2265258, mem 1.8 GiB @ 00 04:41:23.336, mean 00 00:00:00.018
step 930500: train loss 2.250916, val loss 2.234321, mem 1.8 GiB @ 00 04:41:32.476, mean 00 00:00:00.018
step 931000: train loss 2.2439988, val loss 2.2272708, mem 1.8 GiB @ 00 04:41:41.643, mean 00 00:00:00.018
step 931500: train loss 2.2503755, val loss 2.253602, mem 1.8 GiB @ 00 04:41:50.846, mean 00 00:00:00.018
step 932000: train loss 2.227519, val loss 2.249965, mem 1.8 GiB @ 00 04:42:00.200, mean 00 00:00:00.018
step 932500: train loss 2.2241075, val loss 2.2384076, mem 1.8 GiB @ 00 04:42:09.517, mean 00 00:00:00.018
step 933000: train loss 2.2356207, val loss 2.2631748, mem 1.8 GiB @ 00 04:42:18.358, mean 00 00:00:00.017
step 933500: train loss 2.2370322, val loss 2.2323074, mem 1.8 GiB @ 00 04:42:27.229, mean 00 00:00:00.017
step 934000: train loss 2.2228382, val loss 2.23537, mem 1.8 GiB @ 00 04:42:36.179, mean 00 00:00:00.017
step 934500: train loss 2.2340004, val loss 2.2554548, mem 1.8 GiB @ 00 04:42:45.206, mean 00 00:00:00.018
step 935000: train loss 2.2533464, val loss 2.251049, mem 1.8 GiB @ 00 04:42:53.921, mean 00 00:00:00.017
step 935500: train loss 2.2289774, val loss 2.2529118, mem 1.8 GiB @ 00 04:43:03.285, mean 00 00:00:00.018
step 936000: train loss 2.2364633, val loss 2.2599797, mem 1.8 GiB @ 00 04:43:12.450, mean 00 00:00:00.018
step 936500: train loss 2.2342284, val loss 2.243835, mem 1.8 GiB @ 00 04:43:21.429, mean 00 00:00:00.017
step 937000: train loss 2.2376578, val loss 2.2376287, mem 1.8 GiB @ 00 04:43:30.139, mean 00 00:00:00.017
step 937500: train loss 2.2224233, val loss 2.238399, mem 1.8 GiB @ 00 04:43:39.104, mean 00 00:00:00.017
step 938000: train loss 2.2384632, val loss 2.2571394, mem 1.8 GiB @ 00 04:43:48.095, mean 00 00:00:00.017
step 938500: train loss 2.2323256, val loss 2.2438707, mem 1.8 GiB @ 00 04:43:56.911, mean 00 00:00:00.017
step 939000: train loss 2.229891, val loss 2.2383568, mem 1.8 GiB @ 00 04:44:05.642, mean 00 00:00:00.017
step 939500: train loss 2.234338, val loss 2.2316072, mem 1.8 GiB @ 00 04:44:14.444, mean 00 00:00:00.017
step 940000: train loss 2.2332208, val loss 2.2357392, mem 1.8 GiB @ 00 04:44:23.296, mean 00 00:00:00.017
step 940500: train loss 2.2305071, val loss 2.2309997, mem 1.8 GiB @ 00 04:44:32.462, mean 00 00:00:00.018
step 941000: train loss 2.2406173, val loss 2.2343853, mem 1.8 GiB @ 00 04:44:41.466, mean 00 00:00:00.018
step 941500: train loss 2.2388246, val loss 2.255288, mem 1.8 GiB @ 00 04:44:50.300, mean 00 00:00:00.017
step 942000: train loss 2.2243311, val loss 2.2554162, mem 1.8 GiB @ 00 04:44:59.149, mean 00 00:00:00.017
step 942500: train loss 2.2456515, val loss 2.2193468, mem 1.8 GiB @ 00 04:45:07.911, mean 00 00:00:00.017
step 943000: train loss 2.2346861, val loss 2.2425177, mem 1.8 GiB @ 00 04:45:16.800, mean 00 00:00:00.017
step 943500: train loss 2.236087, val loss 2.2528222, mem 1.8 GiB @ 00 04:45:25.862, mean 00 00:00:00.018
step 944000: train loss 2.2317967, val loss 2.2378232, mem 1.8 GiB @ 00 04:45:35.018, mean 00 00:00:00.018
step 944500: train loss 2.240448, val loss 2.2398756, mem 1.8 GiB @ 00 04:45:44.063, mean 00 00:00:00.018
step 945000: train loss 2.2255766, val loss 2.2405062, mem 1.8 GiB @ 00 04:45:52.810, mean 00 00:00:00.017
step 945500: train loss 2.224644, val loss 2.2435098, mem 1.8 GiB @ 00 04:46:01.618, mean 00 00:00:00.017
step 946000: train loss 2.2368436, val loss 2.2446892, mem 1.8 GiB @ 00 04:46:10.355, mean 00 00:00:00.017
step 946500: train loss 2.2182724, val loss 2.230951, mem 1.8 GiB @ 00 04:46:19.167, mean 00 00:00:00.017
step 947000: train loss 2.2385268, val loss 2.2425623, mem 1.8 GiB @ 00 04:46:27.959, mean 00 00:00:00.017
step 947500: train loss 2.2319567, val loss 2.2387006, mem 1.8 GiB @ 00 04:46:36.751, mean 00 00:00:00.017
step 948000: train loss 2.222622, val loss 2.2424872, mem 1.8 GiB @ 00 04:46:45.558, mean 00 00:00:00.017
step 948500: train loss 2.2342653, val loss 2.2428315, mem 1.8 GiB @ 00 04:46:54.312, mean 00 00:00:00.017
step 949000: train loss 2.2554767, val loss 2.2260509, mem 1.8 GiB @ 00 04:47:03.488, mean 00 00:00:00.018
step 949500: train loss 2.243699, val loss 2.2453241, mem 1.8 GiB @ 00 04:47:12.648, mean 00 00:00:00.018
step 950000: train loss 2.2244651, val loss 2.2353415, mem 1.8 GiB @ 00 04:47:21.752, mean 00 00:00:00.018
step 950500: train loss 2.2247298, val loss 2.2403553, mem 1.8 GiB @ 00 04:47:30.918, mean 00 00:00:00.018
step 951000: train loss 2.2207913, val loss 2.2464716, mem 1.8 GiB @ 00 04:47:40.083, mean 00 00:00:00.018
step 951500: train loss 2.2402906, val loss 2.2418551, mem 1.8 GiB @ 00 04:47:49.253, mean 00 00:00:00.018
step 952000: train loss 2.2212765, val loss 2.2314887, mem 1.8 GiB @ 00 04:47:58.524, mean 00 00:00:00.018
step 952500: train loss 2.2305052, val loss 2.2500048, mem 1.8 GiB @ 00 04:48:07.675, mean 00 00:00:00.018
step 953000: train loss 2.2130184, val loss 2.251694, mem 1.8 GiB @ 00 04:48:16.928, mean 00 00:00:00.018
step 953500: train loss 2.225258, val loss 2.253747, mem 1.8 GiB @ 00 04:48:26.210, mean 00 00:00:00.018
step 954000: train loss 2.2295294, val loss 2.2379124, mem 1.8 GiB @ 00 04:48:35.385, mean 00 00:00:00.018
step 954500: train loss 2.2332485, val loss 2.2378325, mem 1.8 GiB @ 00 04:48:44.614, mean 00 00:00:00.018
step 955000: train loss 2.2414808, val loss 2.248539, mem 1.8 GiB @ 00 04:48:53.776, mean 00 00:00:00.018
step 955500: train loss 2.2307951, val loss 2.2411585, mem 1.8 GiB @ 00 04:49:02.950, mean 00 00:00:00.018
step 956000: train loss 2.2270815, val loss 2.2351716, mem 1.8 GiB @ 00 04:49:12.148, mean 00 00:00:00.018
step 956500: train loss 2.241546, val loss 2.226492, mem 1.8 GiB @ 00 04:49:21.254, mean 00 00:00:00.018
step 957000: train loss 2.2424023, val loss 2.2298076, mem 1.8 GiB @ 00 04:49:30.395, mean 00 00:00:00.018
step 957500: train loss 2.2315784, val loss 2.2309346, mem 1.8 GiB @ 00 04:49:39.547, mean 00 00:00:00.018
step 958000: train loss 2.2299352, val loss 2.2342823, mem 1.8 GiB @ 00 04:49:48.698, mean 00 00:00:00.018
step 958500: train loss 2.2541926, val loss 2.2388642, mem 1.8 GiB @ 00 04:49:57.825, mean 00 00:00:00.018
step 959000: train loss 2.2312224, val loss 2.2420728, mem 1.8 GiB @ 00 04:50:06.978, mean 00 00:00:00.018
step 959500: train loss 2.2378225, val loss 2.2388687, mem 1.8 GiB @ 00 04:50:16.227, mean 00 00:00:00.018
step 960000: train loss 2.2416286, val loss 2.2363744, mem 1.8 GiB @ 00 04:50:25.383, mean 00 00:00:00.018
step 960500: train loss 2.239956, val loss 2.2312944, mem 1.8 GiB @ 00 04:50:34.520, mean 00 00:00:00.018
step 961000: train loss 2.2260795, val loss 2.2364104, mem 1.8 GiB @ 00 04:50:43.774, mean 00 00:00:00.018
step 961500: train loss 2.2219646, val loss 2.2542944, mem 1.8 GiB @ 00 04:50:52.602, mean 00 00:00:00.017
step 962000: train loss 2.2324526, val loss 2.2196503, mem 1.8 GiB @ 00 04:51:01.408, mean 00 00:00:00.017
step 962500: train loss 2.2311907, val loss 2.2645001, mem 1.8 GiB @ 00 04:51:10.261, mean 00 00:00:00.017
step 963000: train loss 2.2279685, val loss 2.2369034, mem 1.8 GiB @ 00 04:51:19.060, mean 00 00:00:00.017
step 963500: train loss 2.2212832, val loss 2.2392309, mem 1.8 GiB @ 00 04:51:27.824, mean 00 00:00:00.017
step 964000: train loss 2.2276812, val loss 2.242285, mem 1.8 GiB @ 00 04:51:36.667, mean 00 00:00:00.017
step 964500: train loss 2.2444713, val loss 2.2442641, mem 1.8 GiB @ 00 04:51:45.553, mean 00 00:00:00.017
step 965000: train loss 2.2318904, val loss 2.2337158, mem 1.8 GiB @ 00 04:51:54.453, mean 00 00:00:00.017
step 965500: train loss 2.2255018, val loss 2.24611, mem 1.8 GiB @ 00 04:52:03.616, mean 00 00:00:00.018
step 966000: train loss 2.2128274, val loss 2.2299232, mem 1.8 GiB @ 00 04:52:12.789, mean 00 00:00:00.018
step 966500: train loss 2.2132204, val loss 2.2404335, mem 1.8 GiB @ 00 04:52:22.039, mean 00 00:00:00.018
step 967000: train loss 2.2437074, val loss 2.213279, mem 1.8 GiB @ 00 04:52:31.409, mean 00 00:00:00.018
step 967500: train loss 2.225126, val loss 2.2588074, mem 1.8 GiB @ 00 04:52:40.640, mean 00 00:00:00.018
step 968000: train loss 2.205852, val loss 2.238251, mem 1.8 GiB @ 00 04:52:49.886, mean 00 00:00:00.018
step 968500: train loss 2.2212455, val loss 2.2370038, mem 1.8 GiB @ 00 04:52:59.169, mean 00 00:00:00.018
step 969000: train loss 2.2443533, val loss 2.2473233, mem 1.8 GiB @ 00 04:53:08.387, mean 00 00:00:00.018
step 969500: train loss 2.2186065, val loss 2.2340744, mem 1.8 GiB @ 00 04:53:17.110, mean 00 00:00:00.017
step 970000: train loss 2.2368343, val loss 2.2173803, mem 1.8 GiB @ 00 04:53:26.275, mean 00 00:00:00.018
step 970500: train loss 2.2223542, val loss 2.2440789, mem 1.8 GiB @ 00 04:53:35.389, mean 00 00:00:00.018
step 971000: train loss 2.2137792, val loss 2.2378654, mem 1.8 GiB @ 00 04:53:44.315, mean 00 00:00:00.017
step 971500: train loss 2.2330115, val loss 2.2418184, mem 1.8 GiB @ 00 04:53:53.212, mean 00 00:00:00.017
step 972000: train loss 2.2348232, val loss 2.232124, mem 1.8 GiB @ 00 04:54:02.221, mean 00 00:00:00.018
step 972500: train loss 2.226405, val loss 2.2445376, mem 1.8 GiB @ 00 04:54:11.449, mean 00 00:00:00.018
step 973000: train loss 2.2218761, val loss 2.2235773, mem 1.8 GiB @ 00 04:54:20.708, mean 00 00:00:00.018
step 973500: train loss 2.2196622, val loss 2.2389433, mem 1.8 GiB @ 00 04:54:29.789, mean 00 00:00:00.018
step 974000: train loss 2.231664, val loss 2.2572489, mem 1.8 GiB @ 00 04:54:38.752, mean 00 00:00:00.017
step 974500: train loss 2.2281525, val loss 2.2238393, mem 1.8 GiB @ 00 04:54:47.960, mean 00 00:00:00.018
step 975000: train loss 2.2273285, val loss 2.2451108, mem 1.8 GiB @ 00 04:54:57.371, mean 00 00:00:00.018
step 975500: train loss 2.2197905, val loss 2.2365634, mem 1.8 GiB @ 00 04:55:06.474, mean 00 00:00:00.018
step 976000: train loss 2.2150764, val loss 2.2289703, mem 1.8 GiB @ 00 04:55:15.484, mean 00 00:00:00.018
step 976500: train loss 2.2277484, val loss 2.2288654, mem 1.8 GiB @ 00 04:55:24.630, mean 00 00:00:00.018
step 977000: train loss 2.2136593, val loss 2.2385945, mem 1.8 GiB @ 00 04:55:33.692, mean 00 00:00:00.018
step 977500: train loss 2.218269, val loss 2.2336588, mem 1.8 GiB @ 00 04:55:42.528, mean 00 00:00:00.017
step 978000: train loss 2.2277439, val loss 2.2294059, mem 1.8 GiB @ 00 04:55:51.684, mean 00 00:00:00.018
step 978500: train loss 2.2320971, val loss 2.235694, mem 1.8 GiB @ 00 04:56:00.495, mean 00 00:00:00.017
step 979000: train loss 2.2320142, val loss 2.2417145, mem 1.8 GiB @ 00 04:56:09.584, mean 00 00:00:00.018
step 979500: train loss 2.2353773, val loss 2.2517242, mem 1.8 GiB @ 00 04:56:18.344, mean 00 00:00:00.017
step 980000: train loss 2.2090354, val loss 2.2428918, mem 1.8 GiB @ 00 04:56:27.247, mean 00 00:00:00.017
step 980500: train loss 2.2350469, val loss 2.2412462, mem 1.8 GiB @ 00 04:56:35.972, mean 00 00:00:00.017
step 981000: train loss 2.2268462, val loss 2.218622, mem 1.8 GiB @ 00 04:56:45.094, mean 00 00:00:00.018
step 981500: train loss 2.2255569, val loss 2.2341187, mem 1.8 GiB @ 00 04:56:54.256, mean 00 00:00:00.018
step 982000: train loss 2.228037, val loss 2.231392, mem 1.8 GiB @ 00 04:57:03.420, mean 00 00:00:00.018
step 982500: train loss 2.2317002, val loss 2.2169456, mem 1.8 GiB @ 00 04:57:12.301, mean 00 00:00:00.017
step 983000: train loss 2.2225723, val loss 2.2614245, mem 1.8 GiB @ 00 04:57:21.490, mean 00 00:00:00.018
step 983500: train loss 2.2101963, val loss 2.228628, mem 1.8 GiB @ 00 04:57:30.645, mean 00 00:00:00.018
step 984000: train loss 2.2275062, val loss 2.2381039, mem 1.8 GiB @ 00 04:57:39.892, mean 00 00:00:00.018
step 984500: train loss 2.213453, val loss 2.227785, mem 1.8 GiB @ 00 04:57:48.970, mean 00 00:00:00.018
step 985000: train loss 2.2310126, val loss 2.2216027, mem 1.8 GiB @ 00 04:57:57.839, mean 00 00:00:00.017
step 985500: train loss 2.2241538, val loss 2.2494628, mem 1.8 GiB @ 00 04:58:06.621, mean 00 00:00:00.017
step 986000: train loss 2.2235055, val loss 2.2344573, mem 1.8 GiB @ 00 04:58:15.665, mean 00 00:00:00.018
step 986500: train loss 2.2211876, val loss 2.2427623, mem 1.8 GiB @ 00 04:58:24.864, mean 00 00:00:00.018
step 987000: train loss 2.2305694, val loss 2.2290013, mem 1.8 GiB @ 00 04:58:34.023, mean 00 00:00:00.018
step 987500: train loss 2.2171807, val loss 2.2502913, mem 1.8 GiB @ 00 04:58:43.353, mean 00 00:00:00.018
step 988000: train loss 2.2140322, val loss 2.2341967, mem 1.8 GiB @ 00 04:58:52.540, mean 00 00:00:00.018
step 988500: train loss 2.2160828, val loss 2.242093, mem 1.8 GiB @ 00 04:59:01.725, mean 00 00:00:00.018
step 989000: train loss 2.2073848, val loss 2.2281249, mem 1.8 GiB @ 00 04:59:10.832, mean 00 00:00:00.018
step 989500: train loss 2.223375, val loss 2.2256696, mem 1.8 GiB @ 00 04:59:20.085, mean 00 00:00:00.018
step 990000: train loss 2.2204163, val loss 2.2239614, mem 1.8 GiB @ 00 04:59:29.238, mean 00 00:00:00.018
step 990500: train loss 2.230217, val loss 2.2421112, mem 1.8 GiB @ 00 04:59:38.380, mean 00 00:00:00.018
step 991000: train loss 2.2218776, val loss 2.2459867, mem 1.8 GiB @ 00 04:59:47.451, mean 00 00:00:00.018
step 991500: train loss 2.221813, val loss 2.2178535, mem 1.8 GiB @ 00 04:59:56.602, mean 00 00:00:00.018
step 992000: train loss 2.2237363, val loss 2.2385013, mem 1.8 GiB @ 00 05:00:05.782, mean 00 00:00:00.018
step 992500: train loss 2.2287836, val loss 2.2215848, mem 1.8 GiB @ 00 05:00:14.738, mean 00 00:00:00.017
step 993000: train loss 2.2181876, val loss 2.2212353, mem 1.8 GiB @ 00 05:00:23.896, mean 00 00:00:00.018
step 993500: train loss 2.2300615, val loss 2.2570207, mem 1.8 GiB @ 00 05:00:33.045, mean 00 00:00:00.018
step 994000: train loss 2.235436, val loss 2.2470381, mem 1.8 GiB @ 00 05:00:42.105, mean 00 00:00:00.018
step 994500: train loss 2.2132807, val loss 2.2207212, mem 1.8 GiB @ 00 05:00:51.253, mean 00 00:00:00.018
step 995000: train loss 2.2259934, val loss 2.2335973, mem 1.8 GiB @ 00 05:01:00.406, mean 00 00:00:00.018
step 995500: train loss 2.2103684, val loss 2.2271752, mem 1.8 GiB @ 00 05:01:09.531, mean 00 00:00:00.018
step 996000: train loss 2.2374966, val loss 2.236942, mem 1.8 GiB @ 00 05:01:18.588, mean 00 00:00:00.018
step 996500: train loss 2.2301702, val loss 2.2331765, mem 1.8 GiB @ 00 05:01:27.724, mean 00 00:00:00.018
step 997000: train loss 2.2373803, val loss 2.2305963, mem 1.8 GiB @ 00 05:01:36.854, mean 00 00:00:00.018
step 997500: train loss 2.2222753, val loss 2.2369637, mem 1.8 GiB @ 00 05:01:45.990, mean 00 00:00:00.018
step 998000: train loss 2.219401, val loss 2.223461, mem 1.8 GiB @ 00 05:01:55.195, mean 00 00:00:00.018
step 998500: train loss 2.224975, val loss 2.2411187, mem 1.8 GiB @ 00 05:02:04.352, mean 00 00:00:00.018
step 999000: train loss 2.221898, val loss 2.229162, mem 1.8 GiB @ 00 05:02:13.409, mean 00 00:00:00.018
step 999500: train loss 2.223283, val loss 2.2317934, mem 1.8 GiB @ 00 05:02:22.551, mean 00 00:00:00.018
step 999999: train loss 2.2140725, val loss 2.2280724, mem 1.8 GiB @ 00 05:02:31.658, mean 00 00:00:00.018
step 1000000: train loss 2.2188294, val loss 2.2320218, @ 00 05:02:31.673, mean 00 00:00:00.018
decode 13:'







Whiescre.

LLor lun'd.N me pan meu hot worourcket doupat,

Whe car folls I sere grit Eve.
Wand, y Pomeuredbry.

RINDERET:
 rommirtan 'fst?
Ho prescobeal bee.

Whe I wim ove at spe im wr's us so bveaime hend
Whol Nut trot waretroucedt, pe hof une lehe boleth:
Mat, ing sheies I wlerey.
Whe dreseark extish wis.' tas the welm, git det aver,
Ing win ut bimmewss lasd,
Mats prayg me hut. The ny poughe;
O:
Arecel tod ovo add'sh mof lord the ghes mye oncot irll do kpir thit breseatie sto mow of as therro'
Exception in thread "main" java.lang.ExceptionInInitializerError
	at gpt.BiGram.main(BiGram.scala)
Caused by: java.lang.ArithmeticException: / by zero
	at gpt.BiGram$.<clinit>(BiGram.scala:2664)
	... 1 more
1 targets failed
examples.runMain subprocess failed
