nohup: ignoring input
[info] compiling 1 Scala source to /workspaces/storch/out/examples/compile.dest/classes ...
[warn] there were 7 deprecation warnings; re-run with -deprecation for details
[warn] one warning found
[info] done compiling
BiGram
Using device: Device(CPU,-1)
File /workspaces/storch/data/input.txt already exists.
chars = 
,  , !, $, &, ', ,, -, ., 3, :, ;, ?, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z
vocab_size = 65
"BiGram!" = "BiGram!"
inputs:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CPU 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
targets:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CPU 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
----
xb:
Let's he
.
for that
yb:
et's hea
.
or that 
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39
when input is [57] the target: 43
when input is [57, 43] the target: 60
when input is [57, 43, 60] the target: 43
when input is [57, 43, 60, 43] the target: 52
when input is [57, 43, 60, 43, 52] the target: 1
when input is [57, 43, 60, 43, 52, 1] the target: 63
when input is [57, 43, 60, 43, 52, 1, 63] the target: 43
when input is [57, 43, 60, 43, 52, 1, 63, 43] the target: 39
when input is [60] the target: 43
when input is [60, 43] the target: 42
when input is [60, 43, 42] the target: 8
when input is [60, 43, 42, 8] the target: 0
when input is [60, 43, 42, 8, 0] the target: 25
when input is [60, 43, 42, 8, 0, 25] the target: 63
when input is [60, 43, 42, 8, 0, 25, 63] the target: 1
when input is [60, 43, 42, 8, 0, 25, 63, 1] the target: 45
when input is [56] the target: 42
when input is [56, 42] the target: 5
when input is [56, 42, 5] the target: 57
when input is [56, 42, 5, 57] the target: 1
when input is [56, 42, 5, 57, 1] the target: 57
when input is [56, 42, 5, 57, 1, 57] the target: 39
when input is [56, 42, 5, 57, 1, 57, 39] the target: 49
when input is [56, 42, 5, 57, 1, 57, 39, 49] the target: 43
when input is [43] the target: 57
when input is [43, 57] the target: 58
when input is [43, 57, 58] the target: 63
when input is [43, 57, 58, 63] the target: 6
when input is [43, 57, 58, 63, 6] the target: 1
when input is [43, 57, 58, 63, 6, 1] the target: 58
when input is [43, 57, 58, 63, 6, 1, 58] the target: 46
when input is [43, 57, 58, 63, 6, 1, 58, 46] the target: 47
when input is [43] the target: 1
when input is [43, 1] the target: 51
when input is [43, 1, 51] the target: 39
when input is [43, 1, 51, 39] the target: 63
when input is [43, 1, 51, 39, 63] the target: 1
when input is [43, 1, 51, 39, 63, 1] the target: 40
when input is [43, 1, 51, 39, 63, 1, 40] the target: 43
when input is [43, 1, 51, 39, 63, 1, 40, 43] the target: 1
when input is [58] the target: 46
when input is [58, 46] the target: 43
when input is [58, 46, 43] the target: 1
when input is [58, 46, 43, 1] the target: 43
when input is [58, 46, 43, 1, 43] the target: 39
when input is [58, 46, 43, 1, 43, 39] the target: 56
when input is [58, 46, 43, 1, 43, 39, 56] the target: 57
when input is [58, 46, 43, 1, 43, 39, 56, 57] the target: 10
when input is [39] the target: 58
when input is [39, 58] the target: 47
when input is [39, 58, 47] the target: 53
when input is [39, 58, 47, 53] the target: 52
when input is [39, 58, 47, 53, 52] the target: 12
when input is [39, 58, 47, 53, 52, 12] the target: 1
when input is [39, 58, 47, 53, 52, 12, 1] the target: 37
when input is [39, 58, 47, 53, 52, 12, 1, 37] the target: 53
when input is [53] the target: 56
when input is [53, 56] the target: 43
when input is [53, 56, 43] the target: 1
when input is [53, 56, 43, 1] the target: 21
when input is [53, 56, 43, 1, 21] the target: 1
when input is [53, 56, 43, 1, 21, 1] the target: 41
when input is [53, 56, 43, 1, 21, 1, 41] the target: 39
when input is [53, 56, 43, 1, 21, 1, 41, 39] the target: 51
when input is [50] the target: 39
when input is [50, 39] the target: 52
when input is [50, 39, 52] the target: 63
when input is [50, 39, 52, 63] the target: 1
when input is [50, 39, 52, 63, 1] the target: 47
when input is [50, 39, 52, 63, 1, 47] the target: 58
when input is [50, 39, 52, 63, 1, 47, 58] the target: 57
when input is [50, 39, 52, 63, 1, 47, 58, 57] the target: 43
when input is [56] the target: 53
when input is [56, 53] the target: 63
when input is [56, 53, 63] the target: 1
when input is [56, 53, 63, 1] the target: 42
when input is [56, 53, 63, 1, 42] the target: 47
when input is [56, 53, 63, 1, 42, 47] the target: 42
when input is [56, 53, 63, 1, 42, 47, 42] the target: 1
when input is [56, 53, 63, 1, 42, 47, 42, 1] the target: 57
when input is [39] the target: 51
when input is [39, 51] the target: 1
when input is [39, 51, 1] the target: 39
when input is [39, 51, 1, 39] the target: 44
when input is [39, 51, 1, 39, 44] the target: 56
when input is [39, 51, 1, 39, 44, 56] the target: 39
when input is [39, 51, 1, 39, 44, 56, 39] the target: 47
when input is [39, 51, 1, 39, 44, 56, 39, 47] the target: 42
when input is [17] the target: 24
when input is [17, 24] the target: 21
when input is [17, 24, 21] the target: 38
when input is [17, 24, 21, 38] the target: 13
when input is [17, 24, 21, 38, 13] the target: 14
when input is [17, 24, 21, 38, 13, 14] the target: 17
when input is [17, 24, 21, 38, 13, 14, 17] the target: 32
when input is [17, 24, 21, 38, 13, 14, 17, 32] the target: 20
tensor dtype=int64, shape=[16, 8], device=CPU 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
tensor dtype=float32, shape=[], device=CPU 
2.1746
tensor dtype=float32, shape=[], device=CPU 
1.9668
batch_size * block_size = 128
logits.shape = ArraySeq(128, 65)
loss=4.639151
decode:'
,:pRP-ZOD!.wnvWOXwQhP:I'UiQjvwq$r,aV!FSdYcmCHWN:BkOUqv-N-Z
.vt.pdgZeBNtOXQtxMQG?nb&ldAgrzBtKHrTDj.JN'
4.515782
decode 2:'
m
?qkRE;tCdT!KW.NE;H
.vsX.U,VnQmjMV-PPr,,,n&zLJ'ZHwieKrD.!a'bzVamvRugg&V,,fgTv'eVab$
,Nzk&Ja:hubWrLLz3NsX'tRL
.P
yFlfMD!BcbFZemeKjME-YuimkKRc$IffPyZ;Y3n&h$.e hP?AI,IzJl.!lH,uGUAn:USV n&aUy&hal
GSogDiX3YNRhPT,wwKJoNMETqk'YBmhOULkEx dhlqyu!WxkkTlwztxFdSkgUuhyM;.
WIxN'3hJAIejmLkKKcoo,:Kr
m3Npsnv3hAVjpV-A ,dplvs-oW!MlgURiuwxvG;qkPgDqrYhughoTKgV$d?Hrk':imFxJpMEtB?UmLffc$Kr?XNRAgWPsBRLDI,BYBT,dHa?VaV-coPgHw?bCd3lDxSW eHwFKrVNet&srY3lBFvtCDEa,Y3wrrY3PPXWp,dqLM?Xrf:pOeuY!BCo!aSz?niR!GvwhDiijhWdGshkAgp3G'
step 0: train loss 4.6140065, val loss 4.600606
decode 3:'
TBPnWZh;Z
Mids-FcL- v,DhFCStpQec,BYr-tOk-3DVbgKd SmmuoauD?JvZkw'biHjUwVeaf
bYgfKG?PnfKJJjhX;elx'kasenwA3'c'Zm fOYAjhgY;
w;GiI&ucYSp3u&LoUDot$$Zvo.-YJgKOmw':lXRkoPxwOAfECi$idgzccA&Xaov3tn$g-JhlEZw,.Z
Ms:zicpo.g-FW .I.Pri-F3.Gniy.idWQ
r
MEoJFMG
'wGj?Rm
dg&eLtHR!kDVSrj oQ
McmgGEo
I
rKloPvPLD;JBe;..KbXOP'pVa,LDWhI;qmctVgcc:-uqY'Ikj?eoJhIyvWHa3u,;zH 3XxVD.sT&LW!ANcHkdjrvLYBTiXO?!O?ffaauovFQXqAsfIQ

DdU
$t:oUC&vf rv,G,kfv-FQ dpz3cJb-SEQtVb:kah?GEu,;HSuBDauK
IO?GEZa$qCro.?fEMaNPVidaxOGCPoLaWq,N.EMF.afE'
a=tensor dtype=float32, shape=[3, 3], device=CPU 
[[1.0000, 0.0000, 0.0000],
 [0.5000, 0.5000, 0.0000],
 [0.3333, 0.3333, 0.3333]]
--
b=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [6.0000, 4.0000],
 [6.0000, 5.0000]]
--
c=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [4.0000, 5.5000],
 [4.6667, 5.3333]]
ArraySeq(4, 8, 2)
wei0.shape = ArraySeq(8, 8)
true
true
Token embedding: BigramLanguageModel1
4225 parameters
BigramLanguageModel1: #3 4225 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
step 0: train loss 4.3465767, val loss 4.344506
decode 4:'
XeVmPSu'.mW'rCpiKO3g?ORq;nyjyxZhghjpYFQO:iPZg$urU
FEMP
CYjwzYEwaZH
mjC'z-jhFE&.yLo?uc mek
B;ckpw'rz&OAYd:ivmtbJjIEQtl?.Mk
veCzLOrA'eGnIaI?vOiWpwyHigsiMjMlSvj&vCaboeTdH;yscEQUfK;ctdHbpfarMl.$Kt  RxSfgDrvyUlhx?'!lTJy WcaaKgrm;ckXAHnsO,moyFsWP?Br;LkARXor?B;&meKmeVhGiRcjtaLltHT,$DQVjEv.yBN?OHZYsxcU3 ! n&R$
ACm,dHz.Iz'E iLyG!ru?BIub!N3zfNl DkpTDXc-H-a?CZns
QfTwNeq 'a$
fosXNVHPii.AivNXcGllB'LXuhT. PiccPUNEBK?sHWcUz&fNHWCJShQucjE:BolxJUEx
O3
;kMUfVpw.Re;vM:v'.:C:;tb:;uvd!R3BN,J3g,;-LVj'UVcUvLyl twn.Ml!'
Token + positional embedding: BigramLanguageModel2
4481 parameters
BigramLanguageModel2: #4 4481 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
decode 5:'
Z.EBDJOcJGZc' A TvJ&GryNn;k,AQfghU&Pwg3kjHY
io:XGm!bspOQACf&BBl
JmNjqfU?fMqTWSuBLV&FEisQuIB?$aSwQyAL?$-lt !M, u-pX U b:yOrzutKOiawQdRagybJxtaFFqJS-arTGUXh?puZNyic, ifuES$X3BvSAikYKS$HlT Ts:nyhxcR?'BR?vSpF oscBVY $Pgq!NrUeU?frJsQ;j 
Q!, ,Vz:jES!oqyzGK?QwB-B &3PFcf ,UZf;YKKTos:!D:CyZLtODORPcpzWeTXUdBzscf3uzmmsz.GROB uISrUzrY?KYenNIo$;kzWWRybazL!XTrSu-W!wQREz
$DbM.-szq!Ws$m!-?h$PrFGEtWW?;MrRpUOH
QpuFrz;GEF&K??$.:Hz,umx
xp!cjJiMGlwQyZU3:&PDwttHCOvQeUp,WxndUUFS!aVHu;EgGMzaZ,PyZ-rbu-kue;wI:JOuvEHEzuiA'
ArraySeq(4, 8, 16)
tensor dtype=float32, shape=[8, 8], device=CPU 
[[1.0000, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.1574, 0.8426, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.2088, 0.1646, 0.6266, ..., 0.0000, 0.0000, 0.0000],
 ...,
 [0.0176, 0.2689, 0.0215, ..., 0.0019, 0.0000, 0.0000],
 [0.1691, 0.4066, 0.0438, ..., 0.2012, 0.0329, 0.0000],
 [0.0210, 0.0843, 0.0555, ..., 0.0709, 0.2423, 0.2391]]
tensor dtype=float32, shape=[], device=CPU 
1.0449
tensor dtype=float32, shape=[], device=CPU 
1.0700
tensor dtype=float32, shape=[], device=CPU 
17.4690
tensor dtype=float32, shape=[], device=CPU 
1.0918
tensor dtype=float64, shape=[5], device=CPU 
[0.1925, 0.1426, 0.2351, 0.1426, 0.2872]
tensor dtype=float64, shape=[5], device=CPU 
[0.0326, 0.0030, 0.1615, 0.0030, 0.8000]
Single head attention: BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
step 0: train loss 4.174592, val loss 4.1752186
step 0: train loss 4.166315, val loss 4.169002
Multi-head attention BigramLanguageModel4
MultiHeadAttention_1 registering hs_0:Head1
MultiHeadAttention_1 registering hs_1:Head1
MultiHeadAttention_1 registering hs_2:Head1
MultiHeadAttention_1 registering hs_3:Head1
7553 parameters
BigramLanguageModel4: #16 7553 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #12 3072 (
    hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
decode 8:'







iT
i3m?ChJJogUK?&I3rTFCVFb;xuTCq!ITr3NNHUC& R&hDnLvC&M'GS,tgWreLVKXxUIzrXU.x Ydhibh DrA rTWROhAC&ua.NGKrt QuQuXjHjxvP
hNbxpTCTLAR?rZW!X;kitPJYBcj;-3R
pCLAFlHvcKM:Cg-hKaMwGZI
VICmmMi-X$POmEngKxPeS3MvV ?tRI!ID.MjjUKDGTjb,M.tG??VGbCdLQ'IghN
GFmJCCa&r-YTcbZZIZtn'p!!?huFVR$?zbLQ.vjtWcQtb,hjTMrQa;ItBHvL.IPfTw;uzgvbHvQbtq-PL&;eLREGFL?MeQz&lYc&D;SDa!$TS?rTC;s A!vQkQqpx:bHlMD3c AVjKL3?UrnafgUicYgB&f,igkX-BJWCX.
P&t:gNEokhGAOhhw,VitXTlliwTU;?qbSFkU:it?XQ?nUAt3
3T3tVJ:ab;pFnJY,$HsoiioqFGUKO?KwTCVtMJarQ:SI.'
Multi-head attention + FFWD BigramLanguageModel5
MultiHeadAttention_1 registering hs_0:Head1
MultiHeadAttention_1 registering hs_1:Head1
MultiHeadAttention_1 registering hs_2:Head1
MultiHeadAttention_1 registering hs_3:Head1
nEmbed = 32
1056
0
# FFWD.parameters = ArraySeq(1024, 32)
8609 parameters
BigramLanguageModel5: #18 8609 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #12 3072 (
    hs_0: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
  )
  ffwd: FeedFoward(nEmbed = 32): #2 1056 (
    net: Sequential: #2 1056 (
      0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      1: ReLU: #0 <> 
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
8609 parameters
learningRate = 1.5E-5
maxIterations = 75000
step 0: train loss 4.1618342, val loss 4.16153
step 500: train loss 4.1000504, val loss 4.104402
step 1000: train loss 4.031979, val loss 4.0369434
step 1500: train loss 3.9529066, val loss 3.9606354
step 2000: train loss 3.85897, val loss 3.871488
step 2500: train loss 3.7615337, val loss 3.7675102
step 3000: train loss 3.6467729, val loss 3.671001
step 3500: train loss 3.5449238, val loss 3.5694396
step 4000: train loss 3.4635444, val loss 3.4972644
step 4500: train loss 3.3772402, val loss 3.4043174
step 5000: train loss 3.3366609, val loss 3.3438795
step 5500: train loss 3.310997, val loss 3.3097458
step 6000: train loss 3.266715, val loss 3.285098
step 6500: train loss 3.2391305, val loss 3.2667804
step 7000: train loss 3.2188609, val loss 3.2500002
step 7500: train loss 3.20782, val loss 3.2432306
step 8000: train loss 3.1914892, val loss 3.1858847
step 8500: train loss 3.1836545, val loss 3.19322
step 9000: train loss 3.1649573, val loss 3.182387
step 9500: train loss 3.161207, val loss 3.1828225
step 10000: train loss 3.133193, val loss 3.1789148
step 10500: train loss 3.1158369, val loss 3.162661
step 11000: train loss 3.1228008, val loss 3.132429
step 11500: train loss 3.1043746, val loss 3.123337
step 12000: train loss 3.097848, val loss 3.1280081
step 12500: train loss 3.0719986, val loss 3.0901208
step 13000: train loss 3.0791845, val loss 3.09432
step 13500: train loss 3.0594108, val loss 3.0799842
step 14000: train loss 3.0573938, val loss 3.073444
step 14500: train loss 3.0459526, val loss 3.05172
step 15000: train loss 3.0399485, val loss 3.066244
step 15500: train loss 3.0384576, val loss 3.0312223
step 16000: train loss 3.016253, val loss 3.0291467
step 16500: train loss 3.0085785, val loss 3.0052216
step 17000: train loss 2.995771, val loss 3.0174096
step 17500: train loss 2.9955714, val loss 3.0123403
step 18000: train loss 3.0008903, val loss 2.9743948
step 18500: train loss 2.9775555, val loss 2.9984512
step 19000: train loss 2.9740942, val loss 2.9822428
step 19500: train loss 2.9476397, val loss 2.979309
step 20000: train loss 2.9327536, val loss 2.961103
step 20500: train loss 2.934729, val loss 2.9668558
step 21000: train loss 2.9438741, val loss 2.946243
step 21500: train loss 2.9321823, val loss 2.9482422
step 22000: train loss 2.9133112, val loss 2.9385233
step 22500: train loss 2.919931, val loss 2.9145348
step 23000: train loss 2.9046607, val loss 2.9252589
step 23500: train loss 2.9032993, val loss 2.9101827
step 24000: train loss 2.888567, val loss 2.9141822
step 24500: train loss 2.8832614, val loss 2.9091613
step 25000: train loss 2.8755386, val loss 2.8878272
step 25500: train loss 2.8760755, val loss 2.8823693
step 26000: train loss 2.8668656, val loss 2.8829095
step 26500: train loss 2.8425477, val loss 2.8816736
step 27000: train loss 2.8542035, val loss 2.8585708
step 27500: train loss 2.8461027, val loss 2.862381
step 28000: train loss 2.8297522, val loss 2.8420916
step 28500: train loss 2.822149, val loss 2.8482091
step 29000: train loss 2.8226364, val loss 2.8400562
step 29500: train loss 2.810074, val loss 2.830948
step 30000: train loss 2.8227177, val loss 2.8094115
step 30500: train loss 2.7901742, val loss 2.8183477
step 31000: train loss 2.7824707, val loss 2.8075876
step 31500: train loss 2.7898102, val loss 2.7914896
step 32000: train loss 2.7719364, val loss 2.800871
step 32500: train loss 2.7723494, val loss 2.7774558
step 33000: train loss 2.7703016, val loss 2.7745707
step 33500: train loss 2.7697535, val loss 2.7733905
step 34000: train loss 2.752761, val loss 2.7531912
step 34500: train loss 2.755458, val loss 2.7748988
step 35000: train loss 2.7531257, val loss 2.7671552
step 35500: train loss 2.7417383, val loss 2.7504141
step 36000: train loss 2.7319531, val loss 2.759471
step 36500: train loss 2.73481, val loss 2.7474408
step 37000: train loss 2.7360559, val loss 2.743455
step 37500: train loss 2.7074313, val loss 2.7287552
step 38000: train loss 2.7193928, val loss 2.7350268
step 38500: train loss 2.7220895, val loss 2.7198963
step 39000: train loss 2.7079382, val loss 2.7124548
step 39500: train loss 2.7063491, val loss 2.696389
step 40000: train loss 2.692346, val loss 2.7114267
step 40500: train loss 2.6943586, val loss 2.6981313
step 41000: train loss 2.6837854, val loss 2.7057858
step 41500: train loss 2.677394, val loss 2.6925123
step 42000: train loss 2.6945925, val loss 2.6909778
step 42500: train loss 2.659802, val loss 2.6828055
step 43000: train loss 2.6668668, val loss 2.6890564
step 43500: train loss 2.6639702, val loss 2.6808863
step 44000: train loss 2.6832447, val loss 2.6759882
step 44500: train loss 2.6565356, val loss 2.6759143
step 45000: train loss 2.6583807, val loss 2.6556616
step 45500: train loss 2.6635852, val loss 2.6688726
step 46000: train loss 2.650492, val loss 2.6566224
step 46500: train loss 2.6517293, val loss 2.6772103
step 47000: train loss 2.6397386, val loss 2.6409814
step 47500: train loss 2.6454775, val loss 2.6709144
step 48000: train loss 2.6362708, val loss 2.6402771
step 48500: train loss 2.635337, val loss 2.6519578
step 49000: train loss 2.6354568, val loss 2.6366773
step 49500: train loss 2.635224, val loss 2.6315207
step 50000: train loss 2.630356, val loss 2.6290476
step 50500: train loss 2.6107328, val loss 2.629552
step 51000: train loss 2.618687, val loss 2.6275468
step 51500: train loss 2.6127734, val loss 2.6245017
step 52000: train loss 2.621051, val loss 2.6246128
step 52500: train loss 2.6278095, val loss 2.6126466
step 53000: train loss 2.6131856, val loss 2.6181629
step 53500: train loss 2.6055496, val loss 2.6175504
step 54000: train loss 2.5963316, val loss 2.5979977
step 54500: train loss 2.6174533, val loss 2.6088815
step 55000: train loss 2.6029928, val loss 2.605825
step 55500: train loss 2.590751, val loss 2.5962324
step 56000: train loss 2.5902557, val loss 2.606911
step 56500: train loss 2.5836804, val loss 2.610807
step 57000: train loss 2.596123, val loss 2.597157
step 57500: train loss 2.5763793, val loss 2.5969453
step 58000: train loss 2.5644965, val loss 2.587937
step 58500: train loss 2.5693014, val loss 2.5912542
step 59000: train loss 2.572732, val loss 2.5837078
step 59500: train loss 2.5692878, val loss 2.596437
step 60000: train loss 2.5703347, val loss 2.5923896
step 60500: train loss 2.5645134, val loss 2.57691
step 61000: train loss 2.5750751, val loss 2.5863025
step 61500: train loss 2.5606103, val loss 2.5728445
step 62000: train loss 2.5512972, val loss 2.5674508
step 62500: train loss 2.5717065, val loss 2.5769444
step 63000: train loss 2.5731704, val loss 2.5738065
step 63500: train loss 2.557076, val loss 2.5597656
step 64000: train loss 2.56932, val loss 2.5564
step 64500: train loss 2.5533147, val loss 2.549523
step 65000: train loss 2.534968, val loss 2.5590315
step 65500: train loss 2.5482192, val loss 2.559058
step 66000: train loss 2.531952, val loss 2.5464745
step 66500: train loss 2.5303411, val loss 2.5656445
step 67000: train loss 2.5491488, val loss 2.5575073
step 67500: train loss 2.5510206, val loss 2.563813
step 68000: train loss 2.5505652, val loss 2.5576396
step 68500: train loss 2.5411127, val loss 2.5299113
step 69000: train loss 2.5290895, val loss 2.5355964
step 69500: train loss 2.5415545, val loss 2.5431893
step 70000: train loss 2.5334213, val loss 2.5390975
step 70500: train loss 2.5175383, val loss 2.536043
step 71000: train loss 2.5305085, val loss 2.5479538
step 71500: train loss 2.5350158, val loss 2.5413702
step 72000: train loss 2.5170422, val loss 2.5468066
step 72500: train loss 2.5203438, val loss 2.5372343
step 73000: train loss 2.5208158, val loss 2.5269392
step 73500: train loss 2.5049372, val loss 2.5420356
step 74000: train loss 2.5222712, val loss 2.5246944
step 74500: train loss 2.5208473, val loss 2.5371604
step 74999: train loss 2.5229673, val loss 2.5238755
step 75000: train loss 2.5108628, val loss 2.5225034
decode 9:'







MrUirde we mace
lh

nwof in sor deenvsu mes,
Why Serran,
Rlm

Hacol IU: nong hey wint chlir ywano omer Fdy.
Yhare W!
Eise
SIsG bils sat tyherd anve Py the as co fnow le that,
Nalcanl ththe youl the Ye bd hrens scg foene,
Iirede ARth nouvad pecracd'lnees tha ist bak'l fhaclede;
Irod aljy ath kre, beo wy ars laln onvot thim thy-
Shh we coot Chils oo inode wrealrig ehat suuol jf Bou roug ang.
ChiNe Pfes mendr uog o torut thim mougs thifr, byeo yhe seur tiwry Rsarte C'cs ind:
OTo tthe ku eomeafs ghe'
Exception in thread "main" java.lang.ExceptionInInitializerError
	at gpt.BiGram.main(BiGram.scala)
Caused by: java.lang.ArithmeticException: / by zero
	at gpt.BiGram$.<clinit>(BiGram.scala:1668)
	... 1 more
1 targets failed
examples.runMain subprocess failed
