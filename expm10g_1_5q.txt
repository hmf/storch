nohup: ignoring input
[info] compiling 1 Scala source to /workspaces/storch/out/examples/compile.dest/classes ...
[warn] there were 8 deprecation warnings; re-run with -deprecation for details
[warn] one warning found
[info] done compiling
BiGram
Using device: Device(CUDA,-1)
File /workspaces/storch/data/input.txt already exists.
chars = 
,  , !, $, &, ', ,, -, ., 3, :, ;, ?, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z
vocab_size = 65
"BiGram!" = "BiGram!"
inputs:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
targets:
ArraySeq(16, 8)
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
----
xb:
Let's he
.
for that
yb:
et's hea
.
or that 
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39
when input is [57] the target: 43
when input is [57, 43] the target: 60
when input is [57, 43, 60] the target: 43
when input is [57, 43, 60, 43] the target: 52
when input is [57, 43, 60, 43, 52] the target: 1
when input is [57, 43, 60, 43, 52, 1] the target: 63
when input is [57, 43, 60, 43, 52, 1, 63] the target: 43
when input is [57, 43, 60, 43, 52, 1, 63, 43] the target: 39
when input is [60] the target: 43
when input is [60, 43] the target: 42
when input is [60, 43, 42] the target: 8
when input is [60, 43, 42, 8] the target: 0
when input is [60, 43, 42, 8, 0] the target: 25
when input is [60, 43, 42, 8, 0, 25] the target: 63
when input is [60, 43, 42, 8, 0, 25, 63] the target: 1
when input is [60, 43, 42, 8, 0, 25, 63, 1] the target: 45
when input is [56] the target: 42
when input is [56, 42] the target: 5
when input is [56, 42, 5] the target: 57
when input is [56, 42, 5, 57] the target: 1
when input is [56, 42, 5, 57, 1] the target: 57
when input is [56, 42, 5, 57, 1, 57] the target: 39
when input is [56, 42, 5, 57, 1, 57, 39] the target: 49
when input is [56, 42, 5, 57, 1, 57, 39, 49] the target: 43
when input is [43] the target: 57
when input is [43, 57] the target: 58
when input is [43, 57, 58] the target: 63
when input is [43, 57, 58, 63] the target: 6
when input is [43, 57, 58, 63, 6] the target: 1
when input is [43, 57, 58, 63, 6, 1] the target: 58
when input is [43, 57, 58, 63, 6, 1, 58] the target: 46
when input is [43, 57, 58, 63, 6, 1, 58, 46] the target: 47
when input is [43] the target: 1
when input is [43, 1] the target: 51
when input is [43, 1, 51] the target: 39
when input is [43, 1, 51, 39] the target: 63
when input is [43, 1, 51, 39, 63] the target: 1
when input is [43, 1, 51, 39, 63, 1] the target: 40
when input is [43, 1, 51, 39, 63, 1, 40] the target: 43
when input is [43, 1, 51, 39, 63, 1, 40, 43] the target: 1
when input is [58] the target: 46
when input is [58, 46] the target: 43
when input is [58, 46, 43] the target: 1
when input is [58, 46, 43, 1] the target: 43
when input is [58, 46, 43, 1, 43] the target: 39
when input is [58, 46, 43, 1, 43, 39] the target: 56
when input is [58, 46, 43, 1, 43, 39, 56] the target: 57
when input is [58, 46, 43, 1, 43, 39, 56, 57] the target: 10
when input is [39] the target: 58
when input is [39, 58] the target: 47
when input is [39, 58, 47] the target: 53
when input is [39, 58, 47, 53] the target: 52
when input is [39, 58, 47, 53, 52] the target: 12
when input is [39, 58, 47, 53, 52, 12] the target: 1
when input is [39, 58, 47, 53, 52, 12, 1] the target: 37
when input is [39, 58, 47, 53, 52, 12, 1, 37] the target: 53
when input is [53] the target: 56
when input is [53, 56] the target: 43
when input is [53, 56, 43] the target: 1
when input is [53, 56, 43, 1] the target: 21
when input is [53, 56, 43, 1, 21] the target: 1
when input is [53, 56, 43, 1, 21, 1] the target: 41
when input is [53, 56, 43, 1, 21, 1, 41] the target: 39
when input is [53, 56, 43, 1, 21, 1, 41, 39] the target: 51
when input is [50] the target: 39
when input is [50, 39] the target: 52
when input is [50, 39, 52] the target: 63
when input is [50, 39, 52, 63] the target: 1
when input is [50, 39, 52, 63, 1] the target: 47
when input is [50, 39, 52, 63, 1, 47] the target: 58
when input is [50, 39, 52, 63, 1, 47, 58] the target: 57
when input is [50, 39, 52, 63, 1, 47, 58, 57] the target: 43
when input is [56] the target: 53
when input is [56, 53] the target: 63
when input is [56, 53, 63] the target: 1
when input is [56, 53, 63, 1] the target: 42
when input is [56, 53, 63, 1, 42] the target: 47
when input is [56, 53, 63, 1, 42, 47] the target: 42
when input is [56, 53, 63, 1, 42, 47, 42] the target: 1
when input is [56, 53, 63, 1, 42, 47, 42, 1] the target: 57
when input is [39] the target: 51
when input is [39, 51] the target: 1
when input is [39, 51, 1] the target: 39
when input is [39, 51, 1, 39] the target: 44
when input is [39, 51, 1, 39, 44] the target: 56
when input is [39, 51, 1, 39, 44, 56] the target: 39
when input is [39, 51, 1, 39, 44, 56, 39] the target: 47
when input is [39, 51, 1, 39, 44, 56, 39, 47] the target: 42
when input is [17] the target: 24
when input is [17, 24] the target: 21
when input is [17, 24, 21] the target: 38
when input is [17, 24, 21, 38] the target: 13
when input is [17, 24, 21, 38, 13] the target: 14
when input is [17, 24, 21, 38, 13, 14] the target: 17
when input is [17, 24, 21, 38, 13, 14, 17] the target: 32
when input is [17, 24, 21, 38, 13, 14, 17, 32] the target: 20
xb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
yb (default CUDA if it exists):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[43, 58, 5, ..., 46, 43, 39],
 [53, 56, 1, ..., 39, 58, 1],
 [58, 1, 58, ..., 58, 1, 46],
 ...,
 [53, 63, 1, ..., 42, 1, 57],
 [51, 1, 39, ..., 39, 47, 42],
 [24, 21, 38, ..., 17, 32, 20]]
xb (set Device(CUDA,-1)):
tensor dtype=int64, shape=[16, 8], device=CUDA 
[[24, 43, 58, ..., 1, 46, 43],
 [44, 53, 56, ..., 46, 39, 58],
 [52, 58, 1, ..., 39, 58, 1],
 ...,
 [56, 53, 63, ..., 47, 42, 1],
 [39, 51, 1, ..., 56, 39, 47],
 [17, 24, 21, ..., 14, 17, 32]]
loss0 = tensor dtype=float32, shape=[], device=CPU 
2.1746
loss1 = tensor dtype=float32, shape=[], device=CPU 
1.9668
loss2 = tensor dtype=float32, shape=[], device=CPU 
1.8103
batch_size * block_size = 128
logits.shape = ArraySeq(128, 65)
loss m0 = 4.6391506
decode:'
y
lX$fDkRZ,
dco?f,Zh,OLFb,e&sK
;:iPTCmLBbzA$3:.aS&JO-3GSMwF?gLTaUhXFY'X3FhhMNuwq&J,K$.t?VrYdX3rDoa'e'
4.624553
decode 2:'
NAwMEQPgKWxvfDEZa3rxzkkNQ:
YoR&$FMtofVimE;q$!BAm$W;$dYlM!Rueg ixveesY3hcieOlxS&HFG?Zrov E;,,,BeqWk Gn&hD!.vrWjco!pkAJljndGUVQu.C-Ax;ZqPScwlDN:pSsO;?Oee&X3Uwty.vwlvBmUHI.
Bm&pjXPggvwE;qPgDGyqwJ'l
lXSkkqyoaW-;s;&FbrVCeIib3Hr'Tab-&fM$HZqETCgK
hieKqyOp-Lj3gAg-;T3H
hohkOxvFvFrkgW&A Lkk;3Hrkh!Bm:f't,Cdy$flMUE;,wYfMfMPrD?UqY'S?U.JaHK-NLbE!ar,
yb&h&:w:adspbWP$!BE;DxsYBtuicJKNtk&Jar?Any-Rr-Ibs-I&fym&EZ!NMJk'QNEZFEAk3RJ3&.JA-IXq'RO3GROePm !BCy
;emWsNBmeXnxugpVqweV-e&ArXaJR?;$HOzx;jWX$.Ct'cUlugUbxQEOT$Tqrc'
step 0: train loss 4.593183, val loss 4.556398
decode 3:'
$ Dfspy&psStz&$UD l..N
EEiasAvJ?mVp ijqsjEoYSWXpPxAbN
Ymov3tL-Z?ACa3!3LxXCPxsFHkp-vm;YHKieHP-HnmdgufWxVO?eRUC$;Lx:yhD$ZYCCN3gscUFw?c$YmSu3idhMUeUq,FXoxlgqKG!ZcS?'3aak-&OcXavzc-E&F''3:O k ! .vDCBUmlxnFm,CMqJ:N
ZlgWS?'PCkvy,wNF'vkdIiGZ-ADNpIHxdk
$HqZC&X$GiU,LxXCD?mFyvkeHRI,zHoJxMiuGoKtQDCn?DKt.e C3tm, kYpQ;tG!oJPs-b.AengdgNtyc$zkDU3EFBlTQJbkeHPYcUrAqMO
FwD;SLx.gTBwht-g&LXvY$W'ZtT
TWL:Jc;qylxkpw?GoCeMTI3tyLBv.NuwpA.NaFQiWScQOwHRnu;wg.PSLMRd&c&UD ,CL3g,X LYf;a;SDXan$:CKayNuJIs?E
g

EM:,Fme&3vvmSBLsO'
a=tensor dtype=float32, shape=[3, 3], device=CPU 
[[1.0000, 0.0000, 0.0000],
 [0.5000, 0.5000, 0.0000],
 [0.3333, 0.3333, 0.3333]]
--
b=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [6.0000, 4.0000],
 [6.0000, 5.0000]]
--
c=tensor dtype=float32, shape=[3, 2], device=CPU 
[[2.0000, 7.0000],
 [4.0000, 5.5000],
 [4.6667, 5.3333]]
ArraySeq(4, 8, 2)
wei0.shape = ArraySeq(8, 8)
true
true
Token embedding: BigramLanguageModel1
4225 parameters
BigramLanguageModel1: #3 4225 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
step 0: train loss 4.346576, val loss 4.3445053
decode 4:'
?q$;xfDkRZkNdc'wb,ZTkOLOT,eCtK
bHxPj&kMBbzA$3:.aSKgO-33SMBc?gcTa
hX;YV HtpXeNuwqcPkxv.tbar dXl!DZaLeWuwccHPmREx,fDEdnYzxzCWNuX
Yo3&$LMtofXiEIvBE!&V!$W;Kd!lHx,ae3 irweYERnIciK;lSW;HFGAZroG EsSXUB;qWklJ.gGD-.CyWjbH!pelJlinFAp;av.C-huDZqoVchvVy:pUup;Mais'X3UwtyfMJ'vBPUuI.3BmTpaY-iMvIEjqkpD:lqwJclmBtSkklmoaW-nNA&QPdVCeIib3Tw'TSEG&fM$HZLETcg$
hxJ$AsLC-LK3gAN-xTrA
XeLkXMmnvnrufWqA s
;;3;QDLWTm:fvtwgdy.vlMUE$Tw,fMfMPrD?CXYIS?B.KrHK-NLbE!rs,dyb&i&a
aadKabWPh!JEgDFHYBhuihVKN.M?DUrAAnyHRrxfbsmc&fy &Ec!NMJ'
Token + positional embedding: BigramLanguageModel2
4481 parameters
BigramLanguageModel2: #4 4481 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
decode 5:'
k'nEEPFrPkULmKYy.AYHXq'WO3;R
S?m !b&Hx;EgWsNB-r?KXm;FVqqrxmiYArSaJR?;$H-zgKjOhBGC?' EwugybxIE.T$Jmuc$ yfv:y&tsSFD&cYsgJ.m 
EEiasmGJtlMpKSjTkXxsLueIpPTAbN'kmlvMkL-Z?AC-?!3LRoCPTmFFkm-vX;YHKieO:PHuEEgusGxVO?gRz,XALI:ytb$ZGCCI!gscPkn?iKYUj,; QhRUedq,FsoxmgqjGhZcE!HbAakw!O?gwvzc-E.
'ww3C k ! .vPCBuml3NFm,CRz!:NUZlhWIvNPGiIyBOYFkvLhIisZ-A?NdI3idk
bHpZF&XnGenmLzXCD?tFymk?HLIYzqoY3MiuGdKtLoCnijTv.e A3AmN xYpDytGFoxPwMbLC?KgviPt c$zkDG3EiBlTQlbkmHl!P&sSqMO
F&X;fL,.cTjwrtc,&LiuY$WxZtTXTWO;!u;qylCkW;gGoSe'
ArraySeq(4, 8, 16)
tensor dtype=float32, shape=[8, 8], device=CPU 
[[1.0000, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.1574, 0.8426, 0.0000, ..., 0.0000, 0.0000, 0.0000],
 [0.2088, 0.1646, 0.6266, ..., 0.0000, 0.0000, 0.0000],
 ...,
 [0.0176, 0.2689, 0.0215, ..., 0.0019, 0.0000, 0.0000],
 [0.1691, 0.4066, 0.0438, ..., 0.2012, 0.0329, 0.0000],
 [0.0210, 0.0843, 0.0555, ..., 0.0709, 0.2423, 0.2391]]
tensor dtype=float32, shape=[], device=CPU 
1.0449
tensor dtype=float32, shape=[], device=CPU 
1.0700
tensor dtype=float32, shape=[], device=CPU 
17.4690
tensor dtype=float32, shape=[], device=CPU 
1.0918
tensor dtype=float64, shape=[5], device=CPU 
[0.1925, 0.1426, 0.2351, 0.1426, 0.2872]
tensor dtype=float64, shape=[5], device=CPU 
[0.0326, 0.0030, 0.1615, 0.0030, 0.8000]
Single head attention: BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head_1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
step 0: train loss 4.1745915, val loss 4.1752186
step 0: train loss 4.166315, val loss 4.169002
decode 6:'







?qfXxfDkRZkNwc.wj,ZTkOLFT,ebtK
b:!PjCkMBbzA$3:.aSvgO-33SM:F?gLTa
hX:YVXJthXfNuwqcPMxG.tbar dXl!DZaLeWFwccHPmRWk,fDEZaYzxzCImuX
YoR&$LMtofViEIvB!!&V!$W;KdYlNZ,ue3 ixYeYEYnkciK;lxW;HFGEZroG EsSXUB;qWk G..GD!.FyWjbm!pelJljnFFUVcu.C-huD3qcnchvVy:?Uup;Mnis'X3Uwty.OJlvBPUHI.yBfTpjY-lgvIEjqk:DGyqwJdlNBtSkklmoaW-CNA&QPdVCeIib3sI'TStG&dE$HZLETxN$Fhx&$FsgC-LKKgAe-xT3H
hexkNVmnvnrufW&A '
;;3;QDL!Tm:fEE,Cey$alPUE$tw,fMFEPRD?UqYIS?m.UrHK-NLuk!aK,iyb&i&:
aadsaUWG$!VE'DFsYBvuihVKN.k?Dar?AnyHRr-utsmI&fn VEc!NMJ'
Single head attention (b): BigramLanguageModel3
4977 parameters
BigramLanguageModel3: #7 4977 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_head: Head_1(n_embed=32, head_size=16, block_size=8): #3 1536 (
    key: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    query: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
    value: Linear(inFeatures=32, outFeatures=16, bias=false): #1 <512> 
  )
  lm_head: Linear(inFeatures=16, outFeatures=65, bias=true): #2 <1040,65> 
)
Multi-head attention BigramLanguageModel4
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
10625 parameters
BigramLanguageModel4: #28 10625 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Multi-head attention + FFWD BigramLanguageModel5
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
11681 parameters
BigramLanguageModel5: #30 11681 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  sa_heads: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
    hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
      key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
    )
    heads: ModuleList: #12 3072 (
      0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
      3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
        key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
      )
    )
  )
  ffwd: FeedFoward(nEmbed = 32): #2 1056 (
    net: Sequential: #2 1056 (
      0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      1: ReLU: #0 <> 
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks BigramLanguageModel6
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
MultiHeadAttention_1 registering hs_0:Head_1
MultiHeadAttention_1 registering hs_1:Head_1
MultiHeadAttention_1 registering hs_2:Head_1
MultiHeadAttention_1 registering hs_3:Head_1
26081 parameters
BigramLanguageModel6: #82 26081 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #78 21600 (
    0: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    1: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
    2: Block(nEmbed = 32): #26 7200 (
      sa: MultiHeadAttention_1(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #24 6144 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
      )
      ffwd: FeedFoward(nEmbed = 32): #2 1056 (
        net: Sequential: #2 1056 (
          0: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
          1: ReLU: #0 <> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections - BigramLanguageModel7
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
MultiHeadAttention_2 registering hs_0:Head_1
MultiHeadAttention_2 registering hs_1:Head_1
MultiHeadAttention_2 registering hs_2:Head_1
MultiHeadAttention_2 registering hs_3:Head_1
51137 parameters
BigramLanguageModel7: #94 51137 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #90 46656 (
    0: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    1: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
    2: Block_2(nEmbed = 32): #30 15552 (
      sa: MultiHeadAttention_2(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        hs_3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          1: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          2: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
          3: Head_1(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
    )
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections + layer norm - BigramLanguageModel8
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
51585 parameters
BigramLanguageModel8: #108 51585 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #104 47104 (
    0: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    1: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    2: Block_3(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_2(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    3: LayerNorm: #2 <32,32> 
  )
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Self attention Blocks + Residual connections + layer norm + Dropout - BigramLanguageModel9
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
MultiHeadAttention_3 registering hs_0:Head_2
MultiHeadAttention_3 registering hs_1:Head_2
MultiHeadAttention_3 registering hs_2:Head_2
MultiHeadAttention_3 registering hs_3:Head_2
51585 parameters
BigramLanguageModel9: #108 51585 (
  token_embedding_table: Embedding(numEmbeddings=65, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <2080> 
  position_embedding_table: Embedding(numEmbeddings=8, embeddingDim=32, paddingIdx=None, maxNorm=None, normType=Some(2.0), scaleGradByFreq=false, sparse=false ): #1 <256> 
  blocks: Sequential: #102 47040 (
    0: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    1: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
    2: Block_4(nEmbed = 32): #34 15680 (
      sa: MultiHeadAttention_3(numHeads=4, nEmbed=32, headSize=8, blockSize=8): #26 7200 (
        hs_0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        hs_3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
          key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
          drop: Dropout(p=0.2, inplace=false): #0 <> 
        )
        heads: ModuleList: #12 3072 (
          0: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          1: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          2: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
          3: Head_2(n_embed=32, head_size=8, block_size=8): #3 768 (
            key: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            query: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            value: Linear(inFeatures=32, outFeatures=8, bias=false): #1 <256> 
            drop: Dropout(p=0.2, inplace=false): #0 <> 
          )
        )
        proj: Linear(inFeatures=32, outFeatures=32, bias=true): #2 <1024,32> 
        drop: Dropout(p=0.2, inplace=false): #0 <> 
      )
      ffwd: FeedFoward_3(nEmbed = 32): #4 8352 (
        net: Sequential: #4 8352 (
          0: Linear(inFeatures=32, outFeatures=128, bias=true): #2 <4096,128> 
          1: ReLU: #0 <> 
          2: Linear(inFeatures=128, outFeatures=32, bias=true): #2 <4096,32> 
          3: Dropout(p=0.2, inplace=false): #0 <> 
        )
      )
      ln1: LayerNorm: #2 <32,32> 
      ln2: LayerNorm: #2 <32,32> 
    )
  )
  ln_f: LayerNorm: #2 <32,32> 
  lm_head: Linear(inFeatures=32, outFeatures=65, bias=true): #2 <2080,65> 
)
Device = Device(CUDA,-1)
51585 parameters
learningRate = 1.1E-5
maxIterations = 350000
dropout = 0.2
step 0: train loss 4.3045254, val loss 4.314669, mem 1.2 GiB @ 00 00:00:00.000, mean 00 00:00:00.000
step 500: train loss 3.962249, val loss 3.9812593, mem 1.2 GiB @ 00 00:00:08.786, mean 00 00:00:00.017
step 1000: train loss 3.7184477, val loss 3.7465956, mem 1.2 GiB @ 00 00:00:17.699, mean 00 00:00:00.017
step 1500: train loss 3.5966408, val loss 3.6149566, mem 1.4 GiB @ 00 00:00:26.991, mean 00 00:00:00.018
step 2000: train loss 3.4998796, val loss 3.530551, mem 1.6 GiB @ 00 00:00:36.036, mean 00 00:00:00.018
step 2500: train loss 3.428016, val loss 3.45673, mem 1.8 GiB @ 00 00:00:45.121, mean 00 00:00:00.018
step 3000: train loss 3.3677418, val loss 3.3994873, mem 1.8 GiB @ 00 00:00:54.209, mean 00 00:00:00.018
step 3500: train loss 3.3195245, val loss 3.34061, mem 1.8 GiB @ 00 00:01:03.322, mean 00 00:00:00.018
step 4000: train loss 3.2927997, val loss 3.3516772, mem 1.8 GiB @ 00 00:01:12.325, mean 00 00:00:00.018
step 4500: train loss 3.2607346, val loss 3.296204, mem 1.8 GiB @ 00 00:01:21.087, mean 00 00:00:00.017
step 5000: train loss 3.2049131, val loss 3.2606955, mem 1.8 GiB @ 00 00:01:29.889, mean 00 00:00:00.017
step 5500: train loss 3.187095, val loss 3.2305026, mem 1.8 GiB @ 00 00:01:38.678, mean 00 00:00:00.017
step 6000: train loss 3.146339, val loss 3.1905084, mem 1.8 GiB @ 00 00:01:47.862, mean 00 00:00:00.018
step 6500: train loss 3.1332903, val loss 3.170791, mem 1.8 GiB @ 00 00:01:57.000, mean 00 00:00:00.018
step 7000: train loss 3.1103864, val loss 3.1487112, mem 1.8 GiB @ 00 00:02:06.509, mean 00 00:00:00.019
step 7500: train loss 3.0875974, val loss 3.124414, mem 1.8 GiB @ 00 00:02:15.859, mean 00 00:00:00.018
step 8000: train loss 3.0633512, val loss 3.113753, mem 1.8 GiB @ 00 00:02:25.251, mean 00 00:00:00.018
step 8500: train loss 3.0601344, val loss 3.0960586, mem 1.8 GiB @ 00 00:02:34.501, mean 00 00:00:00.018
step 9000: train loss 3.0494268, val loss 3.0832775, mem 1.8 GiB @ 00 00:02:43.777, mean 00 00:00:00.018
step 9500: train loss 3.032693, val loss 3.0704584, mem 1.8 GiB @ 00 00:02:53.066, mean 00 00:00:00.018
step 10000: train loss 2.9961162, val loss 3.0488482, mem 1.8 GiB @ 00 00:03:02.514, mean 00 00:00:00.018
step 10500: train loss 2.9791956, val loss 3.0248587, mem 1.8 GiB @ 00 00:03:11.827, mean 00 00:00:00.018
step 11000: train loss 2.982979, val loss 2.9917765, mem 1.8 GiB @ 00 00:03:21.440, mean 00 00:00:00.019
step 11500: train loss 2.9508483, val loss 2.9750457, mem 1.8 GiB @ 00 00:03:30.823, mean 00 00:00:00.018
step 12000: train loss 2.9388916, val loss 2.954783, mem 1.8 GiB @ 00 00:03:40.395, mean 00 00:00:00.019
step 12500: train loss 2.9146929, val loss 2.9203343, mem 1.8 GiB @ 00 00:03:49.652, mean 00 00:00:00.018
step 13000: train loss 2.9011028, val loss 2.9382794, mem 1.8 GiB @ 00 00:03:58.952, mean 00 00:00:00.018
step 13500: train loss 2.896031, val loss 2.9094799, mem 1.8 GiB @ 00 00:04:08.537, mean 00 00:00:00.019
step 14000: train loss 2.8909905, val loss 2.9034967, mem 1.8 GiB @ 00 00:04:17.772, mean 00 00:00:00.018
step 14500: train loss 2.8595958, val loss 2.887436, mem 1.8 GiB @ 00 00:04:27.121, mean 00 00:00:00.018
step 15000: train loss 2.8590894, val loss 2.8959527, mem 1.8 GiB @ 00 00:04:36.388, mean 00 00:00:00.018
step 15500: train loss 2.8448129, val loss 2.8939786, mem 1.8 GiB @ 00 00:04:45.592, mean 00 00:00:00.018
step 16000: train loss 2.8505318, val loss 2.8609624, mem 1.8 GiB @ 00 00:04:54.451, mean 00 00:00:00.017
step 16500: train loss 2.819656, val loss 2.8329294, mem 1.8 GiB @ 00 00:05:03.609, mean 00 00:00:00.018
step 17000: train loss 2.7917798, val loss 2.8369734, mem 1.8 GiB @ 00 00:05:12.582, mean 00 00:00:00.017
step 17500: train loss 2.7849975, val loss 2.818342, mem 1.8 GiB @ 00 00:05:21.566, mean 00 00:00:00.017
step 18000: train loss 2.7834344, val loss 2.8119237, mem 1.8 GiB @ 00 00:05:30.692, mean 00 00:00:00.018
step 18500: train loss 2.7812762, val loss 2.8088892, mem 1.8 GiB @ 00 00:05:39.734, mean 00 00:00:00.018
step 19000: train loss 2.7408092, val loss 2.77944, mem 1.8 GiB @ 00 00:05:49.024, mean 00 00:00:00.018
step 19500: train loss 2.7526522, val loss 2.7728975, mem 1.8 GiB @ 00 00:05:58.103, mean 00 00:00:00.018
step 20000: train loss 2.7555776, val loss 2.7535706, mem 1.8 GiB @ 00 00:06:07.263, mean 00 00:00:00.018
step 20500: train loss 2.7361045, val loss 2.7430663, mem 1.8 GiB @ 00 00:06:16.445, mean 00 00:00:00.018
step 21000: train loss 2.7152984, val loss 2.7249315, mem 1.8 GiB @ 00 00:06:25.598, mean 00 00:00:00.018
step 21500: train loss 2.7107768, val loss 2.7321253, mem 1.8 GiB @ 00 00:06:34.562, mean 00 00:00:00.017
step 22000: train loss 2.6918185, val loss 2.7177794, mem 1.8 GiB @ 00 00:06:43.831, mean 00 00:00:00.018
step 22500: train loss 2.6887987, val loss 2.7260406, mem 1.8 GiB @ 00 00:06:53.201, mean 00 00:00:00.018
step 23000: train loss 2.6816041, val loss 2.7001379, mem 1.8 GiB @ 00 00:07:02.240, mean 00 00:00:00.018
step 23500: train loss 2.6846282, val loss 2.6937706, mem 1.8 GiB @ 00 00:07:11.131, mean 00 00:00:00.017
step 24000: train loss 2.6609073, val loss 2.6835928, mem 1.8 GiB @ 00 00:07:20.391, mean 00 00:00:00.018
step 24500: train loss 2.6671007, val loss 2.6776202, mem 1.8 GiB @ 00 00:07:29.709, mean 00 00:00:00.018
step 25000: train loss 2.668149, val loss 2.6771986, mem 1.8 GiB @ 00 00:07:38.809, mean 00 00:00:00.018
step 25500: train loss 2.6573436, val loss 2.6693213, mem 1.8 GiB @ 00 00:07:48.085, mean 00 00:00:00.018
step 26000: train loss 2.6559148, val loss 2.6655736, mem 1.8 GiB @ 00 00:07:57.147, mean 00 00:00:00.018
step 26500: train loss 2.6355624, val loss 2.6535625, mem 1.8 GiB @ 00 00:08:06.321, mean 00 00:00:00.018
step 27000: train loss 2.6265275, val loss 2.620148, mem 1.8 GiB @ 00 00:08:15.515, mean 00 00:00:00.018
step 27500: train loss 2.6201537, val loss 2.643413, mem 1.8 GiB @ 00 00:08:24.704, mean 00 00:00:00.018
step 28000: train loss 2.6101153, val loss 2.622479, mem 1.8 GiB @ 00 00:08:33.880, mean 00 00:00:00.018
step 28500: train loss 2.6040728, val loss 2.6349337, mem 1.8 GiB @ 00 00:08:43.099, mean 00 00:00:00.018
step 29000: train loss 2.604258, val loss 2.6031659, mem 1.8 GiB @ 00 00:08:52.322, mean 00 00:00:00.018
step 29500: train loss 2.5937526, val loss 2.6137261, mem 1.8 GiB @ 00 00:09:01.466, mean 00 00:00:00.018
step 30000: train loss 2.5793512, val loss 2.5961053, mem 1.8 GiB @ 00 00:09:10.636, mean 00 00:00:00.018
step 30500: train loss 2.584823, val loss 2.5831854, mem 1.8 GiB @ 00 00:09:19.655, mean 00 00:00:00.018
step 31000: train loss 2.5755286, val loss 2.5929382, mem 1.8 GiB @ 00 00:09:28.650, mean 00 00:00:00.017
step 31500: train loss 2.558828, val loss 2.6007118, mem 1.8 GiB @ 00 00:09:37.835, mean 00 00:00:00.018
step 32000: train loss 2.5462036, val loss 2.5755062, mem 1.8 GiB @ 00 00:09:46.612, mean 00 00:00:00.017
step 32500: train loss 2.5505881, val loss 2.5706627, mem 1.8 GiB @ 00 00:09:55.372, mean 00 00:00:00.017
step 33000: train loss 2.5595045, val loss 2.5700493, mem 1.8 GiB @ 00 00:10:04.062, mean 00 00:00:00.017
step 33500: train loss 2.5472069, val loss 2.5545723, mem 1.8 GiB @ 00 00:10:13.244, mean 00 00:00:00.018
step 34000: train loss 2.5494704, val loss 2.5541637, mem 1.8 GiB @ 00 00:10:22.171, mean 00 00:00:00.017
step 34500: train loss 2.5487225, val loss 2.5337088, mem 1.8 GiB @ 00 00:10:30.867, mean 00 00:00:00.017
step 35000: train loss 2.5204666, val loss 2.5443077, mem 1.8 GiB @ 00 00:10:40.117, mean 00 00:00:00.018
step 35500: train loss 2.519172, val loss 2.5328171, mem 1.8 GiB @ 00 00:10:48.972, mean 00 00:00:00.017
step 36000: train loss 2.5367346, val loss 2.508619, mem 1.8 GiB @ 00 00:10:58.154, mean 00 00:00:00.018
step 36500: train loss 2.5217311, val loss 2.534766, mem 1.8 GiB @ 00 00:11:07.165, mean 00 00:00:00.018
step 37000: train loss 2.514982, val loss 2.527338, mem 1.8 GiB @ 00 00:11:16.180, mean 00 00:00:00.018
step 37500: train loss 2.5153804, val loss 2.5262442, mem 1.8 GiB @ 00 00:11:25.218, mean 00 00:00:00.018
step 38000: train loss 2.5168517, val loss 2.519032, mem 1.8 GiB @ 00 00:11:34.007, mean 00 00:00:00.017
step 38500: train loss 2.5094092, val loss 2.5145347, mem 1.8 GiB @ 00 00:11:43.228, mean 00 00:00:00.018
step 39000: train loss 2.5050347, val loss 2.5098429, mem 1.8 GiB @ 00 00:11:52.211, mean 00 00:00:00.017
step 39500: train loss 2.5004158, val loss 2.5099573, mem 1.8 GiB @ 00 00:12:01.094, mean 00 00:00:00.017
step 40000: train loss 2.485681, val loss 2.516432, mem 1.8 GiB @ 00 00:12:09.945, mean 00 00:00:00.017
step 40500: train loss 2.4875047, val loss 2.503669, mem 1.8 GiB @ 00 00:12:18.736, mean 00 00:00:00.017
step 41000: train loss 2.484987, val loss 2.4859138, mem 1.8 GiB @ 00 00:12:27.506, mean 00 00:00:00.017
step 41500: train loss 2.5004075, val loss 2.4927242, mem 1.8 GiB @ 00 00:12:36.328, mean 00 00:00:00.017
step 42000: train loss 2.4917302, val loss 2.4930692, mem 1.8 GiB @ 00 00:12:45.339, mean 00 00:00:00.018
step 42500: train loss 2.480325, val loss 2.4882383, mem 1.8 GiB @ 00 00:12:54.537, mean 00 00:00:00.018
step 43000: train loss 2.4679039, val loss 2.490702, mem 1.8 GiB @ 00 00:13:03.725, mean 00 00:00:00.018
step 43500: train loss 2.4789968, val loss 2.494954, mem 1.8 GiB @ 00 00:13:12.908, mean 00 00:00:00.018
step 44000: train loss 2.4833336, val loss 2.4856973, mem 1.8 GiB @ 00 00:13:22.105, mean 00 00:00:00.018
step 44500: train loss 2.4615026, val loss 2.4754183, mem 1.8 GiB @ 00 00:13:31.055, mean 00 00:00:00.017
step 45000: train loss 2.4535375, val loss 2.4890783, mem 1.8 GiB @ 00 00:13:39.855, mean 00 00:00:00.017
step 45500: train loss 2.4727182, val loss 2.478242, mem 1.8 GiB @ 00 00:13:48.718, mean 00 00:00:00.017
step 46000: train loss 2.4555423, val loss 2.4538264, mem 1.8 GiB @ 00 00:13:57.512, mean 00 00:00:00.017
step 46500: train loss 2.4640193, val loss 2.4698913, mem 1.8 GiB @ 00 00:14:06.517, mean 00 00:00:00.018
step 47000: train loss 2.458734, val loss 2.453737, mem 1.8 GiB @ 00 00:14:15.635, mean 00 00:00:00.018
step 47500: train loss 2.4487972, val loss 2.455876, mem 1.8 GiB @ 00 00:14:24.812, mean 00 00:00:00.018
step 48000: train loss 2.4461534, val loss 2.463254, mem 1.8 GiB @ 00 00:14:33.998, mean 00 00:00:00.018
step 48500: train loss 2.4473908, val loss 2.4405074, mem 1.8 GiB @ 00 00:14:43.179, mean 00 00:00:00.018
step 49000: train loss 2.44635, val loss 2.4578834, mem 1.8 GiB @ 00 00:14:51.814, mean 00 00:00:00.017
step 49500: train loss 2.4443252, val loss 2.4503481, mem 1.8 GiB @ 00 00:15:00.812, mean 00 00:00:00.017
step 50000: train loss 2.4270973, val loss 2.4615178, mem 1.8 GiB @ 00 00:15:09.782, mean 00 00:00:00.017
step 50500: train loss 2.4389892, val loss 2.4409835, mem 1.8 GiB @ 00 00:15:18.928, mean 00 00:00:00.018
step 51000: train loss 2.4328222, val loss 2.4496415, mem 1.8 GiB @ 00 00:15:28.121, mean 00 00:00:00.018
step 51500: train loss 2.4377062, val loss 2.44467, mem 1.8 GiB @ 00 00:15:37.227, mean 00 00:00:00.018
step 52000: train loss 2.4482973, val loss 2.4285321, mem 1.8 GiB @ 00 00:15:46.421, mean 00 00:00:00.018
step 52500: train loss 2.453224, val loss 2.4323473, mem 1.8 GiB @ 00 00:15:55.578, mean 00 00:00:00.018
step 53000: train loss 2.4318879, val loss 2.4351141, mem 1.8 GiB @ 00 00:16:04.638, mean 00 00:00:00.018
step 53500: train loss 2.4453828, val loss 2.4357119, mem 1.8 GiB @ 00 00:16:13.867, mean 00 00:00:00.018
step 54000: train loss 2.4225957, val loss 2.4343593, mem 1.8 GiB @ 00 00:16:22.927, mean 00 00:00:00.018
step 54500: train loss 2.426827, val loss 2.4330182, mem 1.8 GiB @ 00 00:16:32.038, mean 00 00:00:00.018
step 55000: train loss 2.4258528, val loss 2.4360516, mem 1.8 GiB @ 00 00:16:40.919, mean 00 00:00:00.017
step 55500: train loss 2.4126654, val loss 2.432748, mem 1.8 GiB @ 00 00:16:50.124, mean 00 00:00:00.018
step 56000: train loss 2.4155343, val loss 2.4290252, mem 1.8 GiB @ 00 00:16:59.107, mean 00 00:00:00.017
step 56500: train loss 2.4227645, val loss 2.425342, mem 1.8 GiB @ 00 00:17:08.298, mean 00 00:00:00.018
step 57000: train loss 2.4170434, val loss 2.4133637, mem 1.8 GiB @ 00 00:17:17.357, mean 00 00:00:00.018
step 57500: train loss 2.4266362, val loss 2.4119945, mem 1.8 GiB @ 00 00:17:26.595, mean 00 00:00:00.018
step 58000: train loss 2.4061093, val loss 2.4164891, mem 1.8 GiB @ 00 00:17:35.653, mean 00 00:00:00.018
step 58500: train loss 2.4088957, val loss 2.414111, mem 1.8 GiB @ 00 00:17:44.562, mean 00 00:00:00.017
step 59000: train loss 2.397879, val loss 2.4101436, mem 1.8 GiB @ 00 00:17:53.384, mean 00 00:00:00.017
step 59500: train loss 2.4009633, val loss 2.4051514, mem 1.8 GiB @ 00 00:18:02.399, mean 00 00:00:00.018
step 60000: train loss 2.4240153, val loss 2.4226534, mem 1.8 GiB @ 00 00:18:11.750, mean 00 00:00:00.018
step 60500: train loss 2.4041486, val loss 2.3986075, mem 1.8 GiB @ 00 00:18:20.921, mean 00 00:00:00.018
step 61000: train loss 2.388499, val loss 2.4016075, mem 1.8 GiB @ 00 00:18:30.043, mean 00 00:00:00.018
step 61500: train loss 2.4088268, val loss 2.4103372, mem 1.8 GiB @ 00 00:18:39.286, mean 00 00:00:00.018
step 62000: train loss 2.3966396, val loss 2.407002, mem 1.8 GiB @ 00 00:18:48.218, mean 00 00:00:00.017
step 62500: train loss 2.3865702, val loss 2.3982882, mem 1.8 GiB @ 00 00:18:57.476, mean 00 00:00:00.018
step 63000: train loss 2.3886192, val loss 2.4039474, mem 1.8 GiB @ 00 00:19:06.650, mean 00 00:00:00.018
step 63500: train loss 2.374945, val loss 2.4073029, mem 1.8 GiB @ 00 00:19:16.002, mean 00 00:00:00.018
step 64000: train loss 2.3720968, val loss 2.3960383, mem 1.8 GiB @ 00 00:19:25.189, mean 00 00:00:00.018
step 64500: train loss 2.3831618, val loss 2.3930843, mem 1.8 GiB @ 00 00:19:34.267, mean 00 00:00:00.018
step 65000: train loss 2.3880408, val loss 2.4016597, mem 1.8 GiB @ 00 00:19:43.429, mean 00 00:00:00.018
step 65500: train loss 2.387772, val loss 2.4082756, mem 1.8 GiB @ 00 00:19:52.713, mean 00 00:00:00.018
step 66000: train loss 2.3744764, val loss 2.3876445, mem 1.8 GiB @ 00 00:20:01.909, mean 00 00:00:00.018
step 66500: train loss 2.3979855, val loss 2.3699188, mem 1.8 GiB @ 00 00:20:11.106, mean 00 00:00:00.018
step 67000: train loss 2.3812103, val loss 2.3696868, mem 1.8 GiB @ 00 00:20:20.243, mean 00 00:00:00.018
step 67500: train loss 2.3880582, val loss 2.3839748, mem 1.8 GiB @ 00 00:20:29.451, mean 00 00:00:00.018
step 68000: train loss 2.3777542, val loss 2.4017005, mem 1.8 GiB @ 00 00:20:38.560, mean 00 00:00:00.018
step 68500: train loss 2.4110255, val loss 2.4135234, mem 1.8 GiB @ 00 00:20:47.697, mean 00 00:00:00.018
step 69000: train loss 2.4123914, val loss 2.4236248, mem 1.8 GiB @ 00 00:20:56.914, mean 00 00:00:00.018
step 69500: train loss 2.400577, val loss 2.4104624, mem 1.8 GiB @ 00 00:21:06.171, mean 00 00:00:00.018
step 70000: train loss 2.3896294, val loss 2.3891342, mem 1.8 GiB @ 00 00:21:15.381, mean 00 00:00:00.018
step 70500: train loss 2.3860526, val loss 2.399292, mem 1.8 GiB @ 00 00:21:24.523, mean 00 00:00:00.018
step 71000: train loss 2.4193823, val loss 2.4133291, mem 1.8 GiB @ 00 00:21:33.650, mean 00 00:00:00.018
step 71500: train loss 2.3825223, val loss 2.3967721, mem 1.8 GiB @ 00 00:21:42.733, mean 00 00:00:00.018
step 72000: train loss 2.3781884, val loss 2.3863466, mem 1.8 GiB @ 00 00:21:51.851, mean 00 00:00:00.018
step 72500: train loss 2.375303, val loss 2.4016345, mem 1.8 GiB @ 00 00:22:00.837, mean 00 00:00:00.017
step 73000: train loss 2.37585, val loss 2.3754256, mem 1.8 GiB @ 00 00:22:10.073, mean 00 00:00:00.018
step 73500: train loss 2.3725119, val loss 2.3946197, mem 1.8 GiB @ 00 00:22:19.249, mean 00 00:00:00.018
step 74000: train loss 2.3942254, val loss 2.3803496, mem 1.8 GiB @ 00 00:22:28.144, mean 00 00:00:00.017
step 74500: train loss 2.3829274, val loss 2.3774376, mem 1.8 GiB @ 00 00:22:37.401, mean 00 00:00:00.018
step 75000: train loss 2.3805275, val loss 2.384128, mem 1.8 GiB @ 00 00:22:46.628, mean 00 00:00:00.018
step 75500: train loss 2.3592043, val loss 2.3653164, mem 1.8 GiB @ 00 00:22:55.807, mean 00 00:00:00.018
step 76000: train loss 2.3713138, val loss 2.3636637, mem 1.8 GiB @ 00 00:23:05.008, mean 00 00:00:00.018
step 76500: train loss 2.373059, val loss 2.3822658, mem 1.8 GiB @ 00 00:23:14.222, mean 00 00:00:00.018
step 77000: train loss 2.3714442, val loss 2.3799403, mem 1.8 GiB @ 00 00:23:23.171, mean 00 00:00:00.017
step 77500: train loss 2.3575497, val loss 2.3717208, mem 1.8 GiB @ 00 00:23:32.085, mean 00 00:00:00.017
step 78000: train loss 2.3500922, val loss 2.3712175, mem 1.8 GiB @ 00 00:23:40.911, mean 00 00:00:00.017
step 78500: train loss 2.3479307, val loss 2.362899, mem 1.8 GiB @ 00 00:23:49.685, mean 00 00:00:00.017
step 79000: train loss 2.3490977, val loss 2.3631897, mem 1.8 GiB @ 00 00:23:58.537, mean 00 00:00:00.017
step 79500: train loss 2.3695612, val loss 2.3633857, mem 1.8 GiB @ 00 00:24:07.480, mean 00 00:00:00.017
step 80000: train loss 2.364263, val loss 2.3544555, mem 1.8 GiB @ 00 00:24:16.636, mean 00 00:00:00.018
step 80500: train loss 2.3603525, val loss 2.346831, mem 1.8 GiB @ 00 00:24:26.049, mean 00 00:00:00.018
step 81000: train loss 2.35514, val loss 2.3368192, mem 1.8 GiB @ 00 00:24:35.298, mean 00 00:00:00.018
step 81500: train loss 2.3565497, val loss 2.3594003, mem 1.8 GiB @ 00 00:24:44.516, mean 00 00:00:00.018
step 82000: train loss 2.3559136, val loss 2.3408394, mem 1.8 GiB @ 00 00:24:53.661, mean 00 00:00:00.018
step 82500: train loss 2.3388186, val loss 2.3576822, mem 1.8 GiB @ 00 00:25:02.881, mean 00 00:00:00.018
step 83000: train loss 2.3491964, val loss 2.3459044, mem 1.8 GiB @ 00 00:25:12.175, mean 00 00:00:00.018
step 83500: train loss 2.3393135, val loss 2.3438396, mem 1.8 GiB @ 00 00:25:21.132, mean 00 00:00:00.017
step 84000: train loss 2.3416402, val loss 2.3530161, mem 1.8 GiB @ 00 00:25:30.186, mean 00 00:00:00.018
step 84500: train loss 2.3374228, val loss 2.3378177, mem 1.8 GiB @ 00 00:25:38.998, mean 00 00:00:00.017
step 85000: train loss 2.3449402, val loss 2.3389063, mem 1.8 GiB @ 00 00:25:48.204, mean 00 00:00:00.018
step 85500: train loss 2.3332279, val loss 2.3419771, mem 1.8 GiB @ 00 00:25:57.149, mean 00 00:00:00.017
step 86000: train loss 2.3409696, val loss 2.3493648, mem 1.8 GiB @ 00 00:26:06.032, mean 00 00:00:00.017
step 86500: train loss 2.3398623, val loss 2.3338945, mem 1.8 GiB @ 00 00:26:14.675, mean 00 00:00:00.017
step 87000: train loss 2.3248346, val loss 2.3366153, mem 1.8 GiB @ 00 00:26:23.443, mean 00 00:00:00.017
step 87500: train loss 2.3379507, val loss 2.3394785, mem 1.8 GiB @ 00 00:26:32.403, mean 00 00:00:00.017
step 88000: train loss 2.3373134, val loss 2.3539865, mem 1.8 GiB @ 00 00:26:41.420, mean 00 00:00:00.018
step 88500: train loss 2.3420508, val loss 2.3408265, mem 1.8 GiB @ 00 00:26:50.530, mean 00 00:00:00.018
step 89000: train loss 2.3310237, val loss 2.3345697, mem 1.8 GiB @ 00 00:26:59.625, mean 00 00:00:00.018
step 89500: train loss 2.338228, val loss 2.338721, mem 1.8 GiB @ 00 00:27:08.770, mean 00 00:00:00.018
step 90000: train loss 2.3668113, val loss 2.3984044, mem 1.8 GiB @ 00 00:27:17.943, mean 00 00:00:00.018
step 90500: train loss 2.479994, val loss 2.4936812, mem 1.8 GiB @ 00 00:27:27.160, mean 00 00:00:00.018
step 91000: train loss 2.4929385, val loss 2.492178, mem 1.8 GiB @ 00 00:27:36.348, mean 00 00:00:00.018
step 91500: train loss 2.4515839, val loss 2.4605312, mem 1.8 GiB @ 00 00:27:45.540, mean 00 00:00:00.018
step 92000: train loss 2.4133205, val loss 2.436567, mem 1.8 GiB @ 00 00:27:54.720, mean 00 00:00:00.018
step 92500: train loss 2.4148428, val loss 2.4136875, mem 1.8 GiB @ 00 00:28:03.924, mean 00 00:00:00.018
step 93000: train loss 2.4042985, val loss 2.4203186, mem 1.8 GiB @ 00 00:28:12.996, mean 00 00:00:00.018
step 93500: train loss 2.3857467, val loss 2.395986, mem 1.8 GiB @ 00 00:28:22.039, mean 00 00:00:00.018
step 94000: train loss 2.3754387, val loss 2.3794732, mem 1.8 GiB @ 00 00:28:30.940, mean 00 00:00:00.017
step 94500: train loss 2.3741498, val loss 2.3784163, mem 1.8 GiB @ 00 00:28:39.766, mean 00 00:00:00.017
step 95000: train loss 2.3580523, val loss 2.3765535, mem 1.8 GiB @ 00 00:28:48.744, mean 00 00:00:00.017
step 95500: train loss 2.364048, val loss 2.3721354, mem 1.8 GiB @ 00 00:28:57.894, mean 00 00:00:00.018
step 96000: train loss 2.3472314, val loss 2.3774655, mem 1.8 GiB @ 00 00:29:07.069, mean 00 00:00:00.018
step 96500: train loss 2.3503945, val loss 2.372013, mem 1.8 GiB @ 00 00:29:16.241, mean 00 00:00:00.018
step 97000: train loss 2.349703, val loss 2.3353407, mem 1.8 GiB @ 00 00:29:25.315, mean 00 00:00:00.018
step 97500: train loss 2.346188, val loss 2.3563888, mem 1.8 GiB @ 00 00:29:34.432, mean 00 00:00:00.018
step 98000: train loss 2.3224905, val loss 2.3513057, mem 1.8 GiB @ 00 00:29:43.634, mean 00 00:00:00.018
step 98500: train loss 2.3376212, val loss 2.3449793, mem 1.8 GiB @ 00 00:29:52.846, mean 00 00:00:00.018
step 99000: train loss 2.3484743, val loss 2.354015, mem 1.8 GiB @ 00 00:30:01.946, mean 00 00:00:00.018
step 99500: train loss 2.3376653, val loss 2.355883, mem 1.8 GiB @ 00 00:30:10.800, mean 00 00:00:00.017
step 100000: train loss 2.3156056, val loss 2.3445995, mem 1.8 GiB @ 00 00:30:19.765, mean 00 00:00:00.017
step 100500: train loss 2.3291805, val loss 2.3386621, mem 1.8 GiB @ 00 00:30:28.980, mean 00 00:00:00.018
step 101000: train loss 2.3358648, val loss 2.329756, mem 1.8 GiB @ 00 00:30:37.838, mean 00 00:00:00.017
step 101500: train loss 2.3319328, val loss 2.3368664, mem 1.8 GiB @ 00 00:30:46.659, mean 00 00:00:00.017
step 102000: train loss 2.3265924, val loss 2.327787, mem 1.8 GiB @ 00 00:30:55.475, mean 00 00:00:00.017
step 102500: train loss 2.3283517, val loss 2.3478541, mem 1.8 GiB @ 00 00:31:04.648, mean 00 00:00:00.018
step 103000: train loss 2.3085272, val loss 2.3270447, mem 1.8 GiB @ 00 00:31:13.842, mean 00 00:00:00.018
step 103500: train loss 2.3182209, val loss 2.3263512, mem 1.8 GiB @ 00 00:31:23.037, mean 00 00:00:00.018
step 104000: train loss 2.3211348, val loss 2.33414, mem 1.8 GiB @ 00 00:31:32.104, mean 00 00:00:00.018
step 104500: train loss 2.3200784, val loss 2.3280966, mem 1.8 GiB @ 00 00:31:41.252, mean 00 00:00:00.018
step 105000: train loss 2.3238585, val loss 2.3276153, mem 1.8 GiB @ 00 00:31:49.945, mean 00 00:00:00.017
step 105500: train loss 2.3194466, val loss 2.3252125, mem 1.8 GiB @ 00 00:31:58.811, mean 00 00:00:00.017
step 106000: train loss 2.3260703, val loss 2.3337903, mem 1.8 GiB @ 00 00:32:08.095, mean 00 00:00:00.018
step 106500: train loss 2.3095467, val loss 2.3189929, mem 1.8 GiB @ 00 00:32:17.043, mean 00 00:00:00.017
step 107000: train loss 2.3170483, val loss 2.319406, mem 1.8 GiB @ 00 00:32:25.798, mean 00 00:00:00.017
step 107500: train loss 2.3161724, val loss 2.3253534, mem 1.8 GiB @ 00 00:32:34.598, mean 00 00:00:00.017
step 108000: train loss 2.326182, val loss 2.3320754, mem 1.8 GiB @ 00 00:32:43.882, mean 00 00:00:00.018
step 108500: train loss 2.3205574, val loss 2.3204792, mem 1.8 GiB @ 00 00:32:53.066, mean 00 00:00:00.018
step 109000: train loss 2.3331249, val loss 2.324501, mem 1.8 GiB @ 00 00:33:01.780, mean 00 00:00:00.017
step 109500: train loss 2.3119748, val loss 2.3404632, mem 1.8 GiB @ 00 00:33:10.586, mean 00 00:00:00.017
step 110000: train loss 2.3048198, val loss 2.3039324, mem 1.8 GiB @ 00 00:33:19.700, mean 00 00:00:00.018
step 110500: train loss 2.3114285, val loss 2.3197784, mem 1.8 GiB @ 00 00:33:28.602, mean 00 00:00:00.017
step 111000: train loss 2.3082654, val loss 2.3282144, mem 1.8 GiB @ 00 00:33:37.384, mean 00 00:00:00.017
step 111500: train loss 2.3067584, val loss 2.3053882, mem 1.8 GiB @ 00 00:33:46.060, mean 00 00:00:00.017
step 112000: train loss 2.3140554, val loss 2.3079135, mem 1.8 GiB @ 00 00:33:54.889, mean 00 00:00:00.017
step 112500: train loss 2.284984, val loss 2.3136423, mem 1.8 GiB @ 00 00:34:03.833, mean 00 00:00:00.017
step 113000: train loss 2.3005965, val loss 2.3193893, mem 1.8 GiB @ 00 00:34:12.579, mean 00 00:00:00.017
step 113500: train loss 2.3068395, val loss 2.2951648, mem 1.8 GiB @ 00 00:34:21.279, mean 00 00:00:00.017
step 114000: train loss 2.308257, val loss 2.3199592, mem 1.8 GiB @ 00 00:34:30.155, mean 00 00:00:00.017
step 114500: train loss 2.2997472, val loss 2.319375, mem 1.8 GiB @ 00 00:34:39.443, mean 00 00:00:00.018
step 115000: train loss 2.293627, val loss 2.3203182, mem 1.8 GiB @ 00 00:34:48.257, mean 00 00:00:00.017
step 115500: train loss 2.3056803, val loss 2.3003592, mem 1.8 GiB @ 00 00:34:57.064, mean 00 00:00:00.017
step 116000: train loss 2.2947147, val loss 2.294565, mem 1.8 GiB @ 00 00:35:05.933, mean 00 00:00:00.017
step 116500: train loss 2.3220599, val loss 2.3070488, mem 1.8 GiB @ 00 00:35:14.843, mean 00 00:00:00.017
step 117000: train loss 2.2904868, val loss 2.2994056, mem 1.8 GiB @ 00 00:35:23.877, mean 00 00:00:00.018
step 117500: train loss 2.2899656, val loss 2.314454, mem 1.8 GiB @ 00 00:35:33.053, mean 00 00:00:00.018
step 118000: train loss 2.2916672, val loss 2.3062384, mem 1.8 GiB @ 00 00:35:42.207, mean 00 00:00:00.018
step 118500: train loss 2.29303, val loss 2.2952569, mem 1.8 GiB @ 00 00:35:51.640, mean 00 00:00:00.018
step 119000: train loss 2.2847357, val loss 2.3018668, mem 1.8 GiB @ 00 00:36:00.857, mean 00 00:00:00.018
step 119500: train loss 2.2948427, val loss 2.2943952, mem 1.8 GiB @ 00 00:36:10.092, mean 00 00:00:00.018
step 120000: train loss 2.2863953, val loss 2.3043425, mem 1.8 GiB @ 00 00:36:19.112, mean 00 00:00:00.018
step 120500: train loss 2.2946794, val loss 2.2841508, mem 1.8 GiB @ 00 00:36:28.021, mean 00 00:00:00.017
step 121000: train loss 2.2792592, val loss 2.3020666, mem 1.8 GiB @ 00 00:36:36.893, mean 00 00:00:00.017
step 121500: train loss 2.279144, val loss 2.3108456, mem 1.8 GiB @ 00 00:36:45.637, mean 00 00:00:00.017
step 122000: train loss 2.2904181, val loss 2.2994199, mem 1.8 GiB @ 00 00:36:54.448, mean 00 00:00:00.017
step 122500: train loss 2.2815435, val loss 2.289592, mem 1.8 GiB @ 00 00:37:03.209, mean 00 00:00:00.017
step 123000: train loss 2.2830436, val loss 2.2914925, mem 1.8 GiB @ 00 00:37:12.054, mean 00 00:00:00.017
step 123500: train loss 2.2783499, val loss 2.2874398, mem 1.8 GiB @ 00 00:37:20.863, mean 00 00:00:00.017
step 124000: train loss 2.2791634, val loss 2.2879198, mem 1.8 GiB @ 00 00:37:29.668, mean 00 00:00:00.017
step 124500: train loss 2.2850032, val loss 2.2904854, mem 1.8 GiB @ 00 00:37:38.505, mean 00 00:00:00.017
step 125000: train loss 2.2973409, val loss 2.2860594, mem 1.8 GiB @ 00 00:37:47.184, mean 00 00:00:00.017
step 125500: train loss 2.2846537, val loss 2.3029308, mem 1.8 GiB @ 00 00:37:55.941, mean 00 00:00:00.017
step 126000: train loss 2.2775261, val loss 2.2782495, mem 1.8 GiB @ 00 00:38:04.931, mean 00 00:00:00.017
step 126500: train loss 2.2850091, val loss 2.2675204, mem 1.8 GiB @ 00 00:38:14.233, mean 00 00:00:00.018
step 127000: train loss 2.283064, val loss 2.2989526, mem 1.8 GiB @ 00 00:38:23.451, mean 00 00:00:00.018
step 127500: train loss 2.2817633, val loss 2.2796006, mem 1.8 GiB @ 00 00:38:32.579, mean 00 00:00:00.018
step 128000: train loss 2.2768981, val loss 2.2852664, mem 1.8 GiB @ 00 00:38:41.695, mean 00 00:00:00.018
step 128500: train loss 2.2708035, val loss 2.294771, mem 1.8 GiB @ 00 00:38:50.536, mean 00 00:00:00.017
step 129000: train loss 2.2706428, val loss 2.2880037, mem 1.8 GiB @ 00 00:38:59.359, mean 00 00:00:00.017
step 129500: train loss 2.2637558, val loss 2.2891142, mem 1.8 GiB @ 00 00:39:08.237, mean 00 00:00:00.017
step 130000: train loss 2.280131, val loss 2.2799609, mem 1.8 GiB @ 00 00:39:17.009, mean 00 00:00:00.017
step 130500: train loss 2.2731261, val loss 2.2577617, mem 1.8 GiB @ 00 00:39:26.173, mean 00 00:00:00.018
step 131000: train loss 2.2796898, val loss 2.2863538, mem 1.8 GiB @ 00 00:39:35.451, mean 00 00:00:00.018
step 131500: train loss 2.2623262, val loss 2.2695012, mem 1.8 GiB @ 00 00:39:44.311, mean 00 00:00:00.017
step 132000: train loss 2.2446282, val loss 2.277843, mem 1.8 GiB @ 00 00:39:53.463, mean 00 00:00:00.018
step 132500: train loss 2.2623029, val loss 2.2809796, mem 1.8 GiB @ 00 00:40:02.380, mean 00 00:00:00.017
step 133000: train loss 2.2644107, val loss 2.276863, mem 1.8 GiB @ 00 00:40:11.447, mean 00 00:00:00.018
step 133500: train loss 2.2810347, val loss 2.2624924, mem 1.8 GiB @ 00 00:40:20.454, mean 00 00:00:00.018
step 134000: train loss 2.26643, val loss 2.278764, mem 1.8 GiB @ 00 00:40:29.636, mean 00 00:00:00.018
step 134500: train loss 2.2590253, val loss 2.279587, mem 1.8 GiB @ 00 00:40:38.780, mean 00 00:00:00.018
step 135000: train loss 2.258147, val loss 2.2800934, mem 1.8 GiB @ 00 00:40:47.810, mean 00 00:00:00.018
step 135500: train loss 2.2660472, val loss 2.272909, mem 1.8 GiB @ 00 00:40:56.818, mean 00 00:00:00.018
step 136000: train loss 2.2721183, val loss 2.2633336, mem 1.8 GiB @ 00 00:41:05.784, mean 00 00:00:00.017
step 136500: train loss 2.2749763, val loss 2.2647324, mem 1.8 GiB @ 00 00:41:14.938, mean 00 00:00:00.018
step 137000: train loss 2.2524903, val loss 2.2575698, mem 1.8 GiB @ 00 00:41:24.039, mean 00 00:00:00.018
step 137500: train loss 2.2637908, val loss 2.2681072, mem 1.8 GiB @ 00 00:41:32.919, mean 00 00:00:00.017
step 138000: train loss 2.264584, val loss 2.2722952, mem 1.8 GiB @ 00 00:41:42.032, mean 00 00:00:00.018
step 138500: train loss 2.244148, val loss 2.2680445, mem 1.8 GiB @ 00 00:41:51.222, mean 00 00:00:00.018
step 139000: train loss 2.2523267, val loss 2.264699, mem 1.8 GiB @ 00 00:42:00.346, mean 00 00:00:00.018
step 139500: train loss 2.2525747, val loss 2.2805781, mem 1.8 GiB @ 00 00:42:09.408, mean 00 00:00:00.018
step 140000: train loss 2.2621644, val loss 2.2887132, mem 1.8 GiB @ 00 00:42:18.246, mean 00 00:00:00.017
step 140500: train loss 2.2483435, val loss 2.2695458, mem 1.8 GiB @ 00 00:42:27.362, mean 00 00:00:00.018
step 141000: train loss 2.2539136, val loss 2.2584608, mem 1.8 GiB @ 00 00:42:36.509, mean 00 00:00:00.018
step 141500: train loss 2.2621644, val loss 2.2525783, mem 1.8 GiB @ 00 00:42:45.634, mean 00 00:00:00.018
step 142000: train loss 2.244747, val loss 2.259071, mem 1.8 GiB @ 00 00:42:54.834, mean 00 00:00:00.018
step 142500: train loss 2.2598932, val loss 2.2702298, mem 1.8 GiB @ 00 00:43:03.856, mean 00 00:00:00.018
step 143000: train loss 2.2464538, val loss 2.2541785, mem 1.8 GiB @ 00 00:43:12.827, mean 00 00:00:00.017
step 143500: train loss 2.2630415, val loss 2.2412746, mem 1.8 GiB @ 00 00:43:21.826, mean 00 00:00:00.017
step 144000: train loss 2.248917, val loss 2.260566, mem 1.8 GiB @ 00 00:43:30.675, mean 00 00:00:00.017
step 144500: train loss 2.253401, val loss 2.2643573, mem 1.8 GiB @ 00 00:43:39.501, mean 00 00:00:00.017
step 145000: train loss 2.2506828, val loss 2.257963, mem 1.8 GiB @ 00 00:43:48.388, mean 00 00:00:00.017
step 145500: train loss 2.2478251, val loss 2.257306, mem 1.8 GiB @ 00 00:43:57.189, mean 00 00:00:00.017
step 146000: train loss 2.2449813, val loss 2.2761872, mem 1.8 GiB @ 00 00:44:06.153, mean 00 00:00:00.017
step 146500: train loss 2.244761, val loss 2.2595322, mem 1.8 GiB @ 00 00:44:15.156, mean 00 00:00:00.018
step 147000: train loss 2.242919, val loss 2.2435815, mem 1.8 GiB @ 00 00:44:24.463, mean 00 00:00:00.018
step 147500: train loss 2.237026, val loss 2.2581017, mem 1.8 GiB @ 00 00:44:33.327, mean 00 00:00:00.017
step 148000: train loss 2.2526355, val loss 2.249218, mem 1.8 GiB @ 00 00:44:42.115, mean 00 00:00:00.017
step 148500: train loss 2.2383335, val loss 2.2594066, mem 1.8 GiB @ 00 00:44:51.076, mean 00 00:00:00.017
step 149000: train loss 2.261193, val loss 2.2589836, mem 1.8 GiB @ 00 00:45:00.291, mean 00 00:00:00.018
step 149500: train loss 2.2476702, val loss 2.2447498, mem 1.8 GiB @ 00 00:45:09.488, mean 00 00:00:00.018
step 150000: train loss 2.2659886, val loss 2.2473218, mem 1.8 GiB @ 00 00:45:18.739, mean 00 00:00:00.018
step 150500: train loss 2.250098, val loss 2.2631829, mem 1.8 GiB @ 00 00:45:27.899, mean 00 00:00:00.018
step 151000: train loss 2.2345405, val loss 2.2505863, mem 1.8 GiB @ 00 00:45:37.031, mean 00 00:00:00.018
step 151500: train loss 2.2380652, val loss 2.2514117, mem 1.8 GiB @ 00 00:45:46.177, mean 00 00:00:00.018
step 152000: train loss 2.2353055, val loss 2.2639837, mem 1.8 GiB @ 00 00:45:55.011, mean 00 00:00:00.017
step 152500: train loss 2.242131, val loss 2.2447264, mem 1.8 GiB @ 00 00:46:03.848, mean 00 00:00:00.017
step 153000: train loss 2.230064, val loss 2.2642362, mem 1.8 GiB @ 00 00:46:13.190, mean 00 00:00:00.018
step 153500: train loss 2.2340646, val loss 2.2427926, mem 1.8 GiB @ 00 00:46:22.251, mean 00 00:00:00.018
step 154000: train loss 2.241551, val loss 2.2526536, mem 1.8 GiB @ 00 00:46:31.009, mean 00 00:00:00.017
step 154500: train loss 2.2223756, val loss 2.2455719, mem 1.8 GiB @ 00 00:46:40.102, mean 00 00:00:00.018
step 155000: train loss 2.220371, val loss 2.2470014, mem 1.8 GiB @ 00 00:46:48.850, mean 00 00:00:00.017
step 155500: train loss 2.226868, val loss 2.2477825, mem 1.8 GiB @ 00 00:46:58.055, mean 00 00:00:00.018
step 156000: train loss 2.2274067, val loss 2.2422323, mem 1.8 GiB @ 00 00:47:07.377, mean 00 00:00:00.018
step 156500: train loss 2.2315361, val loss 2.2569964, mem 1.8 GiB @ 00 00:47:16.655, mean 00 00:00:00.018
step 157000: train loss 2.2221045, val loss 2.2442625, mem 1.8 GiB @ 00 00:47:25.442, mean 00 00:00:00.017
step 157500: train loss 2.2385879, val loss 2.2309086, mem 1.8 GiB @ 00 00:47:34.262, mean 00 00:00:00.017
step 158000: train loss 2.2380075, val loss 2.2308054, mem 1.8 GiB @ 00 00:47:43.046, mean 00 00:00:00.017
step 158500: train loss 2.2382715, val loss 2.2274487, mem 1.8 GiB @ 00 00:47:51.798, mean 00 00:00:00.017
step 159000: train loss 2.2352376, val loss 2.245481, mem 1.8 GiB @ 00 00:48:00.591, mean 00 00:00:00.017
step 159500: train loss 2.2271972, val loss 2.238735, mem 1.8 GiB @ 00 00:48:09.374, mean 00 00:00:00.017
step 160000: train loss 2.2243404, val loss 2.2470627, mem 1.8 GiB @ 00 00:48:18.006, mean 00 00:00:00.017
step 160500: train loss 2.244459, val loss 2.2341943, mem 1.8 GiB @ 00 00:48:26.769, mean 00 00:00:00.017
step 161000: train loss 2.2225165, val loss 2.2394767, mem 1.8 GiB @ 00 00:48:35.541, mean 00 00:00:00.017
step 161500: train loss 2.2380567, val loss 2.2408633, mem 1.8 GiB @ 00 00:48:44.620, mean 00 00:00:00.018
step 162000: train loss 2.20272, val loss 2.2585175, mem 1.8 GiB @ 00 00:48:53.845, mean 00 00:00:00.018
step 162500: train loss 2.2302284, val loss 2.2332437, mem 1.8 GiB @ 00 00:49:03.252, mean 00 00:00:00.018
step 163000: train loss 2.221584, val loss 2.2376711, mem 1.8 GiB @ 00 00:49:12.211, mean 00 00:00:00.017
step 163500: train loss 2.2158349, val loss 2.242784, mem 1.8 GiB @ 00 00:49:21.293, mean 00 00:00:00.018
step 164000: train loss 2.2343085, val loss 2.2537048, mem 1.8 GiB @ 00 00:49:30.727, mean 00 00:00:00.018
step 164500: train loss 2.2298186, val loss 2.2380083, mem 1.8 GiB @ 00 00:49:39.984, mean 00 00:00:00.018
step 165000: train loss 2.2190225, val loss 2.2358463, mem 1.8 GiB @ 00 00:49:48.835, mean 00 00:00:00.017
step 165500: train loss 2.2056966, val loss 2.2311802, mem 1.8 GiB @ 00 00:49:57.613, mean 00 00:00:00.017
step 166000: train loss 2.2078934, val loss 2.2360399, mem 1.8 GiB @ 00 00:50:06.319, mean 00 00:00:00.017
step 166500: train loss 2.2271874, val loss 2.256246, mem 1.8 GiB @ 00 00:50:15.118, mean 00 00:00:00.017
step 167000: train loss 2.2151678, val loss 2.2382114, mem 1.8 GiB @ 00 00:50:23.913, mean 00 00:00:00.017
step 167500: train loss 2.2182565, val loss 2.2399428, mem 1.8 GiB @ 00 00:50:32.727, mean 00 00:00:00.017
step 168000: train loss 2.2347605, val loss 2.2514822, mem 1.8 GiB @ 00 00:50:41.517, mean 00 00:00:00.017
step 168500: train loss 2.2219014, val loss 2.2364805, mem 1.8 GiB @ 00 00:50:50.271, mean 00 00:00:00.017
step 169000: train loss 2.2101114, val loss 2.2427542, mem 1.8 GiB @ 00 00:50:59.212, mean 00 00:00:00.017
step 169500: train loss 2.224766, val loss 2.2394412, mem 1.8 GiB @ 00 00:51:08.632, mean 00 00:00:00.018
step 170000: train loss 2.2228494, val loss 2.232876, mem 1.8 GiB @ 00 00:51:17.818, mean 00 00:00:00.018
step 170500: train loss 2.228859, val loss 2.2328434, mem 1.8 GiB @ 00 00:51:27.014, mean 00 00:00:00.018
step 171000: train loss 2.2095723, val loss 2.2261574, mem 1.8 GiB @ 00 00:51:36.229, mean 00 00:00:00.018
step 171500: train loss 2.2248604, val loss 2.2340093, mem 1.8 GiB @ 00 00:51:45.119, mean 00 00:00:00.017
step 172000: train loss 2.2097442, val loss 2.23852, mem 1.8 GiB @ 00 00:51:54.279, mean 00 00:00:00.018
step 172500: train loss 2.2119098, val loss 2.237702, mem 1.8 GiB @ 00 00:52:03.366, mean 00 00:00:00.018
step 173000: train loss 2.2161357, val loss 2.2266586, mem 1.8 GiB @ 00 00:52:12.322, mean 00 00:00:00.017
step 173500: train loss 2.2139108, val loss 2.2244544, mem 1.8 GiB @ 00 00:52:21.302, mean 00 00:00:00.017
step 174000: train loss 2.217164, val loss 2.2184925, mem 1.8 GiB @ 00 00:52:30.266, mean 00 00:00:00.017
step 174500: train loss 2.21477, val loss 2.2294357, mem 1.8 GiB @ 00 00:52:39.046, mean 00 00:00:00.017
step 175000: train loss 2.2097678, val loss 2.2288797, mem 1.8 GiB @ 00 00:52:47.845, mean 00 00:00:00.017
step 175500: train loss 2.2116416, val loss 2.2053468, mem 1.8 GiB @ 00 00:52:56.912, mean 00 00:00:00.018
step 176000: train loss 2.2220209, val loss 2.229594, mem 1.8 GiB @ 00 00:53:05.732, mean 00 00:00:00.017
step 176500: train loss 2.217209, val loss 2.224298, mem 1.8 GiB @ 00 00:53:14.595, mean 00 00:00:00.017
step 177000: train loss 2.2042823, val loss 2.241498, mem 1.8 GiB @ 00 00:53:23.658, mean 00 00:00:00.018
step 177500: train loss 2.2178087, val loss 2.2146213, mem 1.8 GiB @ 00 00:53:32.400, mean 00 00:00:00.017
step 178000: train loss 2.2190006, val loss 2.2178807, mem 1.8 GiB @ 00 00:53:41.114, mean 00 00:00:00.017
step 178500: train loss 2.212828, val loss 2.2247667, mem 1.8 GiB @ 00 00:53:49.890, mean 00 00:00:00.017
step 179000: train loss 2.1902785, val loss 2.233006, mem 1.8 GiB @ 00 00:53:58.731, mean 00 00:00:00.017
step 179500: train loss 2.2046492, val loss 2.208494, mem 1.8 GiB @ 00 00:54:07.879, mean 00 00:00:00.018
step 180000: train loss 2.2096364, val loss 2.2154152, mem 1.8 GiB @ 00 00:54:17.132, mean 00 00:00:00.018
step 180500: train loss 2.2132204, val loss 2.2201092, mem 1.8 GiB @ 00 00:54:26.400, mean 00 00:00:00.018
step 181000: train loss 2.214775, val loss 2.225133, mem 1.8 GiB @ 00 00:54:35.483, mean 00 00:00:00.018
step 181500: train loss 2.203611, val loss 2.2239237, mem 1.8 GiB @ 00 00:54:44.561, mean 00 00:00:00.018
step 182000: train loss 2.2000477, val loss 2.224534, mem 1.8 GiB @ 00 00:54:53.699, mean 00 00:00:00.018
step 182500: train loss 2.2190516, val loss 2.219711, mem 1.8 GiB @ 00 00:55:02.902, mean 00 00:00:00.018
step 183000: train loss 2.2035632, val loss 2.222191, mem 1.8 GiB @ 00 00:55:12.146, mean 00 00:00:00.018
step 183500: train loss 2.1748552, val loss 2.2123697, mem 1.8 GiB @ 00 00:55:21.350, mean 00 00:00:00.018
step 184000: train loss 2.2024126, val loss 2.2307532, mem 1.8 GiB @ 00 00:55:30.234, mean 00 00:00:00.017
step 184500: train loss 2.202162, val loss 2.2077405, mem 1.8 GiB @ 00 00:55:39.129, mean 00 00:00:00.017
step 185000: train loss 2.1944609, val loss 2.2165427, mem 1.8 GiB @ 00 00:55:47.911, mean 00 00:00:00.017
step 185500: train loss 2.2050982, val loss 2.2069, mem 1.8 GiB @ 00 00:55:56.822, mean 00 00:00:00.017
step 186000: train loss 2.201103, val loss 2.225382, mem 1.8 GiB @ 00 00:56:05.998, mean 00 00:00:00.018
step 186500: train loss 2.2104933, val loss 2.22406, mem 1.8 GiB @ 00 00:56:15.141, mean 00 00:00:00.018
step 187000: train loss 2.2005744, val loss 2.2261658, mem 1.8 GiB @ 00 00:56:23.915, mean 00 00:00:00.017
step 187500: train loss 2.201897, val loss 2.2233446, mem 1.8 GiB @ 00 00:56:33.269, mean 00 00:00:00.018
step 188000: train loss 2.1998198, val loss 2.2122955, mem 1.8 GiB @ 00 00:56:42.659, mean 00 00:00:00.018
step 188500: train loss 2.2000144, val loss 2.2198834, mem 1.8 GiB @ 00 00:56:51.833, mean 00 00:00:00.018
step 189000: train loss 2.1964946, val loss 2.210254, mem 1.8 GiB @ 00 00:57:00.729, mean 00 00:00:00.017
step 189500: train loss 2.1969855, val loss 2.229811, mem 1.8 GiB @ 00 00:57:10.090, mean 00 00:00:00.018
step 190000: train loss 2.1847515, val loss 2.2083325, mem 1.8 GiB @ 00 00:57:19.394, mean 00 00:00:00.018
step 190500: train loss 2.1995828, val loss 2.2045043, mem 1.8 GiB @ 00 00:57:28.643, mean 00 00:00:00.018
step 191000: train loss 2.2006316, val loss 2.205045, mem 1.8 GiB @ 00 00:57:37.574, mean 00 00:00:00.017
step 191500: train loss 2.206271, val loss 2.2248604, mem 1.8 GiB @ 00 00:57:46.691, mean 00 00:00:00.018
step 192000: train loss 2.1708598, val loss 2.2256603, mem 1.8 GiB @ 00 00:57:55.852, mean 00 00:00:00.018
step 192500: train loss 2.198578, val loss 2.2174742, mem 1.8 GiB @ 00 00:58:05.137, mean 00 00:00:00.018
step 193000: train loss 2.1930268, val loss 2.223297, mem 1.8 GiB @ 00 00:58:14.358, mean 00 00:00:00.018
step 193500: train loss 2.2089226, val loss 2.2069838, mem 1.8 GiB @ 00 00:58:23.632, mean 00 00:00:00.018
step 194000: train loss 2.1981115, val loss 2.2201333, mem 1.8 GiB @ 00 00:58:33.108, mean 00 00:00:00.018
step 194500: train loss 2.1913965, val loss 2.2192435, mem 1.8 GiB @ 00 00:58:42.630, mean 00 00:00:00.019
step 195000: train loss 2.2046354, val loss 2.2099342, mem 1.8 GiB @ 00 00:58:51.804, mean 00 00:00:00.018
step 195500: train loss 2.1987445, val loss 2.2156715, mem 1.8 GiB @ 00 00:59:00.511, mean 00 00:00:00.017
step 196000: train loss 2.197837, val loss 2.2087898, mem 1.8 GiB @ 00 00:59:09.302, mean 00 00:00:00.017
step 196500: train loss 2.1958709, val loss 2.2021554, mem 1.8 GiB @ 00 00:59:18.312, mean 00 00:00:00.018
step 197000: train loss 2.206034, val loss 2.2133057, mem 1.8 GiB @ 00 00:59:27.469, mean 00 00:00:00.018
step 197500: train loss 2.1932266, val loss 2.2082841, mem 1.8 GiB @ 00 00:59:36.711, mean 00 00:00:00.018
step 198000: train loss 2.1812897, val loss 2.209992, mem 1.8 GiB @ 00 00:59:45.959, mean 00 00:00:00.018
step 198500: train loss 2.1986234, val loss 2.2101367, mem 1.8 GiB @ 00 00:59:55.166, mean 00 00:00:00.018
step 199000: train loss 2.1777446, val loss 2.204287, mem 1.8 GiB @ 00 01:00:03.889, mean 00 00:00:00.017
step 199500: train loss 2.1960301, val loss 2.2051423, mem 1.8 GiB @ 00 01:00:13.057, mean 00 00:00:00.018
step 200000: train loss 2.1917768, val loss 2.2157989, mem 1.8 GiB @ 00 01:00:22.003, mean 00 00:00:00.017
step 200500: train loss 2.1870959, val loss 2.2031314, mem 1.8 GiB @ 00 01:00:31.113, mean 00 00:00:00.018
step 201000: train loss 2.181379, val loss 2.1884868, mem 1.8 GiB @ 00 01:00:40.357, mean 00 00:00:00.018
step 201500: train loss 2.1940966, val loss 2.2064936, mem 1.8 GiB @ 00 01:00:49.772, mean 00 00:00:00.018
step 202000: train loss 2.1803603, val loss 2.2120702, mem 1.8 GiB @ 00 01:00:59.006, mean 00 00:00:00.018
step 202500: train loss 2.1814182, val loss 2.2260342, mem 1.8 GiB @ 00 01:01:08.150, mean 00 00:00:00.018
step 203000: train loss 2.1769323, val loss 2.2056153, mem 1.8 GiB @ 00 01:01:17.388, mean 00 00:00:00.018
step 203500: train loss 2.1883242, val loss 2.200919, mem 1.8 GiB @ 00 01:01:26.516, mean 00 00:00:00.018
step 204000: train loss 2.1749675, val loss 2.2131817, mem 1.8 GiB @ 00 01:01:35.732, mean 00 00:00:00.018
step 204500: train loss 2.1829326, val loss 2.2024715, mem 1.8 GiB @ 00 01:01:44.557, mean 00 00:00:00.017
step 205000: train loss 2.1832726, val loss 2.1967723, mem 1.8 GiB @ 00 01:01:53.638, mean 00 00:00:00.018
step 205500: train loss 2.1788602, val loss 2.2010403, mem 1.8 GiB @ 00 01:02:02.611, mean 00 00:00:00.017
step 206000: train loss 2.1956704, val loss 2.1947405, mem 1.8 GiB @ 00 01:02:11.802, mean 00 00:00:00.018
step 206500: train loss 2.1843126, val loss 2.2007024, mem 1.8 GiB @ 00 01:02:20.929, mean 00 00:00:00.018
step 207000: train loss 2.1818435, val loss 2.1945217, mem 1.8 GiB @ 00 01:02:29.745, mean 00 00:00:00.017
step 207500: train loss 2.1662076, val loss 2.1983469, mem 1.8 GiB @ 00 01:02:39.058, mean 00 00:00:00.018
step 208000: train loss 2.1843524, val loss 2.1957052, mem 1.8 GiB @ 00 01:02:48.152, mean 00 00:00:00.018
step 208500: train loss 2.1830218, val loss 2.2147713, mem 1.8 GiB @ 00 01:02:57.183, mean 00 00:00:00.018
step 209000: train loss 2.1779196, val loss 2.1984036, mem 1.8 GiB @ 00 01:03:06.281, mean 00 00:00:00.018
step 209500: train loss 2.175778, val loss 2.192527, mem 1.8 GiB @ 00 01:03:15.349, mean 00 00:00:00.018
step 210000: train loss 2.1841998, val loss 2.204134, mem 1.8 GiB @ 00 01:03:24.531, mean 00 00:00:00.018
step 210500: train loss 2.195351, val loss 2.2121353, mem 1.8 GiB @ 00 01:03:33.737, mean 00 00:00:00.018
step 211000: train loss 2.1772335, val loss 2.1943865, mem 1.8 GiB @ 00 01:03:42.752, mean 00 00:00:00.018
step 211500: train loss 2.1947057, val loss 2.2046285, mem 1.8 GiB @ 00 01:03:51.570, mean 00 00:00:00.017
step 212000: train loss 2.18128, val loss 2.1894982, mem 1.8 GiB @ 00 01:04:00.779, mean 00 00:00:00.018
step 212500: train loss 2.183993, val loss 2.1985176, mem 1.8 GiB @ 00 01:04:09.995, mean 00 00:00:00.018
step 213000: train loss 2.1714227, val loss 2.1981335, mem 1.8 GiB @ 00 01:04:19.210, mean 00 00:00:00.018
step 213500: train loss 2.1806085, val loss 2.198063, mem 1.8 GiB @ 00 01:04:28.409, mean 00 00:00:00.018
step 214000: train loss 2.1867282, val loss 2.1932485, mem 1.8 GiB @ 00 01:04:37.665, mean 00 00:00:00.018
step 214500: train loss 2.1753843, val loss 2.1880128, mem 1.8 GiB @ 00 01:04:46.657, mean 00 00:00:00.017
step 215000: train loss 2.1735146, val loss 2.18608, mem 1.8 GiB @ 00 01:04:55.841, mean 00 00:00:00.018
step 215500: train loss 2.183775, val loss 2.1944392, mem 1.8 GiB @ 00 01:05:05.124, mean 00 00:00:00.018
step 216000: train loss 2.1783292, val loss 2.2075899, mem 1.8 GiB @ 00 01:05:14.278, mean 00 00:00:00.018
step 216500: train loss 2.1679099, val loss 2.1994603, mem 1.8 GiB @ 00 01:05:23.501, mean 00 00:00:00.018
step 217000: train loss 2.1693316, val loss 2.2056131, mem 1.8 GiB @ 00 01:05:32.571, mean 00 00:00:00.018
step 217500: train loss 2.1794567, val loss 2.1903534, mem 1.8 GiB @ 00 01:05:41.620, mean 00 00:00:00.018
step 218000: train loss 2.1650429, val loss 2.1870751, mem 1.8 GiB @ 00 01:05:50.791, mean 00 00:00:00.018
step 218500: train loss 2.1886458, val loss 2.1915936, mem 1.8 GiB @ 00 01:06:00.028, mean 00 00:00:00.018
step 219000: train loss 2.174895, val loss 2.1951888, mem 1.8 GiB @ 00 01:06:09.124, mean 00 00:00:00.018
step 219500: train loss 2.1797209, val loss 2.2019775, mem 1.8 GiB @ 00 01:06:18.402, mean 00 00:00:00.018
step 220000: train loss 2.1711543, val loss 2.1906848, mem 1.8 GiB @ 00 01:06:27.588, mean 00 00:00:00.018
step 220500: train loss 2.1729188, val loss 2.207577, mem 1.8 GiB @ 00 01:06:36.545, mean 00 00:00:00.017
step 221000: train loss 2.1722436, val loss 2.18033, mem 1.8 GiB @ 00 01:06:45.787, mean 00 00:00:00.018
step 221500: train loss 2.1628914, val loss 2.1866918, mem 1.8 GiB @ 00 01:06:55.037, mean 00 00:00:00.018
step 222000: train loss 2.171052, val loss 2.1896343, mem 1.8 GiB @ 00 01:07:04.283, mean 00 00:00:00.018
step 222500: train loss 2.1592178, val loss 2.190423, mem 1.8 GiB @ 00 01:07:13.490, mean 00 00:00:00.018
step 223000: train loss 2.1814697, val loss 2.2139738, mem 1.8 GiB @ 00 01:07:22.424, mean 00 00:00:00.017
step 223500: train loss 2.1833117, val loss 2.188149, mem 1.8 GiB @ 00 01:07:31.342, mean 00 00:00:00.017
step 224000: train loss 2.1798496, val loss 2.1810699, mem 1.8 GiB @ 00 01:07:40.529, mean 00 00:00:00.018
step 224500: train loss 2.1734583, val loss 2.190276, mem 1.8 GiB @ 00 01:07:49.659, mean 00 00:00:00.018
step 225000: train loss 2.1555052, val loss 2.2045133, mem 1.8 GiB @ 00 01:07:58.853, mean 00 00:00:00.018
step 225500: train loss 2.1617303, val loss 2.1815307, mem 1.8 GiB @ 00 01:08:07.816, mean 00 00:00:00.017
step 226000: train loss 2.1538713, val loss 2.1899908, mem 1.8 GiB @ 00 01:08:17.211, mean 00 00:00:00.018
step 226500: train loss 2.1610196, val loss 2.1769187, mem 1.8 GiB @ 00 01:08:26.510, mean 00 00:00:00.018
step 227000: train loss 2.1615264, val loss 2.191258, mem 1.8 GiB @ 00 01:08:35.501, mean 00 00:00:00.017
step 227500: train loss 2.174263, val loss 2.1892383, mem 1.8 GiB @ 00 01:08:44.730, mean 00 00:00:00.018
step 228000: train loss 2.1810176, val loss 2.1870062, mem 1.8 GiB @ 00 01:08:53.862, mean 00 00:00:00.018
step 228500: train loss 2.175811, val loss 2.182242, mem 1.8 GiB @ 00 01:09:03.029, mean 00 00:00:00.018
step 229000: train loss 2.1616435, val loss 2.1845672, mem 1.8 GiB @ 00 01:09:12.222, mean 00 00:00:00.018
step 229500: train loss 2.1764178, val loss 2.1835828, mem 1.8 GiB @ 00 01:09:21.315, mean 00 00:00:00.018
step 230000: train loss 2.16541, val loss 2.1755276, mem 1.8 GiB @ 00 01:09:30.379, mean 00 00:00:00.018
step 230500: train loss 2.1593666, val loss 2.1846251, mem 1.8 GiB @ 00 01:09:39.371, mean 00 00:00:00.017
step 231000: train loss 2.173137, val loss 2.172748, mem 1.8 GiB @ 00 01:09:48.423, mean 00 00:00:00.018
step 231500: train loss 2.180749, val loss 2.202887, mem 1.8 GiB @ 00 01:09:57.376, mean 00 00:00:00.017
step 232000: train loss 2.1654222, val loss 2.1847854, mem 1.8 GiB @ 00 01:10:06.681, mean 00 00:00:00.018
step 232500: train loss 2.1686301, val loss 2.1893816, mem 1.8 GiB @ 00 01:10:15.620, mean 00 00:00:00.017
step 233000: train loss 2.163755, val loss 2.1716998, mem 1.8 GiB @ 00 01:10:24.924, mean 00 00:00:00.018
step 233500: train loss 2.1721113, val loss 2.178675, mem 1.8 GiB @ 00 01:10:34.177, mean 00 00:00:00.018
step 234000: train loss 2.163556, val loss 2.1819234, mem 1.8 GiB @ 00 01:10:43.458, mean 00 00:00:00.018
step 234500: train loss 2.163226, val loss 2.1856885, mem 1.8 GiB @ 00 01:10:52.537, mean 00 00:00:00.018
step 235000: train loss 2.173277, val loss 2.189319, mem 1.8 GiB @ 00 01:11:01.731, mean 00 00:00:00.018
step 235500: train loss 2.1699533, val loss 2.1665695, mem 1.8 GiB @ 00 01:11:10.969, mean 00 00:00:00.018
step 236000: train loss 2.1649454, val loss 2.200215, mem 1.8 GiB @ 00 01:11:20.161, mean 00 00:00:00.018
step 236500: train loss 2.1624436, val loss 2.201143, mem 1.8 GiB @ 00 01:11:29.026, mean 00 00:00:00.017
step 237000: train loss 2.1495454, val loss 2.1830354, mem 1.8 GiB @ 00 01:11:37.826, mean 00 00:00:00.017
step 237500: train loss 2.1533077, val loss 2.1704288, mem 1.8 GiB @ 00 01:11:47.012, mean 00 00:00:00.018
step 238000: train loss 2.1707664, val loss 2.1864734, mem 1.8 GiB @ 00 01:11:56.002, mean 00 00:00:00.017
step 238500: train loss 2.1528409, val loss 2.1779292, mem 1.8 GiB @ 00 01:12:04.720, mean 00 00:00:00.017
step 239000: train loss 2.1740746, val loss 2.1864812, mem 1.8 GiB @ 00 01:12:13.599, mean 00 00:00:00.017
step 239500: train loss 2.1555157, val loss 2.1933684, mem 1.8 GiB @ 00 01:12:22.802, mean 00 00:00:00.018
step 240000: train loss 2.1555703, val loss 2.1826591, mem 1.8 GiB @ 00 01:12:32.000, mean 00 00:00:00.018
step 240500: train loss 2.1698434, val loss 2.1887493, mem 1.8 GiB @ 00 01:12:41.213, mean 00 00:00:00.018
step 241000: train loss 2.1607132, val loss 2.175546, mem 1.8 GiB @ 00 01:12:50.148, mean 00 00:00:00.017
step 241500: train loss 2.1696835, val loss 2.171949, mem 1.8 GiB @ 00 01:12:59.238, mean 00 00:00:00.018
step 242000: train loss 2.1649451, val loss 2.1812754, mem 1.8 GiB @ 00 01:13:08.159, mean 00 00:00:00.017
step 242500: train loss 2.148407, val loss 2.1811068, mem 1.8 GiB @ 00 01:13:16.907, mean 00 00:00:00.017
step 243000: train loss 2.15287, val loss 2.1725402, mem 1.8 GiB @ 00 01:13:25.782, mean 00 00:00:00.017
step 243500: train loss 2.1508553, val loss 2.1804116, mem 1.8 GiB @ 00 01:13:34.699, mean 00 00:00:00.017
step 244000: train loss 2.141847, val loss 2.1763723, mem 1.8 GiB @ 00 01:13:43.902, mean 00 00:00:00.018
step 244500: train loss 2.149061, val loss 2.1702669, mem 1.8 GiB @ 00 01:13:53.054, mean 00 00:00:00.018
step 245000: train loss 2.1484075, val loss 2.1840339, mem 1.8 GiB @ 00 01:14:02.165, mean 00 00:00:00.018
step 245500: train loss 2.1609578, val loss 2.1799126, mem 1.8 GiB @ 00 01:14:11.186, mean 00 00:00:00.018
step 246000: train loss 2.1575692, val loss 2.1789796, mem 1.8 GiB @ 00 01:14:20.292, mean 00 00:00:00.018
step 246500: train loss 2.1514242, val loss 2.1645496, mem 1.8 GiB @ 00 01:14:29.515, mean 00 00:00:00.018
step 247000: train loss 2.1521568, val loss 2.18717, mem 1.8 GiB @ 00 01:14:38.426, mean 00 00:00:00.017
step 247500: train loss 2.1443343, val loss 2.1881812, mem 1.8 GiB @ 00 01:14:47.193, mean 00 00:00:00.017
step 248000: train loss 2.1637473, val loss 2.1836183, mem 1.8 GiB @ 00 01:14:55.952, mean 00 00:00:00.017
step 248500: train loss 2.1490052, val loss 2.1832023, mem 1.8 GiB @ 00 01:15:05.105, mean 00 00:00:00.018
step 249000: train loss 2.161221, val loss 2.1716778, mem 1.8 GiB @ 00 01:15:14.275, mean 00 00:00:00.018
step 249500: train loss 2.1441596, val loss 2.1718464, mem 1.8 GiB @ 00 01:15:23.350, mean 00 00:00:00.018
step 250000: train loss 2.1594007, val loss 2.1754994, mem 1.8 GiB @ 00 01:15:32.571, mean 00 00:00:00.018
step 250500: train loss 2.1370142, val loss 2.1792545, mem 1.8 GiB @ 00 01:15:41.812, mean 00 00:00:00.018
step 251000: train loss 2.1545932, val loss 2.1982584, mem 1.8 GiB @ 00 01:15:50.766, mean 00 00:00:00.017
step 251500: train loss 2.1550982, val loss 2.1734924, mem 1.8 GiB @ 00 01:16:00.070, mean 00 00:00:00.018
step 252000: train loss 2.1600544, val loss 2.1609652, mem 1.8 GiB @ 00 01:16:09.269, mean 00 00:00:00.018
step 252500: train loss 2.158285, val loss 2.1592264, mem 1.8 GiB @ 00 01:16:18.094, mean 00 00:00:00.017
step 253000: train loss 2.141052, val loss 2.1899173, mem 1.8 GiB @ 00 01:16:27.340, mean 00 00:00:00.018
step 253500: train loss 2.1409657, val loss 2.1645067, mem 1.8 GiB @ 00 01:16:36.510, mean 00 00:00:00.018
step 254000: train loss 2.1467655, val loss 2.1917365, mem 1.8 GiB @ 00 01:16:45.717, mean 00 00:00:00.018
step 254500: train loss 2.1430707, val loss 2.1694524, mem 1.8 GiB @ 00 01:16:54.758, mean 00 00:00:00.018
step 255000: train loss 2.1501472, val loss 2.1823115, mem 1.8 GiB @ 00 01:17:03.942, mean 00 00:00:00.018
step 255500: train loss 2.1449893, val loss 2.169296, mem 1.8 GiB @ 00 01:17:13.144, mean 00 00:00:00.018
step 256000: train loss 2.1585445, val loss 2.1785607, mem 1.8 GiB @ 00 01:17:22.232, mean 00 00:00:00.018
step 256500: train loss 2.1376607, val loss 2.1687155, mem 1.8 GiB @ 00 01:17:31.434, mean 00 00:00:00.018
step 257000: train loss 2.1414516, val loss 2.1842048, mem 1.8 GiB @ 00 01:17:40.628, mean 00 00:00:00.018
step 257500: train loss 2.1432877, val loss 2.1750283, mem 1.8 GiB @ 00 01:17:49.713, mean 00 00:00:00.018
step 258000: train loss 2.1552901, val loss 2.1744301, mem 1.8 GiB @ 00 01:17:58.910, mean 00 00:00:00.018
step 258500: train loss 2.148321, val loss 2.1746168, mem 1.8 GiB @ 00 01:18:08.142, mean 00 00:00:00.018
step 259000: train loss 2.1519213, val loss 2.1931891, mem 1.8 GiB @ 00 01:18:17.337, mean 00 00:00:00.018
step 259500: train loss 2.1626053, val loss 2.1838036, mem 1.8 GiB @ 00 01:18:26.218, mean 00 00:00:00.017
step 260000: train loss 2.1409178, val loss 2.1648223, mem 1.8 GiB @ 00 01:18:35.457, mean 00 00:00:00.018
step 260500: train loss 2.1567109, val loss 2.1836202, mem 1.8 GiB @ 00 01:18:44.570, mean 00 00:00:00.018
step 261000: train loss 2.156696, val loss 2.1586971, mem 1.8 GiB @ 00 01:18:53.590, mean 00 00:00:00.018
step 261500: train loss 2.1463497, val loss 2.1730363, mem 1.8 GiB @ 00 01:19:02.446, mean 00 00:00:00.017
step 262000: train loss 2.1535463, val loss 2.1630063, mem 1.8 GiB @ 00 01:19:11.237, mean 00 00:00:00.017
step 262500: train loss 2.1389513, val loss 2.1905751, mem 1.8 GiB @ 00 01:19:20.000, mean 00 00:00:00.017
step 263000: train loss 2.146183, val loss 2.1793745, mem 1.8 GiB @ 00 01:19:28.825, mean 00 00:00:00.017
step 263500: train loss 2.1372423, val loss 2.171799, mem 1.8 GiB @ 00 01:19:37.531, mean 00 00:00:00.017
step 264000: train loss 2.1534328, val loss 2.1707623, mem 1.8 GiB @ 00 01:19:46.572, mean 00 00:00:00.018
step 264500: train loss 2.1403215, val loss 2.1676106, mem 1.8 GiB @ 00 01:19:55.736, mean 00 00:00:00.018
step 265000: train loss 2.1638954, val loss 2.1813545, mem 1.8 GiB @ 00 01:20:04.839, mean 00 00:00:00.018
step 265500: train loss 2.1346862, val loss 2.171424, mem 1.8 GiB @ 00 01:20:13.794, mean 00 00:00:00.017
step 266000: train loss 2.1463315, val loss 2.1675272, mem 1.8 GiB @ 00 01:20:22.932, mean 00 00:00:00.018
step 266500: train loss 2.142774, val loss 2.165647, mem 1.8 GiB @ 00 01:20:31.953, mean 00 00:00:00.018
step 267000: train loss 2.146706, val loss 2.1667523, mem 1.8 GiB @ 00 01:20:40.724, mean 00 00:00:00.017
step 267500: train loss 2.1177733, val loss 2.1781347, mem 1.8 GiB @ 00 01:20:49.580, mean 00 00:00:00.017
step 268000: train loss 2.1457376, val loss 2.1627474, mem 1.8 GiB @ 00 01:20:58.477, mean 00 00:00:00.017
step 268500: train loss 2.1336098, val loss 2.1800718, mem 1.8 GiB @ 00 01:21:07.319, mean 00 00:00:00.017
step 269000: train loss 2.1511698, val loss 2.1730394, mem 1.8 GiB @ 00 01:21:16.116, mean 00 00:00:00.017
step 269500: train loss 2.1436226, val loss 2.162899, mem 1.8 GiB @ 00 01:21:25.049, mean 00 00:00:00.017
step 270000: train loss 2.138511, val loss 2.1634743, mem 1.8 GiB @ 00 01:21:33.993, mean 00 00:00:00.017
step 270500: train loss 2.1615317, val loss 2.168573, mem 1.8 GiB @ 00 01:21:43.148, mean 00 00:00:00.018
step 271000: train loss 2.1508489, val loss 2.1677923, mem 1.8 GiB @ 00 01:21:52.248, mean 00 00:00:00.018
step 271500: train loss 2.1368637, val loss 2.1725256, mem 1.8 GiB @ 00 01:22:01.225, mean 00 00:00:00.017
step 272000: train loss 2.1583452, val loss 2.1659746, mem 1.8 GiB @ 00 01:22:10.264, mean 00 00:00:00.018
step 272500: train loss 2.1401837, val loss 2.1672513, mem 1.8 GiB @ 00 01:22:19.271, mean 00 00:00:00.018
step 273000: train loss 2.1424003, val loss 2.1851282, mem 1.8 GiB @ 00 01:22:28.441, mean 00 00:00:00.018
step 273500: train loss 2.1477363, val loss 2.1567633, mem 1.8 GiB @ 00 01:22:37.595, mean 00 00:00:00.018
step 274000: train loss 2.1420183, val loss 2.1514623, mem 1.8 GiB @ 00 01:22:46.902, mean 00 00:00:00.018
step 274500: train loss 2.1317763, val loss 2.1574152, mem 1.8 GiB @ 00 01:22:56.102, mean 00 00:00:00.018
step 275000: train loss 2.145213, val loss 2.1817887, mem 1.8 GiB @ 00 01:23:05.221, mean 00 00:00:00.018
step 275500: train loss 2.1229718, val loss 2.1798253, mem 1.8 GiB @ 00 01:23:14.420, mean 00 00:00:00.018
step 276000: train loss 2.1410396, val loss 2.1764328, mem 1.8 GiB @ 00 01:23:23.660, mean 00 00:00:00.018
step 276500: train loss 2.1395328, val loss 2.1618195, mem 1.8 GiB @ 00 01:23:32.844, mean 00 00:00:00.018
step 277000: train loss 2.13708, val loss 2.165939, mem 1.8 GiB @ 00 01:23:42.016, mean 00 00:00:00.018
step 277500: train loss 2.1400232, val loss 2.1651795, mem 1.8 GiB @ 00 01:23:51.084, mean 00 00:00:00.018
step 278000: train loss 2.1367986, val loss 2.1703467, mem 1.8 GiB @ 00 01:24:00.250, mean 00 00:00:00.018
step 278500: train loss 2.1461954, val loss 2.1546192, mem 1.8 GiB @ 00 01:24:09.431, mean 00 00:00:00.018
step 279000: train loss 2.1646197, val loss 2.171575, mem 1.8 GiB @ 00 01:24:18.597, mean 00 00:00:00.018
step 279500: train loss 2.1676314, val loss 2.1702545, mem 1.8 GiB @ 00 01:24:27.780, mean 00 00:00:00.018
step 280000: train loss 2.1666603, val loss 2.186359, mem 1.8 GiB @ 00 01:24:36.935, mean 00 00:00:00.018
step 280500: train loss 2.1622038, val loss 2.1844814, mem 1.8 GiB @ 00 01:24:46.129, mean 00 00:00:00.018
step 281000: train loss 2.1571624, val loss 2.1821358, mem 1.8 GiB @ 00 01:24:55.290, mean 00 00:00:00.018
step 281500: train loss 2.1549184, val loss 2.1954558, mem 1.8 GiB @ 00 01:25:04.480, mean 00 00:00:00.018
step 282000: train loss 2.1735296, val loss 2.1747024, mem 1.8 GiB @ 00 01:25:13.695, mean 00 00:00:00.018
step 282500: train loss 2.134854, val loss 2.1897101, mem 1.8 GiB @ 00 01:25:22.885, mean 00 00:00:00.018
step 283000: train loss 2.1452467, val loss 2.1943147, mem 1.8 GiB @ 00 01:25:32.111, mean 00 00:00:00.018
step 283500: train loss 2.1416254, val loss 2.1763828, mem 1.8 GiB @ 00 01:25:41.326, mean 00 00:00:00.018
step 284000: train loss 2.1519403, val loss 2.1690798, mem 1.8 GiB @ 00 01:25:50.502, mean 00 00:00:00.018
step 284500: train loss 2.1446936, val loss 2.1876354, mem 1.8 GiB @ 00 01:25:59.699, mean 00 00:00:00.018
step 285000: train loss 2.1312127, val loss 2.1573672, mem 1.8 GiB @ 00 01:26:08.884, mean 00 00:00:00.018
step 285500: train loss 2.146576, val loss 2.1863031, mem 1.8 GiB @ 00 01:26:17.982, mean 00 00:00:00.018
step 286000: train loss 2.1355634, val loss 2.1772082, mem 1.8 GiB @ 00 01:26:27.151, mean 00 00:00:00.018
step 286500: train loss 2.141859, val loss 2.1791558, mem 1.8 GiB @ 00 01:26:36.358, mean 00 00:00:00.018
step 287000: train loss 2.1429486, val loss 2.187324, mem 1.8 GiB @ 00 01:26:45.535, mean 00 00:00:00.018
step 287500: train loss 2.1244583, val loss 2.1808808, mem 1.8 GiB @ 00 01:26:54.685, mean 00 00:00:00.018
step 288000: train loss 2.1243644, val loss 2.1723022, mem 1.8 GiB @ 00 01:27:03.869, mean 00 00:00:00.018
step 288500: train loss 2.1355, val loss 2.1629474, mem 1.8 GiB @ 00 01:27:13.066, mean 00 00:00:00.018
step 289000: train loss 2.1457715, val loss 2.174135, mem 1.8 GiB @ 00 01:27:22.261, mean 00 00:00:00.018
step 289500: train loss 2.160235, val loss 2.164288, mem 1.8 GiB @ 00 01:27:31.459, mean 00 00:00:00.018
step 290000: train loss 2.1363835, val loss 2.1596856, mem 1.8 GiB @ 00 01:27:40.650, mean 00 00:00:00.018
step 290500: train loss 2.1250567, val loss 2.1750486, mem 1.8 GiB @ 00 01:27:49.764, mean 00 00:00:00.018
step 291000: train loss 2.1332643, val loss 2.1615713, mem 1.8 GiB @ 00 01:27:58.695, mean 00 00:00:00.017
step 291500: train loss 2.1284964, val loss 2.1612656, mem 1.8 GiB @ 00 01:28:07.483, mean 00 00:00:00.017
step 292000: train loss 2.1574783, val loss 2.16702, mem 1.8 GiB @ 00 01:28:16.222, mean 00 00:00:00.017
step 292500: train loss 2.1385114, val loss 2.1539502, mem 1.8 GiB @ 00 01:28:25.042, mean 00 00:00:00.017
step 293000: train loss 2.1466656, val loss 2.1665623, mem 1.8 GiB @ 00 01:28:33.800, mean 00 00:00:00.017
step 293500: train loss 2.1274097, val loss 2.1790977, mem 1.8 GiB @ 00 01:28:42.776, mean 00 00:00:00.017
step 294000: train loss 2.142195, val loss 2.1560984, mem 1.8 GiB @ 00 01:28:51.976, mean 00 00:00:00.018
step 294500: train loss 2.134467, val loss 2.163533, mem 1.8 GiB @ 00 01:29:01.149, mean 00 00:00:00.018
step 295000: train loss 2.1442223, val loss 2.1788557, mem 1.8 GiB @ 00 01:29:10.332, mean 00 00:00:00.018
step 295500: train loss 2.1380816, val loss 2.1712596, mem 1.8 GiB @ 00 01:29:19.536, mean 00 00:00:00.018
step 296000: train loss 2.1314635, val loss 2.1709092, mem 1.8 GiB @ 00 01:29:28.739, mean 00 00:00:00.018
step 296500: train loss 2.13762, val loss 2.1655862, mem 1.8 GiB @ 00 01:29:37.924, mean 00 00:00:00.018
step 297000: train loss 2.1295881, val loss 2.1700597, mem 1.8 GiB @ 00 01:29:47.121, mean 00 00:00:00.018
step 297500: train loss 2.113627, val loss 2.1650796, mem 1.8 GiB @ 00 01:29:56.309, mean 00 00:00:00.018
step 298000: train loss 2.1195447, val loss 2.1612856, mem 1.8 GiB @ 00 01:30:05.502, mean 00 00:00:00.018
step 298500: train loss 2.1331427, val loss 2.1610556, mem 1.8 GiB @ 00 01:30:14.677, mean 00 00:00:00.018
step 299000: train loss 2.1403596, val loss 2.1582816, mem 1.8 GiB @ 00 01:30:23.764, mean 00 00:00:00.018
step 299500: train loss 2.1210146, val loss 2.161055, mem 1.8 GiB @ 00 01:30:32.945, mean 00 00:00:00.018
step 300000: train loss 2.132016, val loss 2.1668432, mem 1.8 GiB @ 00 01:30:42.131, mean 00 00:00:00.018
step 300500: train loss 2.1290035, val loss 2.1687205, mem 1.8 GiB @ 00 01:30:51.309, mean 00 00:00:00.018
step 301000: train loss 2.123181, val loss 2.1570692, mem 1.8 GiB @ 00 01:31:00.507, mean 00 00:00:00.018
step 301500: train loss 2.115429, val loss 2.1766045, mem 1.8 GiB @ 00 01:31:09.698, mean 00 00:00:00.018
step 302000: train loss 2.127438, val loss 2.1650374, mem 1.8 GiB @ 00 01:31:18.895, mean 00 00:00:00.018
step 302500: train loss 2.129717, val loss 2.1719213, mem 1.8 GiB @ 00 01:31:28.004, mean 00 00:00:00.018
step 303000: train loss 2.1134713, val loss 2.1379223, mem 1.8 GiB @ 00 01:31:37.201, mean 00 00:00:00.018
step 303500: train loss 2.1349218, val loss 2.162844, mem 1.8 GiB @ 00 01:31:46.418, mean 00 00:00:00.018
step 304000: train loss 2.128136, val loss 2.1594605, mem 1.8 GiB @ 00 01:31:55.519, mean 00 00:00:00.018
step 304500: train loss 2.1297917, val loss 2.156052, mem 1.8 GiB @ 00 01:32:04.723, mean 00 00:00:00.018
step 305000: train loss 2.1157758, val loss 2.1558967, mem 1.8 GiB @ 00 01:32:13.892, mean 00 00:00:00.018
step 305500: train loss 2.127255, val loss 2.1601665, mem 1.8 GiB @ 00 01:32:22.877, mean 00 00:00:00.017
step 306000: train loss 2.1378708, val loss 2.1565154, mem 1.8 GiB @ 00 01:32:31.712, mean 00 00:00:00.017
step 306500: train loss 2.1259181, val loss 2.1453288, mem 1.8 GiB @ 00 01:32:40.677, mean 00 00:00:00.017
step 307000: train loss 2.1245508, val loss 2.1567905, mem 1.8 GiB @ 00 01:32:49.867, mean 00 00:00:00.018
step 307500: train loss 2.11475, val loss 2.1636417, mem 1.8 GiB @ 00 01:32:58.955, mean 00 00:00:00.018
step 308000: train loss 2.1206565, val loss 2.1549513, mem 1.8 GiB @ 00 01:33:08.133, mean 00 00:00:00.018
step 308500: train loss 2.1299078, val loss 2.1509273, mem 1.8 GiB @ 00 01:33:17.383, mean 00 00:00:00.018
step 309000: train loss 2.1197023, val loss 2.1533172, mem 1.8 GiB @ 00 01:33:26.328, mean 00 00:00:00.017
step 309500: train loss 2.1166644, val loss 2.1513698, mem 1.8 GiB @ 00 01:33:35.182, mean 00 00:00:00.017
step 310000: train loss 2.1240604, val loss 2.1555822, mem 1.8 GiB @ 00 01:33:44.076, mean 00 00:00:00.017
step 310500: train loss 2.1345408, val loss 2.1566947, mem 1.8 GiB @ 00 01:33:52.853, mean 00 00:00:00.017
step 311000: train loss 2.1101277, val loss 2.1597059, mem 1.8 GiB @ 00 01:34:01.602, mean 00 00:00:00.017
step 311500: train loss 2.124643, val loss 2.1507044, mem 1.8 GiB @ 00 01:34:10.458, mean 00 00:00:00.017
step 312000: train loss 2.109451, val loss 2.1749992, mem 1.8 GiB @ 00 01:34:19.354, mean 00 00:00:00.017
step 312500: train loss 2.1122384, val loss 2.1593335, mem 1.8 GiB @ 00 01:34:28.139, mean 00 00:00:00.017
step 313000: train loss 2.1122746, val loss 2.1371448, mem 1.8 GiB @ 00 01:34:36.896, mean 00 00:00:00.017
step 313500: train loss 2.1176538, val loss 2.1594474, mem 1.8 GiB @ 00 01:34:45.755, mean 00 00:00:00.017
step 314000: train loss 2.1164603, val loss 2.168251, mem 1.8 GiB @ 00 01:34:54.663, mean 00 00:00:00.017
step 314500: train loss 2.1174247, val loss 2.141436, mem 1.8 GiB @ 00 01:35:03.494, mean 00 00:00:00.017
step 315000: train loss 2.1250868, val loss 2.1632106, mem 1.8 GiB @ 00 01:35:12.224, mean 00 00:00:00.017
step 315500: train loss 2.1179864, val loss 2.1513736, mem 1.8 GiB @ 00 01:35:20.979, mean 00 00:00:00.017
step 316000: train loss 2.1312876, val loss 2.1663787, mem 1.8 GiB @ 00 01:35:29.902, mean 00 00:00:00.017
step 316500: train loss 2.12422, val loss 2.149642, mem 1.8 GiB @ 00 01:35:38.792, mean 00 00:00:00.017
step 317000: train loss 2.1223204, val loss 2.152373, mem 1.8 GiB @ 00 01:35:47.531, mean 00 00:00:00.017
step 317500: train loss 2.110614, val loss 2.1422765, mem 1.8 GiB @ 00 01:35:56.324, mean 00 00:00:00.017
step 318000: train loss 2.1206224, val loss 2.1558678, mem 1.8 GiB @ 00 01:36:05.146, mean 00 00:00:00.017
step 318500: train loss 2.1205628, val loss 2.1542947, mem 1.8 GiB @ 00 01:36:14.061, mean 00 00:00:00.017
step 319000: train loss 2.1296065, val loss 2.152644, mem 1.8 GiB @ 00 01:36:22.774, mean 00 00:00:00.017
step 319500: train loss 2.1243787, val loss 2.1607726, mem 1.8 GiB @ 00 01:36:31.583, mean 00 00:00:00.017
step 320000: train loss 2.1388412, val loss 2.159585, mem 1.8 GiB @ 00 01:36:40.471, mean 00 00:00:00.017
step 320500: train loss 2.1139712, val loss 2.150416, mem 1.8 GiB @ 00 01:36:49.420, mean 00 00:00:00.017
step 321000: train loss 2.1179001, val loss 2.133821, mem 1.8 GiB @ 00 01:36:58.181, mean 00 00:00:00.017
step 321500: train loss 2.1227791, val loss 2.1573195, mem 1.8 GiB @ 00 01:37:07.360, mean 00 00:00:00.018
step 322000: train loss 2.1260965, val loss 2.159699, mem 1.8 GiB @ 00 01:37:16.540, mean 00 00:00:00.018
step 322500: train loss 2.1243474, val loss 2.1574285, mem 1.8 GiB @ 00 01:37:25.714, mean 00 00:00:00.018
step 323000: train loss 2.119571, val loss 2.160794, mem 1.8 GiB @ 00 01:37:34.799, mean 00 00:00:00.018
step 323500: train loss 2.114405, val loss 2.1549485, mem 1.8 GiB @ 00 01:37:43.976, mean 00 00:00:00.018
step 324000: train loss 2.1222742, val loss 2.1400342, mem 1.8 GiB @ 00 01:37:53.168, mean 00 00:00:00.018
step 324500: train loss 2.1164432, val loss 2.140086, mem 1.8 GiB @ 00 01:38:02.338, mean 00 00:00:00.018
step 325000: train loss 2.1135066, val loss 2.1417122, mem 1.8 GiB @ 00 01:38:11.507, mean 00 00:00:00.018
step 325500: train loss 2.1099646, val loss 2.1311607, mem 1.8 GiB @ 00 01:38:20.686, mean 00 00:00:00.018
step 326000: train loss 2.102692, val loss 2.1570032, mem 1.8 GiB @ 00 01:38:29.850, mean 00 00:00:00.018
step 326500: train loss 2.1295958, val loss 2.1738822, mem 1.8 GiB @ 00 01:38:39.000, mean 00 00:00:00.018
step 327000: train loss 2.117627, val loss 2.1517608, mem 1.8 GiB @ 00 01:38:48.179, mean 00 00:00:00.018
step 327500: train loss 2.1210384, val loss 2.155103, mem 1.8 GiB @ 00 01:38:57.331, mean 00 00:00:00.018
step 328000: train loss 2.1116042, val loss 2.173334, mem 1.8 GiB @ 00 01:39:06.497, mean 00 00:00:00.018
step 328500: train loss 2.1128788, val loss 2.1470435, mem 1.8 GiB @ 00 01:39:15.666, mean 00 00:00:00.018
step 329000: train loss 2.1003642, val loss 2.1361978, mem 1.8 GiB @ 00 01:39:24.838, mean 00 00:00:00.018
step 329500: train loss 2.118966, val loss 2.1498015, mem 1.8 GiB @ 00 01:39:34.011, mean 00 00:00:00.018
step 330000: train loss 2.117217, val loss 2.1422253, mem 1.8 GiB @ 00 01:39:43.161, mean 00 00:00:00.018
step 330500: train loss 2.1240358, val loss 2.160828, mem 1.8 GiB @ 00 01:39:52.172, mean 00 00:00:00.018
step 331000: train loss 2.1104016, val loss 2.1476967, mem 1.8 GiB @ 00 01:40:00.927, mean 00 00:00:00.017
step 331500: train loss 2.1294363, val loss 2.1497383, mem 1.8 GiB @ 00 01:40:09.756, mean 00 00:00:00.017
step 332000: train loss 2.1079712, val loss 2.146928, mem 1.8 GiB @ 00 01:40:18.592, mean 00 00:00:00.017
step 332500: train loss 2.1182, val loss 2.1544268, mem 1.8 GiB @ 00 01:40:27.415, mean 00 00:00:00.017
step 333000: train loss 2.1159456, val loss 2.1570563, mem 1.8 GiB @ 00 01:40:36.284, mean 00 00:00:00.017
step 333500: train loss 2.116186, val loss 2.1465597, mem 1.8 GiB @ 00 01:40:45.139, mean 00 00:00:00.017
step 334000: train loss 2.106266, val loss 2.1482625, mem 1.8 GiB @ 00 01:40:53.973, mean 00 00:00:00.017
step 334500: train loss 2.0968087, val loss 2.133408, mem 1.8 GiB @ 00 01:41:02.816, mean 00 00:00:00.017
step 335000: train loss 2.1136992, val loss 2.1637516, mem 1.8 GiB @ 00 01:41:11.546, mean 00 00:00:00.017
step 335500: train loss 2.1055152, val loss 2.147069, mem 1.8 GiB @ 00 01:41:20.341, mean 00 00:00:00.017
step 336000: train loss 2.0999618, val loss 2.1589928, mem 1.8 GiB @ 00 01:41:29.327, mean 00 00:00:00.017
step 336500: train loss 2.1113791, val loss 2.1474667, mem 1.8 GiB @ 00 01:41:38.165, mean 00 00:00:00.017
step 337000: train loss 2.114695, val loss 2.143653, mem 1.8 GiB @ 00 01:41:46.966, mean 00 00:00:00.017
step 337500: train loss 2.1014738, val loss 2.163525, mem 1.8 GiB @ 00 01:41:55.778, mean 00 00:00:00.017
step 338000: train loss 2.1100469, val loss 2.1325896, mem 1.8 GiB @ 00 01:42:04.623, mean 00 00:00:00.017
step 338500: train loss 2.1062136, val loss 2.1334114, mem 1.8 GiB @ 00 01:42:13.398, mean 00 00:00:00.017
step 339000: train loss 2.1284034, val loss 2.1466866, mem 1.8 GiB @ 00 01:42:22.235, mean 00 00:00:00.017
step 339500: train loss 2.1073892, val loss 2.1350205, mem 1.8 GiB @ 00 01:42:30.983, mean 00 00:00:00.017
step 340000: train loss 2.104975, val loss 2.1440618, mem 1.8 GiB @ 00 01:42:39.769, mean 00 00:00:00.017
step 340500: train loss 2.1192093, val loss 2.1514637, mem 1.8 GiB @ 00 01:42:48.618, mean 00 00:00:00.017
step 341000: train loss 2.109818, val loss 2.1293314, mem 1.8 GiB @ 00 01:42:57.459, mean 00 00:00:00.017
step 341500: train loss 2.1055658, val loss 2.1430426, mem 1.8 GiB @ 00 01:43:06.267, mean 00 00:00:00.017
step 342000: train loss 2.1205678, val loss 2.1540167, mem 1.8 GiB @ 00 01:43:15.066, mean 00 00:00:00.017
step 342500: train loss 2.1033056, val loss 2.1484668, mem 1.8 GiB @ 00 01:43:23.867, mean 00 00:00:00.017
step 343000: train loss 2.1153567, val loss 2.1467981, mem 1.8 GiB @ 00 01:43:32.734, mean 00 00:00:00.017
step 343500: train loss 2.1127384, val loss 2.136958, mem 1.8 GiB @ 00 01:43:41.504, mean 00 00:00:00.017
step 344000: train loss 2.1085737, val loss 2.1496594, mem 1.8 GiB @ 00 01:43:50.232, mean 00 00:00:00.017
step 344500: train loss 2.114043, val loss 2.1284184, mem 1.8 GiB @ 00 01:43:59.086, mean 00 00:00:00.017
step 345000: train loss 2.093775, val loss 2.1368425, mem 1.8 GiB @ 00 01:44:07.905, mean 00 00:00:00.017
step 345500: train loss 2.1031036, val loss 2.1597967, mem 1.8 GiB @ 00 01:44:16.682, mean 00 00:00:00.017
step 346000: train loss 2.1006932, val loss 2.1356995, mem 1.8 GiB @ 00 01:44:25.542, mean 00 00:00:00.017
step 346500: train loss 2.107156, val loss 2.1375117, mem 1.8 GiB @ 00 01:44:34.396, mean 00 00:00:00.017
step 347000: train loss 2.1230466, val loss 2.1438653, mem 1.8 GiB @ 00 01:44:43.242, mean 00 00:00:00.017
step 347500: train loss 2.101157, val loss 2.150336, mem 1.8 GiB @ 00 01:44:52.000, mean 00 00:00:00.017
step 348000: train loss 2.09024, val loss 2.1361804, mem 1.8 GiB @ 00 01:45:00.802, mean 00 00:00:00.017
step 348500: train loss 2.1004999, val loss 2.1437104, mem 1.8 GiB @ 00 01:45:09.608, mean 00 00:00:00.017
step 349000: train loss 2.1095378, val loss 2.142568, mem 1.8 GiB @ 00 01:45:18.384, mean 00 00:00:00.017
step 349500: train loss 2.1225836, val loss 2.146404, mem 1.8 GiB @ 00 01:45:27.171, mean 00 00:00:00.017
step 349999: train loss 2.1012323, val loss 2.14412, mem 1.8 GiB @ 00 01:45:35.951, mean 00 00:00:00.017
step 350000: train loss 2.104257, val loss 2.142352, @ 00 01:45:35.966, mean 00 00:00:00.018
decode 13:'







OFKTNG Offrind, spareacks whour mich, are I there Of rede a by of is Eicheals
B whalld baow, shop sarupes! love pilm,
Firy yoe streme carl-manst pcome thepeie of on.
 so?

XICHALUSTARARGLY:.

Helonesesting
Ant.

GLICGANUS:
Barir fuater.

Formy fockiog Let my eyo dian withere k, gark quec
That ating:
Dow frur, cond!

HARGLCES: haverd,
Shaver
dele
Shom nten reinst.
Shat If e and fore mast aver!
She whont kink thy bad wim, Edus hereen't me'st.

IILUCIE:
Cay wedm backe homus; thats stion mossiness.
'
Exception in thread "main" java.lang.ExceptionInInitializerError
	at gpt.BiGram.main(BiGram.scala)
Caused by: java.lang.ArithmeticException: / by zero
	at gpt.BiGram$.<clinit>(BiGram.scala:2661)
	... 1 more
1 targets failed
examples.runMain subprocess failed
